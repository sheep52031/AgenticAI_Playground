{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TextSQL RAG Pipeline 學習筆記\n",
    "\n",
    "本筆記本將詳細介紹如何構建一個完整的 textSQL RAG (Retrieval-Augmented Generation) 流水線。\n",
    "\n",
    "## 目錄\n",
    "1. [RAG 概念介紹](#1-rag-概念介紹)\n",
    "2. [TextSQL 基礎](#2-textsql-基礎)\n",
    "3. [環境設置](#3-環境設置)\n",
    "4. [數據預處理](#4-數據預處理)\n",
    "5. [向量化與索引](#5-向量化與索引)\n",
    "6. [檢索系統](#6-檢索系統)\n",
    "7. [SQL 生成](#7-sql-生成)\n",
    "8. [完整流水線](#8-完整流水線)\n",
    "9. [評估與優化](#9-評估與優化)\n",
    "10. [實際應用案例](#10-實際應用案例)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. RAG 概念介紹\n",
    "\n",
    "### 什麼是 RAG？\n",
    "RAG（Retrieval-Augmented Generation）是一種結合了檢索和生成的架構：\n",
    "1. **檢索階段**：從知識庫中找到相關信息\n",
    "2. **生成階段**：基於檢索到的信息生成回答\n",
    "\n",
    "### TextSQL RAG 的特點\n",
    "- 專注於自然語言到 SQL 查詢的轉換\n",
    "- 結合數據庫模式信息\n",
    "- 支持複雜的數據庫查詢生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG 示例結果:\n",
      "基於上下文 [] 對查詢 '如何查詢用戶的訂單信息？' 的回答\n"
     ]
    }
   ],
   "source": [
    "# 基本概念示例\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# RAG 流程示意\n",
    "class SimpleRAGConcept:\n",
    "    def __init__(self):\n",
    "        self.knowledge_base = []\n",
    "        self.query_history = []\n",
    "    \n",
    "    def retrieve(self, query: str) -> List[str]:\n",
    "        \"\"\"檢索相關信息\"\"\"\n",
    "        # 這裡是簡化的檢索邏輯\n",
    "        relevant_docs = [doc for doc in self.knowledge_base \n",
    "                        if any(word.lower() in doc.lower() for word in query.split())]\n",
    "        return relevant_docs[:3]  # 返回前3個相關文檔\n",
    "    \n",
    "    def generate(self, query: str, context: List[str]) -> str:\n",
    "        \"\"\"基於上下文生成回答\"\"\"\n",
    "        # 這裡是簡化的生成邏輯\n",
    "        return f\"基於上下文 {context} 對查詢 '{query}' 的回答\"\n",
    "    \n",
    "    def rag_pipeline(self, query: str) -> str:\n",
    "        \"\"\"完整的RAG流水線\"\"\"\n",
    "        # 1. 檢索\n",
    "        relevant_context = self.retrieve(query)\n",
    "        # 2. 生成\n",
    "        response = self.generate(query, relevant_context)\n",
    "        # 3. 記錄查詢歷史\n",
    "        self.query_history.append({'query': query, 'response': response})\n",
    "        return response\n",
    "\n",
    "# 示例使用\n",
    "rag_demo = SimpleRAGConcept()\n",
    "rag_demo.knowledge_base = [\n",
    "    \"用戶表包含用戶ID、姓名、郵箱等字段\",\n",
    "    \"訂單表記錄了所有的購買信息\",\n",
    "    \"產品表存儲產品的詳細信息\"\n",
    "]\n",
    "\n",
    "result = rag_demo.rag_pipeline(\"如何查詢用戶的訂單信息？\")\n",
    "print(\"RAG 示例結果:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. TextSQL 基礎\n",
    "\n",
    "### 核心概念\n",
    "TextSQL 是將自然語言查詢轉換為 SQL 語句的過程。關鍵組件包括：\n",
    "- **Schema Understanding**: 理解數據庫結構\n",
    "- **Intent Recognition**: 識別用戶意圖\n",
    "- **SQL Generation**: 生成對應的SQL語句"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "數據庫模式信息:\n",
      "表 users: {'user_id': 'INT PRIMARY KEY', 'name': 'VARCHAR(100)', 'email': 'VARCHAR(100)', 'created_at': 'DATETIME'}\n",
      "表 orders: {'order_id': 'INT PRIMARY KEY', 'user_id': 'INT', 'product_id': 'INT', 'quantity': 'INT', 'order_date': 'DATETIME'}\n",
      "表 products: {'product_id': 'INT PRIMARY KEY', 'name': 'VARCHAR(100)', 'price': 'DECIMAL(10,2)', 'category': 'VARCHAR(50)'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TextSQL 基礎組件\n",
    "class DatabaseSchema:\n",
    "    \"\"\"數據庫模式類\"\"\"\n",
    "    def __init__(self):\n",
    "        self.tables = {}\n",
    "        self.relationships = []\n",
    "    \n",
    "    def add_table(self, table_name: str, columns: Dict[str, str]):\n",
    "        \"\"\"添加表結構\"\"\"\n",
    "        self.tables[table_name] = columns\n",
    "    \n",
    "    def add_relationship(self, table1: str, column1: str, table2: str, column2: str):\n",
    "        \"\"\"添加表關係\"\"\"\n",
    "        self.relationships.append({\n",
    "            'from_table': table1,\n",
    "            'from_column': column1,\n",
    "            'to_table': table2,\n",
    "            'to_column': column2\n",
    "        })\n",
    "    \n",
    "    def get_schema_info(self) -> str:\n",
    "        \"\"\"獲取模式信息\"\"\"\n",
    "        schema_info = \"數據庫模式信息:\\n\"\n",
    "        for table, columns in self.tables.items():\n",
    "            schema_info += f\"表 {table}: {columns}\\n\"\n",
    "        return schema_info\n",
    "\n",
    "# 創建示例數據庫模式\n",
    "schema = DatabaseSchema()\n",
    "schema.add_table('users', {\n",
    "    'user_id': 'INT PRIMARY KEY',\n",
    "    'name': 'VARCHAR(100)',\n",
    "    'email': 'VARCHAR(100)',\n",
    "    'created_at': 'DATETIME'\n",
    "})\n",
    "\n",
    "schema.add_table('orders', {\n",
    "    'order_id': 'INT PRIMARY KEY',\n",
    "    'user_id': 'INT',\n",
    "    'product_id': 'INT',\n",
    "    'quantity': 'INT',\n",
    "    'order_date': 'DATETIME'\n",
    "})\n",
    "\n",
    "schema.add_table('products', {\n",
    "    'product_id': 'INT PRIMARY KEY',\n",
    "    'name': 'VARCHAR(100)',\n",
    "    'price': 'DECIMAL(10,2)',\n",
    "    'category': 'VARCHAR(50)'\n",
    "})\n",
    "\n",
    "schema.add_relationship('orders', 'user_id', 'users', 'user_id')\n",
    "schema.add_relationship('orders', 'product_id', 'products', 'product_id')\n",
    "\n",
    "print(schema.get_schema_info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 環境設置\n",
    "\n",
    "### 安裝必要的庫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 在 Kaggle 環境中安裝必要的包\nimport sys\nimport subprocess\n\ndef install_package(package):\n    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n\n# 首先修復protobuf版本衝突問題\nprint(\"🔧 修復protobuf版本衝突...\")\ntry:\n    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"protobuf\"])\n    print(\"✓ protobuf 版本已修復\")\nexcept Exception as e:\n    print(f\"⚠️ protobuf 修復失敗，將使用備用方案: {e}\")\n\n# 核心包列表 (sqlite3 是Python標準庫，不需要安裝)\npackages = [\n    'transformers',\n    'torch',\n    'sentence-transformers',\n    'faiss-cpu',\n    'sqlparse',\n    'datasets'  # 用於加載 Hugging Face 數據集\n]\n\n# 可選包列表（如果安裝失敗不影響核心功能）\noptional_packages = [\n    'chromadb',\n    'langchain',\n    'openai'\n]\n\nprint(\"\\n正在安裝核心包...\")\nfor package in packages:\n    try:\n        install_package(package)\n        print(f\"✓ {package} 安裝成功\")\n    except Exception as e:\n        print(f\"✗ {package} 安裝失敗: {e}\")\n\nprint(\"\\n正在安裝可選包...\")\nfor package in optional_packages:\n    try:\n        install_package(package)\n        print(f\"✓ {package} 安裝成功\")\n    except Exception as e:\n        print(f\"⚠️ {package} 安裝失敗（不影響核心功能）: {e}\")\n\nprint(\"\\n環境設置完成！\")\nprint(\"📝 注意：sqlite3 是 Python 內建庫，無需安裝\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 導入必要的庫（帶錯誤處理）\nimport os\nimport json\nimport sqlite3\nimport pandas as pd\nimport numpy as np\nfrom typing import List, Dict, Any, Tuple\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# 核心導入\nprint(\"導入核心庫...\")\ntry:\n    import sqlparse\n    print(\"✓ sqlparse 導入成功\")\nexcept ImportError as e:\n    print(f\"⚠️ sqlparse 導入失敗: {e}\")\n    # 創建簡單的備用解析器\n    class MockSQLParse:\n        @staticmethod\n        def parse(sql):\n            return [type('Token', (), {'tokens': [sql] if sql.strip() else []})]\n    sqlparse = MockSQLParse()\n\n# 向量化和檢索（帶備用方案）\nprint(\"導入向量化庫...\")\ntry:\n    from sentence_transformers import SentenceTransformer\n    print(\"✓ sentence_transformers 導入成功\")\n    SENTENCE_TRANSFORMERS_AVAILABLE = True\nexcept ImportError as e:\n    print(f\"⚠️ sentence_transformers 導入失敗: {e}\")\n    SENTENCE_TRANSFORMERS_AVAILABLE = False\n    # 將在TextEmbedder中使用備用方案\n\ntry:\n    import faiss\n    print(\"✓ faiss 導入成功\")\n    FAISS_AVAILABLE = True\nexcept ImportError as e:\n    print(f\"⚠️ faiss 導入失敗: {e}\")\n    FAISS_AVAILABLE = False\n    # 將使用簡單的向量搜索備用方案\n\n# 自然語言處理（可選）\nprint(\"導入NLP庫...\")\ntry:\n    from transformers import AutoTokenizer, AutoModel\n    import torch\n    print(\"✓ transformers 和 torch 導入成功\")\n    TRANSFORMERS_AVAILABLE = True\nexcept ImportError as e:\n    print(f\"⚠️ transformers/torch 導入失敗: {e}\")\n    TRANSFORMERS_AVAILABLE = False\n\nprint(\"\\n✅ 庫導入完成！\")\nprint(f\"📊 功能狀態:\")\nprint(f\"  • 句子嵌入: {'可用' if SENTENCE_TRANSFORMERS_AVAILABLE else '使用備用方案'}\")\nprint(f\"  • 向量索引: {'可用' if FAISS_AVAILABLE else '使用備用方案'}\")\nprint(f\"  • 高級NLP: {'可用' if TRANSFORMERS_AVAILABLE else '基礎功能'}\")\nprint(f\"  • SQL解析: 可用\")\nprint(f\"  • 數據庫: 可用 (SQLite3)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 數據預處理\n",
    "\n",
    "### 創建示例數據庫和數據"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 創建示例 SQLite 數據庫\n",
    "def create_sample_database():\n",
    "    \"\"\"創建示例數據庫\"\"\"\n",
    "    conn = sqlite3.connect('sample_ecommerce.db')\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # 創建用戶表\n",
    "    cursor.execute('''\n",
    "    CREATE TABLE IF NOT EXISTS users (\n",
    "        user_id INTEGER PRIMARY KEY,\n",
    "        name TEXT NOT NULL,\n",
    "        email TEXT UNIQUE NOT NULL,\n",
    "        created_at DATETIME DEFAULT CURRENT_TIMESTAMP\n",
    "    )\n",
    "    ''')\n",
    "    \n",
    "    # 創建產品表\n",
    "    cursor.execute('''\n",
    "    CREATE TABLE IF NOT EXISTS products (\n",
    "        product_id INTEGER PRIMARY KEY,\n",
    "        name TEXT NOT NULL,\n",
    "        price DECIMAL(10,2) NOT NULL,\n",
    "        category TEXT NOT NULL,\n",
    "        description TEXT\n",
    "    )\n",
    "    ''')\n",
    "    \n",
    "    # 創建訂單表\n",
    "    cursor.execute('''\n",
    "    CREATE TABLE IF NOT EXISTS orders (\n",
    "        order_id INTEGER PRIMARY KEY,\n",
    "        user_id INTEGER,\n",
    "        product_id INTEGER,\n",
    "        quantity INTEGER NOT NULL,\n",
    "        order_date DATETIME DEFAULT CURRENT_TIMESTAMP,\n",
    "        FOREIGN KEY (user_id) REFERENCES users (user_id),\n",
    "        FOREIGN KEY (product_id) REFERENCES products (product_id)\n",
    "    )\n",
    "    ''')\n",
    "    \n",
    "    # 插入示例數據\n",
    "    users_data = [\n",
    "        (1, '張三', 'zhang@example.com'),\n",
    "        (2, '李四', 'li@example.com'),\n",
    "        (3, '王五', 'wang@example.com')\n",
    "    ]\n",
    "    \n",
    "    products_data = [\n",
    "        (1, 'iPhone 15', 999.99, '電子產品', '最新款智能手機'),\n",
    "        (2, 'MacBook Pro', 1299.99, '電子產品', '專業筆記本電腦'),\n",
    "        (3, '咖啡機', 199.99, '家電', '全自動咖啡機'),\n",
    "        (4, '書籍：Python編程', 29.99, '圖書', 'Python編程入門教程')\n",
    "    ]\n",
    "    \n",
    "    orders_data = [\n",
    "        (1, 1, 1, 1),\n",
    "        (2, 1, 3, 1),\n",
    "        (3, 2, 2, 1),\n",
    "        (4, 3, 4, 2)\n",
    "    ]\n",
    "    \n",
    "    cursor.executemany('INSERT OR REPLACE INTO users (user_id, name, email) VALUES (?, ?, ?)', users_data)\n",
    "    cursor.executemany('INSERT OR REPLACE INTO products (product_id, name, price, category, description) VALUES (?, ?, ?, ?, ?)', products_data)\n",
    "    cursor.executemany('INSERT OR REPLACE INTO orders (order_id, user_id, product_id, quantity) VALUES (?, ?, ?, ?)', orders_data)\n",
    "    \n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    print(\"示例數據庫創建完成！\")\n",
    "\n",
    "# 創建數據庫\n",
    "create_sample_database()\n",
    "\n",
    "# 驗證數據\n",
    "def verify_database():\n",
    "    conn = sqlite3.connect('sample_ecommerce.db')\n",
    "    \n",
    "    print(\"用戶表:\")\n",
    "    users_df = pd.read_sql_query('SELECT * FROM users', conn)\n",
    "    print(users_df)\n",
    "    \n",
    "    print(\"\\n產品表:\")\n",
    "    products_df = pd.read_sql_query('SELECT * FROM products', conn)\n",
    "    print(products_df)\n",
    "    \n",
    "    print(\"\\n訂單表:\")\n",
    "    orders_df = pd.read_sql_query('SELECT * FROM orders', conn)\n",
    "    print(orders_df)\n",
    "    \n",
    "    conn.close()\n",
    "\n",
    "verify_database()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 準備訓練數據集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 準備訓練數據集 - 使用專業數據集和自製數據集結合\n\nprint(\"🔍 獲取專業 Text2SQL 數據集...\")\n\n# 方案1: 使用 Hugging Face 專業數據集\ndef load_professional_datasets():\n    \"\"\"加載專業 Text2SQL 數據集\"\"\"\n    professional_data = []\n    \n    try:\n        # 嘗試使用 datasets 庫加載專業數據集\n        from datasets import load_dataset\n        \n        print(\"📦 嘗試加載 Hugging Face 數據集...\")\n        \n        # 加載一些知名的 Text2SQL 數據集\n        dataset_configs = [\n            (\"gretelai/synthetic_text_to_sql\", \"部分合成數據集\"),\n            (\"NumbersStation/NSText2SQL\", \"專業基準數據集\"),\n            (\"spider\", \"學術標準數據集\")\n        ]\n        \n        for dataset_name, description in dataset_configs:\n            try:\n                print(f\"  • 嘗試加載: {dataset_name}\")\n                dataset = load_dataset(dataset_name, split=\"train[:100]\")  # 只加載前100條\n                \n                for item in dataset:\n                    # 統一數據格式\n                    if 'question' in item and 'query' in item:\n                        professional_data.append({\n                            \"natural_language\": item['question'],\n                            \"sql\": item['query'],\n                            \"explanation\": f\"來自{description}\",\n                            \"source\": dataset_name\n                        })\n                \n                print(f\"    ✓ 成功加載 {len([d for d in professional_data if d['source'] == dataset_name])} 條數據\")\n                break  # 成功加載一個數據集即可\n                \n            except Exception as e:\n                print(f\"    ⚠️ {dataset_name} 加載失敗: {str(e)[:50]}...\")\n                continue\n                \n    except ImportError:\n        print(\"⚠️ datasets 庫不可用，使用本地數據集\")\n    \n    return professional_data\n\n# 嘗試加載專業數據集\nprofessional_data = load_professional_datasets()\n\n# 方案2: 自製基礎數據集（用於學習和備用）\nprint(\"\\n📝 準備基礎訓練數據集...\")\nbasic_training_data = [\n    {\n        \"natural_language\": \"顯示所有用戶的信息\",\n        \"sql\": \"SELECT * FROM users;\",\n        \"explanation\": \"查詢用戶表中的所有記錄\",\n        \"source\": \"basic_examples\"\n    },\n    {\n        \"natural_language\": \"找出價格超過500元的產品\",\n        \"sql\": \"SELECT * FROM products WHERE price > 500;\",\n        \"explanation\": \"使用WHERE子句過濾價格大於500的產品\",\n        \"source\": \"basic_examples\"\n    },\n    {\n        \"natural_language\": \"統計每個用戶的訂單數量\",\n        \"sql\": \"SELECT u.name, COUNT(o.order_id) as order_count FROM users u LEFT JOIN orders o ON u.user_id = o.user_id GROUP BY u.user_id, u.name;\",\n        \"explanation\": \"使用JOIN和GROUP BY統計每個用戶的訂單數量\",\n        \"source\": \"basic_examples\"\n    },\n    {\n        \"natural_language\": \"查找最貴的產品\",\n        \"sql\": \"SELECT * FROM products ORDER BY price DESC LIMIT 1;\",\n        \"explanation\": \"使用ORDER BY和LIMIT找到價格最高的產品\",\n        \"source\": \"basic_examples\"\n    },\n    {\n        \"natural_language\": \"顯示用戶張三的所有訂單\",\n        \"sql\": \"SELECT o.*, p.name as product_name FROM orders o JOIN users u ON o.user_id = u.user_id JOIN products p ON o.product_id = p.product_id WHERE u.name = '張三';\",\n        \"explanation\": \"使用多表JOIN查詢特定用戶的訂單信息\",\n        \"source\": \"basic_examples\"\n    },\n    {\n        \"natural_language\": \"計算每個類別的產品平均價格\",\n        \"sql\": \"SELECT category, AVG(price) as avg_price FROM products GROUP BY category;\",\n        \"explanation\": \"使用GROUP BY和AVG函數計算各類別的平均價格\",\n        \"source\": \"basic_examples\"\n    },\n    {\n        \"natural_language\": \"找出沒有下過訂單的用戶\",\n        \"sql\": \"SELECT u.* FROM users u LEFT JOIN orders o ON u.user_id = o.user_id WHERE o.user_id IS NULL;\",\n        \"explanation\": \"使用LEFT JOIN和IS NULL找出沒有訂單的用戶\",\n        \"source\": \"basic_examples\"\n    },\n    {\n        \"natural_language\": \"顯示銷量最高的產品\",\n        \"sql\": \"SELECT p.name, SUM(o.quantity) as total_sold FROM products p JOIN orders o ON p.product_id = o.product_id GROUP BY p.product_id, p.name ORDER BY total_sold DESC LIMIT 1;\",\n        \"explanation\": \"統計產品銷量並找出銷量最高的產品\",\n        \"source\": \"basic_examples\"\n    }\n]\n\n# 方案3: 整合數據集\nprint(\"\\n🔄 整合數據集...\")\nif professional_data:\n    training_data = professional_data + basic_training_data\n    print(f\"✓ 使用混合數據集: {len(professional_data)} 條專業數據 + {len(basic_training_data)} 條基礎數據\")\nelse:\n    training_data = basic_training_data\n    print(f\"✓ 使用基礎數據集: {len(basic_training_data)} 條自製數據\")\n\n# 數據集統計\nsources = {}\nfor item in training_data:\n    source = item.get('source', 'unknown')\n    sources[source] = sources.get(source, 0) + 1\n\nprint(f\"\\n📊 數據集統計 (總計 {len(training_data)} 條):\")\nfor source, count in sources.items():\n    print(f\"  • {source}: {count} 條\")\n\n# 保存完整數據集\nwith open('training_data.json', 'w', encoding='utf-8') as f:\n    json.dump(training_data, f, ensure_ascii=False, indent=2)\n\nprint(f\"\\n💾 數據集已保存到 training_data.json\")\n\n# 顯示數據集示例\nprint(f\"\\n📋 數據集示例:\")\nfor i, item in enumerate(training_data[:3]):\n    print(f\"\\n樣本 {i+1} ({item.get('source', 'unknown')}):\")\n    print(f\"  自然語言: {item['natural_language']}\")\n    print(f\"  SQL: {item['sql']}\")\n    print(f\"  說明: {item['explanation']}\")\n\nprint(f\"\\n💡 數據集來源說明:\")\nprint(f\"  • 專業數據集: 來自 Hugging Face，質量高，覆蓋面廣\")\nprint(f\"  • 基礎數據集: 針對示例數據庫設計，便於學習理解\")\nprint(f\"  • 混合策略: 結合專業性和針對性，提升RAG效果\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 向量化與索引\n",
    "\n",
    "### 使用句子嵌入模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 初始化句子嵌入模型（增強版，支援備用方案）\nclass TextEmbedder:\n    def __init__(self, model_name='all-MiniLM-L6-v2'):\n        \"\"\"初始化文本嵌入器\"\"\"\n        self.model = None\n        self.use_simple_encoding = False\n        \n        if SENTENCE_TRANSFORMERS_AVAILABLE:\n            try:\n                self.model = SentenceTransformer(model_name)\n                print(f\"✓ 成功加載模型: {model_name}\")\n            except Exception as e:\n                print(f\"⚠️ 模型加載失敗: {e}, 使用備用方案\")\n                self.use_simple_encoding = True\n        else:\n            print(\"⚠️ sentence_transformers 不可用，使用簡單編碼方案\")\n            self.use_simple_encoding = True\n    \n    def encode(self, texts: List[str]) -> np.ndarray:\n        \"\"\"將文本編碼為向量\"\"\"\n        if self.model and not self.use_simple_encoding:\n            try:\n                return self.model.encode(texts)\n            except Exception as e:\n                print(f\"⚠️ 模型編碼失敗: {e}, 切換到備用方案\")\n                self.use_simple_encoding = True\n                return self._simple_encode(texts)\n        else:\n            return self._simple_encode(texts)\n    \n    def _simple_encode(self, texts: List[str]) -> np.ndarray:\n        \"\"\"簡單的文本編碼備用方案（基於TF-IDF概念）\"\"\"\n        # 構建詞彙表\n        all_words = set()\n        for text in texts:\n            words = text.lower().replace('，', ' ').replace('。', ' ').split()\n            all_words.update(words)\n        \n        vocab = {word: idx for idx, word in enumerate(sorted(all_words))}\n        \n        # 創建向量\n        vectors = []\n        for text in texts:\n            vector = np.zeros(len(vocab))\n            words = text.lower().replace('，', ' ').replace('。', ' ').split()\n            \n            # 簡單的詞頻統計\n            word_count = {}\n            for word in words:\n                word_count[word] = word_count.get(word, 0) + 1\n            \n            # 填充向量\n            for word, count in word_count.items():\n                if word in vocab:\n                    vector[vocab[word]] = count\n            \n            # 正規化\n            if np.linalg.norm(vector) > 0:\n                vector = vector / np.linalg.norm(vector)\n            \n            vectors.append(vector)\n        \n        return np.array(vectors)\n\n# 初始化嵌入器\nembedder = TextEmbedder()\n\n# 測試嵌入功能\ntest_texts = [\n    \"查詢所有用戶信息\",\n    \"顯示產品詳細信息\",\n    \"統計訂單數量\"\n]\n\nembeddings = embedder.encode(test_texts)\nprint(f\"\\n📊 文本嵌入測試:\")\nprint(f\"  • 嵌入維度: {embeddings.shape}\")\nprint(f\"  • 編碼方式: {'神經網絡模型' if not embedder.use_simple_encoding else '簡單TF-IDF'}\")\nprint(f\"  • 示例向量前5維: {embeddings[0][:5]}\")\n\n# 測試相似度\nif len(embeddings) >= 2:\n    similarity = np.dot(embeddings[0], embeddings[1])\n    print(f\"  • 前兩個文本相似度: {similarity:.3f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 構建向量索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 構建向量索引（增強版，支援FAISS備用方案）\nclass VectorIndex:\n    def __init__(self, dimension: int):\n        \"\"\"初始化向量索引\"\"\"\n        self.dimension = dimension\n        self.texts = []  # 存儲原始文本\n        self.metadata = []  # 存儲元數據\n        self.vectors = []  # 備用方案：存儲所有向量\n        self.use_faiss = FAISS_AVAILABLE\n        \n        if self.use_faiss:\n            try:\n                self.index = faiss.IndexFlatL2(dimension)  # L2距離索引\n                print(f\"✓ 使用 FAISS 索引，維度: {dimension}\")\n            except Exception as e:\n                print(f\"⚠️ FAISS 初始化失敗: {e}, 使用備用索引\")\n                self.use_faiss = False\n        \n        if not self.use_faiss:\n            print(f\"✓ 使用簡單向量索引，維度: {dimension}\")\n    \n    def add_vectors(self, vectors: np.ndarray, texts: List[str], metadata: List[Dict]):\n        \"\"\"添加向量到索引\"\"\"\n        if self.use_faiss:\n            try:\n                self.index.add(vectors.astype('float32'))\n            except Exception as e:\n                print(f\"⚠️ FAISS 添加向量失敗: {e}, 切換到備用方案\")\n                self.use_faiss = False\n                self.vectors = vectors.tolist()  # 轉換為列表存儲\n        \n        if not self.use_faiss:\n            self.vectors.extend(vectors.tolist())\n        \n        self.texts.extend(texts)\n        self.metadata.extend(metadata)\n        print(f\"✓ 已添加 {len(vectors)} 個向量到索引\")\n    \n    def search(self, query_vector: np.ndarray, k: int = 5) -> List[Dict]:\n        \"\"\"搜索最相似的向量\"\"\"\n        if self.use_faiss:\n            return self._faiss_search(query_vector, k)\n        else:\n            return self._simple_search(query_vector, k)\n    \n    def _faiss_search(self, query_vector: np.ndarray, k: int) -> List[Dict]:\n        \"\"\"使用 FAISS 搜索\"\"\"\n        try:\n            query_vector = query_vector.reshape(1, -1).astype('float32')\n            distances, indices = self.index.search(query_vector, k)\n            \n            results = []\n            for i, (distance, idx) in enumerate(zip(distances[0], indices[0])):\n                if idx < len(self.texts):\n                    results.append({\n                        'text': self.texts[idx],\n                        'metadata': self.metadata[idx],\n                        'distance': float(distance),\n                        'rank': i + 1\n                    })\n            return results\n        except Exception as e:\n            print(f\"⚠️ FAISS 搜索失敗: {e}, 切換到備用搜索\")\n            self.use_faiss = False\n            return self._simple_search(query_vector, k)\n    \n    def _simple_search(self, query_vector: np.ndarray, k: int) -> List[Dict]:\n        \"\"\"簡單的向量搜索（餘弦相似度）\"\"\"\n        if not self.vectors:\n            return []\n        \n        # 計算與所有向量的相似度\n        similarities = []\n        query_norm = np.linalg.norm(query_vector)\n        \n        for i, stored_vector in enumerate(self.vectors):\n            stored_vector = np.array(stored_vector)\n            stored_norm = np.linalg.norm(stored_vector)\n            \n            if query_norm > 0 and stored_norm > 0:\n                # 餘弦相似度\n                similarity = np.dot(query_vector, stored_vector) / (query_norm * stored_norm)\n                # 轉換為距離（距離越小越相似）\n                distance = 1 - similarity\n            else:\n                distance = float('inf')\n            \n            similarities.append((distance, i))\n        \n        # 排序並返回前k個\n        similarities.sort(key=lambda x: x[0])\n        \n        results = []\n        for rank, (distance, idx) in enumerate(similarities[:k]):\n            if idx < len(self.texts):\n                results.append({\n                    'text': self.texts[idx],\n                    'metadata': self.metadata[idx],\n                    'distance': float(distance),\n                    'rank': rank + 1\n                })\n        \n        return results\n    \n    def get_stats(self) -> Dict:\n        \"\"\"獲取索引統計信息\"\"\"\n        total_vectors = len(self.vectors) if not self.use_faiss else (self.index.ntotal if hasattr(self, 'index') else 0)\n        return {\n            'total_vectors': total_vectors,\n            'dimension': self.dimension,\n            'index_type': 'FAISS-FlatL2' if self.use_faiss else 'Simple-Cosine'\n        }\n\n# 為訓練數據創建向量索引\nnatural_language_queries = [item['natural_language'] for item in training_data]\nquery_embeddings = embedder.encode(natural_language_queries)\n\n# 初始化向量索引\nvector_index = VectorIndex(query_embeddings.shape[1])\n\n# 添加向量到索引\nvector_index.add_vectors(\n    query_embeddings,\n    natural_language_queries,\n    training_data\n)\n\nprint(f\"\\n📊 向量索引統計:\")\nstats = vector_index.get_stats()\nfor key, value in stats.items():\n    print(f\"  • {key}: {value}\")\n\n# 測試搜索功能\ntest_query = \"查詢用戶信息\"\ntest_vector = embedder.encode([test_query])[0]\nsearch_results = vector_index.search(test_vector, k=3)\n\nprint(f\"\\n🔍 搜索測試: '{test_query}'\")\nfor result in search_results:\n    print(f\"  • 匹配: {result['text']} (距離: {result['distance']:.3f})\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 檢索系統\n",
    "\n",
    "### 實現語義檢索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 實現檢索系統\n",
    "class SemanticRetriever:\n",
    "    def __init__(self, embedder: TextEmbedder, vector_index: VectorIndex, schema: DatabaseSchema):\n",
    "        \"\"\"初始化語義檢索器\"\"\"\n",
    "        self.embedder = embedder\n",
    "        self.vector_index = vector_index\n",
    "        self.schema = schema\n",
    "    \n",
    "    def retrieve(self, query: str, k: int = 3) -> List[Dict]:\n",
    "        \"\"\"檢索相關的SQL示例\"\"\"\n",
    "        # 1. 將查詢編碼為向量\n",
    "        query_vector = self.embedder.encode([query])[0]\n",
    "        \n",
    "        # 2. 在向量索引中搜索\n",
    "        similar_examples = self.vector_index.search(query_vector, k)\n",
    "        \n",
    "        # 3. 添加數據庫模式信息\n",
    "        schema_info = self.schema.get_schema_info()\n",
    "        \n",
    "        # 4. 構建檢索結果\n",
    "        retrieval_results = {\n",
    "            'query': query,\n",
    "            'schema_info': schema_info,\n",
    "            'similar_examples': similar_examples,\n",
    "            'retrieval_score': self._calculate_retrieval_score(similar_examples)\n",
    "        }\n",
    "        \n",
    "        return retrieval_results\n",
    "    \n",
    "    def _calculate_retrieval_score(self, examples: List[Dict]) -> float:\n",
    "        \"\"\"計算檢索質量分數\"\"\"\n",
    "        if not examples:\n",
    "            return 0.0\n",
    "        \n",
    "        # 基於距離計算分數（距離越小，分數越高）\n",
    "        distances = [ex['distance'] for ex in examples]\n",
    "        avg_distance = sum(distances) / len(distances)\n",
    "        score = max(0, 1 - avg_distance / 10)  # 簡單的評分公式\n",
    "        return score\n",
    "    \n",
    "    def explain_retrieval(self, results: Dict) -> str:\n",
    "        \"\"\"解釋檢索過程\"\"\"\n",
    "        explanation = f\"\\n檢索結果解釋：\\n\"\n",
    "        explanation += f\"查詢: {results['query']}\\n\"\n",
    "        explanation += f\"檢索分數: {results['retrieval_score']:.3f}\\n\"\n",
    "        explanation += f\"找到 {len(results['similar_examples'])} 個相似示例:\\n\"\n",
    "        \n",
    "        for i, example in enumerate(results['similar_examples']):\n",
    "            explanation += f\"\\n{i+1}. 相似度: {1-example['distance']:.3f}\\n\"\n",
    "            explanation += f\"   查詢: {example['text']}\\n\"\n",
    "            explanation += f\"   SQL: {example['metadata']['sql']}\\n\"\n",
    "        \n",
    "        return explanation\n",
    "\n",
    "# 初始化檢索器\n",
    "retriever = SemanticRetriever(embedder, vector_index, schema)\n",
    "\n",
    "# 測試檢索功能\n",
    "test_query = \"我要查看所有產品的詳細信息\"\n",
    "retrieval_results = retriever.retrieve(test_query)\n",
    "\n",
    "print(\"=== 檢索測試 ===\")\n",
    "print(retriever.explain_retrieval(retrieval_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. SQL 生成\n",
    "\n",
    "### 基於檢索結果生成SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# SQL生成器 (2025年增強版 - 支援現代模型)\nimport re\nfrom datetime import datetime\n\nclass SQLGenerator:\n    def __init__(self, schema: DatabaseSchema, model_approach=\"rag\"):\n        \"\"\"初始化SQL生成器\n        \n        Args:\n            schema: 數據庫模式\n            model_approach: 'rag' (檢索增強) 或 'llm' (大語言模型)\n        \"\"\"\n        self.schema = schema\n        self.model_approach = model_approach\n        self.templates = self._load_sql_templates()\n        \n        # 2025年推薦的模型配置\n        self.recommended_models = {\n            \"kaggle_friendly\": {\n                \"name\": \"Salesforce/codet5p-220m\",\n                \"size\": \"220M\",\n                \"memory\": \"~1GB\",\n                \"description\": \"最適合Kaggle，專為代碼生成設計\"\n            },\n            \"balanced\": {\n                \"name\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", \n                \"size\": \"1.1B\",\n                \"memory\": \"~3GB\",\n                \"description\": \"平衡效能與資源，支援QLoRA\"\n            },\n            \"production\": {\n                \"name\": \"mistralai/Mistral-7B-Instruct-v0.3\",\n                \"size\": \"7B\", \n                \"memory\": \"~14GB\",\n                \"description\": \"生產級別，需要較多資源\"\n            }\n        }\n        \n        # 嘗試載入現代LLM（如果可用）\n        self.llm_available = self._try_load_llm()\n        \n        print(f\"✓ SQL生成器初始化完成\")\n        print(f\"  • 生成方法: {model_approach.upper()}\")\n        print(f\"  • LLM可用性: {'是' if self.llm_available else '否 (使用模板方法)'}\")\n    \n    def _try_load_llm(self):\n        \"\"\"嘗試載入LLM模型\"\"\"\n        if self.model_approach == \"rag\":\n            return False  # RAG方法不需要載入LLM\n        \n        try:\n            if TRANSFORMERS_AVAILABLE:\n                # 這裡可以實際載入模型，但為了演示先跳過\n                print(\"  💡 提示: 要使用LLM生成，請安裝並載入推薦模型\")\n                return True\n            return False\n        except Exception as e:\n            print(f\"  ⚠️ LLM載入失敗: {e}\")\n            return False\n    \n    def get_model_recommendations(self) -> str:\n        \"\"\"獲取2025年模型推薦\"\"\"\n        recommendations = \"\\n🏆 2025年TextSQL模型推薦:\\n\"\n        recommendations += \"=\" * 50 + \"\\n\"\n        \n        for category, info in self.recommended_models.items():\n            recommendations += f\"\\n📱 {category.upper()}:\\n\"\n            recommendations += f\"  • 模型: {info['name']}\\n\"\n            recommendations += f\"  • 大小: {info['size']}\\n\" \n            recommendations += f\"  • 記憶體: {info['memory']}\\n\"\n            recommendations += f\"  • 描述: {info['description']}\\n\"\n        \n        recommendations += f\"\\n💡 微調建議:\\n\"\n        recommendations += f\"  • 首選: QLoRA (4-bit量化 + LoRA)\\n\"\n        recommendations += f\"  • 備選: LoRA (8-bit量化)\\n\"\n        recommendations += f\"  • 高端: Full Fine-tuning\\n\"\n        \n        recommendations += f\"\\n🤔 是否需要微調?\\n\"\n        recommendations += f\"  • RAG方法: ❌ 無需微調，成本低\\n\"\n        recommendations += f\"  • 特定領域: ✅ 微調提升精度\\n\"\n        recommendations += f\"  • 通用場景: ❌ 預訓練模型已足夠\\n\"\n        \n        return recommendations\n    \n    def _load_sql_templates(self) -> Dict[str, str]:\n        \"\"\"加載SQL模板\"\"\"\n        return {\n            'select_all': \"SELECT * FROM {table};\",\n            'select_where': \"SELECT * FROM {table} WHERE {condition};\",\n            'count': \"SELECT COUNT(*) FROM {table};\",\n            'join': \"SELECT {columns} FROM {table1} t1 JOIN {table2} t2 ON {join_condition};\",\n            'group_by': \"SELECT {columns}, {aggregate} FROM {table} GROUP BY {group_columns};\",\n            'order_by': \"SELECT * FROM {table} ORDER BY {column} {direction};\",\n            'avg': \"SELECT AVG({column}) as avg_{column} FROM {table};\",\n            'sum': \"SELECT SUM({column}) as total_{column} FROM {table};\",\n            'max': \"SELECT MAX({column}) as max_{column} FROM {table};\",\n            'min': \"SELECT MIN({column}) as min_{column} FROM {table};\"\n        }\n    \n    def generate_sql(self, query: str, retrieval_results: Dict) -> Dict:\n        \"\"\"基於檢索結果生成SQL\"\"\"\n        timestamp = datetime.now().isoformat()\n        \n        # 1. 分析查詢意圖\n        intent = self._analyze_intent(query)\n        \n        # 2. 從檢索結果中提取最佳匹配\n        best_match = retrieval_results['similar_examples'][0] if retrieval_results['similar_examples'] else None\n        \n        # 3. 根據方法生成SQL\n        if self.model_approach == \"llm\" and self.llm_available:\n            generated_sql, method = self._llm_generation(query, retrieval_results, intent)\n        else:\n            generated_sql, method = self._rag_generation(query, retrieval_results, intent, best_match)\n        \n        # 4. 驗證SQL\n        is_valid, validation_error = self._validate_sql(generated_sql)\n        \n        # 5. 後處理\n        generated_sql = self._post_process_sql(generated_sql)\n        \n        return {\n            'query': query,\n            'generated_sql': generated_sql,\n            'method': method,\n            'intent': intent,\n            'is_valid': is_valid,\n            'validation_error': validation_error,\n            'best_match': best_match,\n            'timestamp': timestamp,\n            'model_approach': self.model_approach\n        }\n    \n    def _llm_generation(self, query: str, retrieval_results: Dict, intent: Dict) -> tuple:\n        \"\"\"使用LLM生成SQL (2025年方法)\"\"\"\n        # 構建提示詞\n        prompt = self._build_llm_prompt(query, retrieval_results, intent)\n        \n        # 這裡應該調用實際的LLM API\n        # generated_sql = llm_model.generate(prompt)\n        \n        # 為演示目的，返回基礎生成結果\n        generated_sql, _ = self._rag_generation(query, retrieval_results, intent, None)\n        return generated_sql, \"llm_enhanced\"\n    \n    def _rag_generation(self, query: str, retrieval_results: Dict, intent: Dict, best_match) -> tuple:\n        \"\"\"使用RAG方法生成SQL\"\"\"\n        if best_match and best_match['distance'] < 0.5:  # 高相似度\n            generated_sql = self._adapt_sql(best_match['metadata']['sql'], query, intent)\n            method = 'retrieval_based'\n        else:\n            generated_sql = self._template_based_generation(query, intent)\n            method = 'template_based'\n        \n        return generated_sql, method\n    \n    def _build_llm_prompt(self, query: str, retrieval_results: Dict, intent: Dict) -> str:\n        \"\"\"構建LLM提示詞 (2025年最佳實踐)\"\"\"\n        prompt = f\"\"\"你是一個專業的SQL生成助手。根據以下信息生成準確的SQL查詢。\n\n數據庫架構:\n{self.schema.get_schema_info()}\n\n用戶查詢: {query}\n\n相似示例:\n\"\"\"\n        \n        for i, example in enumerate(retrieval_results['similar_examples'][:2]):\n            prompt += f\"{i+1}. 問題: {example['text']}\\n\"\n            prompt += f\"   SQL: {example['metadata']['sql']}\\n\\n\"\n        \n        prompt += \"\"\"請生成對應的SQL查詢。要求:\n1. 語法正確\n2. 符合數據庫架構\n3. 回答用戶問題\n4. 只返回SQL語句，不要額外解釋\n\nSQL:\"\"\"\n        \n        return prompt\n    \n    def _analyze_intent(self, query: str) -> Dict:\n        \"\"\"增強的查詢意圖分析\"\"\"\n        query_lower = query.lower()\n        \n        intent = {\n            'action': 'select',\n            'tables': [],\n            'conditions': [],\n            'aggregation': None,\n            'sorting': None,\n            'limit': None,\n            'join_type': None\n        }\n        \n        # 識別表名\n        for table in self.schema.tables.keys():\n            if table in query_lower or self._table_synonyms(table, query_lower):\n                intent['tables'].append(table)\n        \n        # 識別聚合操作\n        aggregation_keywords = {\n            '統計': 'count', '計算': 'count', '總和': 'sum', \n            '平均': 'avg', '最大': 'max', '最小': 'min',\n            '數量': 'count'\n        }\n        for keyword, agg_type in aggregation_keywords.items():\n            if keyword in query_lower:\n                intent['aggregation'] = agg_type\n                break\n        \n        # 識別排序\n        if any(word in query_lower for word in ['最高', '最大', '最貴', '降序']):\n            intent['sorting'] = 'desc'\n        elif any(word in query_lower for word in ['最低', '最小', '最便宜', '升序']):\n            intent['sorting'] = 'asc'\n        \n        # 識別限制\n        if any(word in query_lower for word in ['前', '最', '第一']):\n            intent['limit'] = 1\n        \n        return intent\n    \n    def _table_synonyms(self, table: str, query: str) -> bool:\n        \"\"\"擴展的表名同義詞\"\"\"\n        synonyms = {\n            'users': ['用戶', '使用者', '會員', '客戶', '用户'],\n            'products': ['產品', '商品', '物品', '貨品', '产品'],\n            'orders': ['訂單', '訂購', '購買', '交易', '订单']\n        }\n        \n        if table in synonyms:\n            return any(syn in query for syn in synonyms[table])\n        return False\n    \n    def _adapt_sql(self, base_sql: str, query: str, intent: Dict) -> str:\n        \"\"\"智能SQL適配\"\"\"\n        adapted_sql = base_sql\n        \n        # 根據意圖調整SQL\n        if intent['limit'] and 'LIMIT' not in adapted_sql.upper():\n            adapted_sql = adapted_sql.rstrip(';') + f\" LIMIT {intent['limit']};\"\n        \n        return adapted_sql\n    \n    def _template_based_generation(self, query: str, intent: Dict) -> str:\n        \"\"\"增強的模板生成\"\"\"\n        if not intent['tables']:\n            return \"SELECT 'No table identified' as error;\"\n        \n        table = intent['tables'][0]\n        \n        # 聚合查詢\n        if intent['aggregation']:\n            agg_func = intent['aggregation'].upper()\n            if agg_func == 'COUNT':\n                sql = f\"SELECT COUNT(*) as total_count FROM {table};\"\n            else:\n                numeric_columns = self._get_numeric_columns(table)\n                if numeric_columns:\n                    column = numeric_columns[0]\n                    sql = f\"SELECT {agg_func}({column}) as {agg_func.lower()}_{column} FROM {table};\"\n                else:\n                    sql = f\"SELECT COUNT(*) as total_count FROM {table};\"\n        \n        # 排序查詢\n        elif intent['sorting']:\n            numeric_columns = self._get_numeric_columns(table)\n            if numeric_columns:\n                column = numeric_columns[0]\n                direction = intent['sorting'].upper()\n                sql = f\"SELECT * FROM {table} ORDER BY {column} {direction};\"\n                if intent['limit']:\n                    sql = sql.rstrip(';') + f\" LIMIT {intent['limit']};\"\n            else:\n                sql = f\"SELECT * FROM {table};\"\n        \n        # 基本查詢\n        else:\n            sql = f\"SELECT * FROM {table};\"\n            if intent['limit']:\n                sql = sql.rstrip(';') + f\" LIMIT {intent['limit']};\"\n        \n        return sql\n    \n    def _get_numeric_columns(self, table: str) -> List[str]:\n        \"\"\"獲取數值列\"\"\"\n        if table not in self.schema.tables:\n            return []\n        \n        numeric_types = ['INT', 'DECIMAL', 'FLOAT', 'DOUBLE', 'NUMERIC']\n        numeric_columns = []\n        \n        for column, column_type in self.schema.tables[table].items():\n            if any(num_type in column_type.upper() for num_type in numeric_types):\n                numeric_columns.append(column)\n        \n        return numeric_columns\n    \n    def _post_process_sql(self, sql: str) -> str:\n        \"\"\"SQL後處理\"\"\"\n        # 確保以分號結尾\n        sql = sql.strip()\n        if not sql.endswith(';'):\n            sql += ';'\n        \n        # 格式化\n        sql = re.sub(r'\\s+', ' ', sql)  # 統一空格\n        \n        return sql\n    \n    def _validate_sql(self, sql: str) -> Tuple[bool, str]:\n        \"\"\"增強的SQL驗證\"\"\"\n        try:\n            parsed = sqlparse.parse(sql)[0]\n            if parsed.tokens:\n                # 基本語法檢查\n                sql_upper = sql.upper()\n                if not any(keyword in sql_upper for keyword in ['SELECT', 'INSERT', 'UPDATE', 'DELETE']):\n                    return False, \"不包含有效的SQL關鍵字\"\n                \n                return True, \"\"\n            else:\n                return False, \"空的SQL語句\"\n        except Exception as e:\n            return False, str(e)\n\n# 初始化增強版SQL生成器\nsql_generator = SQLGenerator(schema, model_approach=\"rag\")\n\n# 顯示模型推薦\nprint(sql_generator.get_model_recommendations())\n\n# 測試增強版SQL生成\ntest_queries = [\n    \"顯示所有產品信息\",\n    \"找出最貴的產品\", \n    \"統計用戶數量\",\n    \"計算產品平均價格\",\n    \"顯示前3個最貴的產品\"\n]\n\nprint(f\"\\n🧪 增強版SQL生成測試:\")\nprint(\"=\" * 50)\n\nfor query in test_queries:\n    retrieval_results = retriever.retrieve(query)\n    generation_results = sql_generator.generate_sql(query, retrieval_results)\n    \n    print(f\"\\n🔍 查詢: {query}\")\n    print(f\"📝 生成SQL: {generation_results['generated_sql']}\")\n    print(f\"🔧 方法: {generation_results['method']}\")\n    print(f\"🎯 意圖: {generation_results['intent']}\")\n    print(f\"✅ 有效: {generation_results['is_valid']}\")\n    \n    if generation_results['validation_error']:\n        print(f\"❌ 錯誤: {generation_results['validation_error']}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 完整流水線\n",
    "\n",
    "### 整合所有組件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 完整的TextSQL RAG流水線\n",
    "class TextSQLRAGPipeline:\n",
    "    def __init__(self, \n",
    "                 embedder: TextEmbedder,\n",
    "                 vector_index: VectorIndex,\n",
    "                 schema: DatabaseSchema,\n",
    "                 retriever: SemanticRetriever,\n",
    "                 sql_generator: SQLGenerator):\n",
    "        \"\"\"初始化完整的RAG流水線\"\"\"\n",
    "        self.embedder = embedder\n",
    "        self.vector_index = vector_index\n",
    "        self.schema = schema\n",
    "        self.retriever = retriever\n",
    "        self.sql_generator = sql_generator\n",
    "        self.query_history = []\n",
    "    \n",
    "    def process_query(self, natural_language_query: str, execute_sql: bool = False) -> Dict:\n",
    "        \"\"\"處理自然語言查詢的完整流程\"\"\"\n",
    "        print(f\"\\n處理查詢: {natural_language_query}\")\n",
    "        \n",
    "        # 第1步：檢索相關示例\n",
    "        print(\"第1步：檢索相關示例...\")\n",
    "        retrieval_results = self.retriever.retrieve(natural_language_query)\n",
    "        \n",
    "        # 第2步：生成SQL\n",
    "        print(\"第2步：生成SQL...\")\n",
    "        generation_results = self.sql_generator.generate_sql(natural_language_query, retrieval_results)\n",
    "        \n",
    "        # 第3步：執行SQL（可選）\n",
    "        execution_results = None\n",
    "        if execute_sql and generation_results['is_valid']:\n",
    "            print(\"第3步：執行SQL...\")\n",
    "            execution_results = self._execute_sql(generation_results['generated_sql'])\n",
    "        \n",
    "        # 第4步：構建完整結果\n",
    "        complete_results = {\n",
    "            'natural_language_query': natural_language_query,\n",
    "            'retrieval_results': retrieval_results,\n",
    "            'generation_results': generation_results,\n",
    "            'execution_results': execution_results,\n",
    "            'pipeline_success': generation_results['is_valid'],\n",
    "            'timestamp': pd.Timestamp.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        # 第5步：記錄查詢歷史\n",
    "        self.query_history.append(complete_results)\n",
    "        \n",
    "        return complete_results\n",
    "    \n",
    "    def _execute_sql(self, sql: str) -> Dict:\n",
    "        \"\"\"執行SQL查詢\"\"\"\n",
    "        try:\n",
    "            conn = sqlite3.connect('sample_ecommerce.db')\n",
    "            \n",
    "            # 執行查詢\n",
    "            result_df = pd.read_sql_query(sql, conn)\n",
    "            \n",
    "            conn.close()\n",
    "            \n",
    "            return {\n",
    "                'success': True,\n",
    "                'data': result_df.to_dict('records'),\n",
    "                'row_count': len(result_df),\n",
    "                'columns': list(result_df.columns),\n",
    "                'error': None\n",
    "            }\n",
    "        \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'success': False,\n",
    "                'data': None,\n",
    "                'row_count': 0,\n",
    "                'columns': [],\n",
    "                'error': str(e)\n",
    "            }\n",
    "    \n",
    "    def explain_results(self, results: Dict) -> str:\n",
    "        \"\"\"解釋處理結果\"\"\"\n",
    "        explanation = f\"\\n{'='*50}\\n\"\n",
    "        explanation += f\"查詢: {results['natural_language_query']}\\n\"\n",
    "        explanation += f\"{'='*50}\\n\"\n",
    "        \n",
    "        # 檢索階段\n",
    "        explanation += \"\\n🔍 檢索階段:\\n\"\n",
    "        retrieval = results['retrieval_results']\n",
    "        explanation += f\"找到 {len(retrieval['similar_examples'])} 個相似示例\\n\"\n",
    "        explanation += f\"檢索分數: {retrieval['retrieval_score']:.3f}\\n\"\n",
    "        \n",
    "        if retrieval['similar_examples']:\n",
    "            best_match = retrieval['similar_examples'][0]\n",
    "            explanation += f\"最佳匹配: {best_match['text']} (相似度: {1-best_match['distance']:.3f})\\n\"\n",
    "        \n",
    "        # 生成階段\n",
    "        explanation += \"\\n⚙️ 生成階段:\\n\"\n",
    "        generation = results['generation_results']\n",
    "        explanation += f\"生成方法: {generation['method']}\\n\"\n",
    "        explanation += f\"生成的SQL: {generation['generated_sql']}\\n\"\n",
    "        explanation += f\"SQL有效性: {generation['is_valid']}\\n\"\n",
    "        \n",
    "        # 執行階段\n",
    "        if results['execution_results']:\n",
    "            explanation += \"\\n🚀 執行階段:\\n\"\n",
    "            execution = results['execution_results']\n",
    "            if execution['success']:\n",
    "                explanation += f\"執行成功，返回 {execution['row_count']} 行數據\\n\"\n",
    "                if execution['data']:\n",
    "                    explanation += \"前幾行數據:\\n\"\n",
    "                    for i, row in enumerate(execution['data'][:3]):\n",
    "                        explanation += f\"  {i+1}: {row}\\n\"\n",
    "            else:\n",
    "                explanation += f\"執行失敗: {execution['error']}\\n\"\n",
    "        \n",
    "        explanation += f\"\\n✅ 流水線成功: {results['pipeline_success']}\\n\"\n",
    "        \n",
    "        return explanation\n",
    "    \n",
    "    def get_pipeline_stats(self) -> Dict:\n",
    "        \"\"\"獲取流水線統計信息\"\"\"\n",
    "        if not self.query_history:\n",
    "            return {'total_queries': 0}\n",
    "        \n",
    "        successful_queries = sum(1 for q in self.query_history if q['pipeline_success'])\n",
    "        \n",
    "        return {\n",
    "            'total_queries': len(self.query_history),\n",
    "            'successful_queries': successful_queries,\n",
    "            'success_rate': successful_queries / len(self.query_history),\n",
    "            'avg_retrieval_score': np.mean([q['retrieval_results']['retrieval_score'] \n",
    "                                          for q in self.query_history])\n",
    "        }\n",
    "\n",
    "# 初始化完整流水線\n",
    "pipeline = TextSQLRAGPipeline(\n",
    "    embedder=embedder,\n",
    "    vector_index=vector_index,\n",
    "    schema=schema,\n",
    "    retriever=retriever,\n",
    "    sql_generator=sql_generator\n",
    ")\n",
    "\n",
    "print(\"✅ TextSQL RAG 流水線初始化完成！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 流水線演示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 流水線演示\n",
    "demo_queries = [\n",
    "    \"顯示所有用戶的基本信息\",\n",
    "    \"查找價格最高的產品\",\n",
    "    \"統計每個類別有多少產品\",\n",
    "    \"顯示張三購買的所有商品\",\n",
    "    \"找出還沒有下過訂單的用戶\"\n",
    "]\n",
    "\n",
    "print(\"🚀 開始流水線演示\\n\")\n",
    "\n",
    "for i, query in enumerate(demo_queries, 1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"演示 {i}/{len(demo_queries)}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # 處理查詢\n",
    "    results = pipeline.process_query(query, execute_sql=True)\n",
    "    \n",
    "    # 顯示結果\n",
    "    print(pipeline.explain_results(results))\n",
    "    \n",
    "    # 暫停一下讓輸出更清晰\n",
    "    import time\n",
    "    time.sleep(1)\n",
    "\n",
    "# 顯示整體統計\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"流水線統計\")\n",
    "print(\"=\"*60)\n",
    "stats = pipeline.get_pipeline_stats()\n",
    "for key, value in stats.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 評估與優化\n",
    "\n",
    "### 評估指標"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 評估系統\n",
    "class RAGEvaluator:\n",
    "    def __init__(self, pipeline: TextSQLRAGPipeline):\n",
    "        \"\"\"初始化評估器\"\"\"\n",
    "        self.pipeline = pipeline\n",
    "    \n",
    "    def evaluate_retrieval(self, test_cases: List[Dict]) -> Dict:\n",
    "        \"\"\"評估檢索性能\"\"\"\n",
    "        retrieval_scores = []\n",
    "        precision_scores = []\n",
    "        \n",
    "        for test_case in test_cases:\n",
    "            query = test_case['natural_language']\n",
    "            expected_sql = test_case['sql']\n",
    "            \n",
    "            # 檢索相關示例\n",
    "            retrieval_results = self.pipeline.retriever.retrieve(query)\n",
    "            \n",
    "            # 計算檢索分數\n",
    "            retrieval_scores.append(retrieval_results['retrieval_score'])\n",
    "            \n",
    "            # 計算精確度（檢索結果中是否包含正確答案）\n",
    "            precision = self._calculate_precision(retrieval_results, expected_sql)\n",
    "            precision_scores.append(precision)\n",
    "        \n",
    "        return {\n",
    "            'avg_retrieval_score': np.mean(retrieval_scores),\n",
    "            'avg_precision': np.mean(precision_scores),\n",
    "            'retrieval_scores': retrieval_scores,\n",
    "            'precision_scores': precision_scores\n",
    "        }\n",
    "    \n",
    "    def evaluate_generation(self, test_cases: List[Dict]) -> Dict:\n",
    "        \"\"\"評估SQL生成性能\"\"\"\n",
    "        exact_match_scores = []\n",
    "        syntax_valid_scores = []\n",
    "        semantic_similarity_scores = []\n",
    "        \n",
    "        for test_case in test_cases:\n",
    "            query = test_case['natural_language']\n",
    "            expected_sql = test_case['sql']\n",
    "            \n",
    "            # 生成SQL\n",
    "            results = self.pipeline.process_query(query, execute_sql=False)\n",
    "            generated_sql = results['generation_results']['generated_sql']\n",
    "            \n",
    "            # 精確匹配\n",
    "            exact_match = self._normalize_sql(generated_sql) == self._normalize_sql(expected_sql)\n",
    "            exact_match_scores.append(exact_match)\n",
    "            \n",
    "            # 語法有效性\n",
    "            syntax_valid = results['generation_results']['is_valid']\n",
    "            syntax_valid_scores.append(syntax_valid)\n",
    "            \n",
    "            # 語義相似度（簡化實現）\n",
    "            semantic_sim = self._calculate_semantic_similarity(generated_sql, expected_sql)\n",
    "            semantic_similarity_scores.append(semantic_sim)\n",
    "        \n",
    "        return {\n",
    "            'exact_match_rate': np.mean(exact_match_scores),\n",
    "            'syntax_valid_rate': np.mean(syntax_valid_scores),\n",
    "            'avg_semantic_similarity': np.mean(semantic_similarity_scores),\n",
    "            'exact_match_scores': exact_match_scores,\n",
    "            'syntax_valid_scores': syntax_valid_scores,\n",
    "            'semantic_similarity_scores': semantic_similarity_scores\n",
    "        }\n",
    "    \n",
    "    def evaluate_end_to_end(self, test_cases: List[Dict]) -> Dict:\n",
    "        \"\"\"端到端評估\"\"\"\n",
    "        execution_success_scores = []\n",
    "        result_accuracy_scores = []\n",
    "        \n",
    "        for test_case in test_cases:\n",
    "            query = test_case['natural_language']\n",
    "            expected_sql = test_case['sql']\n",
    "            \n",
    "            # 執行完整流水線\n",
    "            results = self.pipeline.process_query(query, execute_sql=True)\n",
    "            \n",
    "            # 執行成功率\n",
    "            execution_success = (\n",
    "                results['execution_results'] is not None and \n",
    "                results['execution_results']['success']\n",
    "            )\n",
    "            execution_success_scores.append(execution_success)\n",
    "            \n",
    "            # 結果准確性（通過執行期望SQL進行比較）\n",
    "            if execution_success:\n",
    "                result_accuracy = self._compare_execution_results(\n",
    "                    results['generation_results']['generated_sql'],\n",
    "                    expected_sql\n",
    "                )\n",
    "                result_accuracy_scores.append(result_accuracy)\n",
    "            else:\n",
    "                result_accuracy_scores.append(0.0)\n",
    "        \n",
    "        return {\n",
    "            'execution_success_rate': np.mean(execution_success_scores),\n",
    "            'result_accuracy_rate': np.mean(result_accuracy_scores),\n",
    "            'execution_success_scores': execution_success_scores,\n",
    "            'result_accuracy_scores': result_accuracy_scores\n",
    "        }\n",
    "    \n",
    "    def _calculate_precision(self, retrieval_results: Dict, expected_sql: str) -> float:\n",
    "        \"\"\"計算檢索精確度\"\"\"\n",
    "        similar_examples = retrieval_results['similar_examples']\n",
    "        if not similar_examples:\n",
    "            return 0.0\n",
    "        \n",
    "        # 檢查是否有檢索結果與期望SQL相似\n",
    "        for example in similar_examples:\n",
    "            if self._sql_similarity(example['metadata']['sql'], expected_sql) > 0.8:\n",
    "                return 1.0\n",
    "        \n",
    "        return 0.0\n",
    "    \n",
    "    def _normalize_sql(self, sql: str) -> str:\n",
    "        \"\"\"標準化SQL字符串\"\"\"\n",
    "        # 移除多餘空格，轉換為小寫\n",
    "        return ' '.join(sql.lower().split())\n",
    "    \n",
    "    def _calculate_semantic_similarity(self, sql1: str, sql2: str) -> float:\n",
    "        \"\"\"計算SQL語義相似度\"\"\"\n",
    "        # 簡化實現：基於關鍵詞重疊\n",
    "        words1 = set(self._normalize_sql(sql1).split())\n",
    "        words2 = set(self._normalize_sql(sql2).split())\n",
    "        \n",
    "        if not words1 and not words2:\n",
    "            return 1.0\n",
    "        if not words1 or not words2:\n",
    "            return 0.0\n",
    "        \n",
    "        intersection = words1.intersection(words2)\n",
    "        union = words1.union(words2)\n",
    "        \n",
    "        return len(intersection) / len(union)\n",
    "    \n",
    "    def _sql_similarity(self, sql1: str, sql2: str) -> float:\n",
    "        \"\"\"計算SQL相似度\"\"\"\n",
    "        return self._calculate_semantic_similarity(sql1, sql2)\n",
    "    \n",
    "    def _compare_execution_results(self, generated_sql: str, expected_sql: str) -> float:\n",
    "        \"\"\"比較執行結果\"\"\"\n",
    "        try:\n",
    "            conn = sqlite3.connect('sample_ecommerce.db')\n",
    "            \n",
    "            # 執行兩個SQL\n",
    "            result1 = pd.read_sql_query(generated_sql, conn)\n",
    "            result2 = pd.read_sql_query(expected_sql, conn)\n",
    "            \n",
    "            conn.close()\n",
    "            \n",
    "            # 比較結果\n",
    "            if result1.equals(result2):\n",
    "                return 1.0\n",
    "            elif len(result1) == len(result2) and len(result1.columns) == len(result2.columns):\n",
    "                return 0.5  # 部分匹配\n",
    "            else:\n",
    "                return 0.0\n",
    "                \n",
    "        except Exception:\n",
    "            return 0.0\n",
    "    \n",
    "    def generate_evaluation_report(self, test_cases: List[Dict]) -> str:\n",
    "        \"\"\"生成評估報告\"\"\"\n",
    "        retrieval_eval = self.evaluate_retrieval(test_cases)\n",
    "        generation_eval = self.evaluate_generation(test_cases)\n",
    "        e2e_eval = self.evaluate_end_to_end(test_cases)\n",
    "        \n",
    "        report = \"\\n\" + \"=\"*50 + \"\\n\"\n",
    "        report += \"TextSQL RAG Pipeline 評估報告\\n\"\n",
    "        report += \"=\"*50 + \"\\n\"\n",
    "        \n",
    "        report += \"\\n📊 檢索性能:\\n\"\n",
    "        report += f\"  平均檢索分數: {retrieval_eval['avg_retrieval_score']:.3f}\\n\"\n",
    "        report += f\"  平均精確度: {retrieval_eval['avg_precision']:.3f}\\n\"\n",
    "        \n",
    "        report += \"\\n🔧 生成性能:\\n\"\n",
    "        report += f\"  精確匹配率: {generation_eval['exact_match_rate']:.3f}\\n\"\n",
    "        report += f\"  語法有效率: {generation_eval['syntax_valid_rate']:.3f}\\n\"\n",
    "        report += f\"  平均語義相似度: {generation_eval['avg_semantic_similarity']:.3f}\\n\"\n",
    "        \n",
    "        report += \"\\n🚀 端到端性能:\\n\"\n",
    "        report += f\"  執行成功率: {e2e_eval['execution_success_rate']:.3f}\\n\"\n",
    "        report += f\"  結果准確率: {e2e_eval['result_accuracy_rate']:.3f}\\n\"\n",
    "        \n",
    "        # 總體評分\n",
    "        overall_score = np.mean([\n",
    "            retrieval_eval['avg_retrieval_score'],\n",
    "            generation_eval['syntax_valid_rate'],\n",
    "            e2e_eval['execution_success_rate']\n",
    "        ])\n",
    "        \n",
    "        report += f\"\\n⭐ 總體評分: {overall_score:.3f}\\n\"\n",
    "        \n",
    "        return report\n",
    "\n",
    "# 創建評估器\n",
    "evaluator = RAGEvaluator(pipeline)\n",
    "\n",
    "# 運行評估\n",
    "print(\"🔍 開始評估流水線性能...\")\n",
    "evaluation_report = evaluator.generate_evaluation_report(training_data)\n",
    "print(evaluation_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 實際應用案例\n",
    "\n",
    "### 互動式查詢界面"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 互動式查詢界面\n",
    "class InteractiveQueryInterface:\n",
    "    def __init__(self, pipeline: TextSQLRAGPipeline):\n",
    "        \"\"\"初始化互動式界面\"\"\"\n",
    "        self.pipeline = pipeline\n",
    "        self.session_history = []\n",
    "    \n",
    "    def start_session(self):\n",
    "        \"\"\"開始互動會話\"\"\"\n",
    "        print(\"\\n🎯 歡迎使用 TextSQL RAG 查詢系統！\")\n",
    "        print(\"輸入自然語言查詢，系統將生成對應的SQL語句並執行\")\n",
    "        print(\"輸入 'help' 查看幫助，輸入 'quit' 退出系統\\n\")\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                user_input = input(\"🔍 請輸入您的查詢: \").strip()\n",
    "                \n",
    "                if user_input.lower() == 'quit':\n",
    "                    print(\"👋 感謝使用，再見！\")\n",
    "                    break\n",
    "                elif user_input.lower() == 'help':\n",
    "                    self._show_help()\n",
    "                    continue\n",
    "                elif user_input.lower() == 'history':\n",
    "                    self._show_history()\n",
    "                    continue\n",
    "                elif user_input.lower() == 'schema':\n",
    "                    self._show_schema()\n",
    "                    continue\n",
    "                elif not user_input:\n",
    "                    continue\n",
    "                \n",
    "                # 處理查詢\n",
    "                self._process_interactive_query(user_input)\n",
    "                \n",
    "            except KeyboardInterrupt:\n",
    "                print(\"\\n👋 用戶中斷，再見！\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"❌ 發生錯誤: {e}\")\n",
    "    \n",
    "    def _process_interactive_query(self, query: str):\n",
    "        \"\"\"處理互動查詢\"\"\"\n",
    "        print(f\"\\n⏳ 處理中...\")\n",
    "        \n",
    "        # 執行查詢\n",
    "        results = self.pipeline.process_query(query, execute_sql=True)\n",
    "        \n",
    "        # 記錄會話歷史\n",
    "        self.session_history.append(results)\n",
    "        \n",
    "        # 顯示結果\n",
    "        self._display_results(results)\n",
    "    \n",
    "    def _display_results(self, results: Dict):\n",
    "        \"\"\"顯示查詢結果\"\"\"\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(f\"📝 查詢: {results['natural_language_query']}\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        # 顯示生成的SQL\n",
    "        generation = results['generation_results']\n",
    "        print(f\"\\n💻 生成的SQL:\")\n",
    "        print(f\"```sql\\n{generation['generated_sql']}\\n```\")\n",
    "        \n",
    "        print(f\"\\n🔧 生成方法: {generation['method']}\")\n",
    "        print(f\"✅ SQL有效性: {generation['is_valid']}\")\n",
    "        \n",
    "        # 顯示執行結果\n",
    "        if results['execution_results']:\n",
    "            execution = results['execution_results']\n",
    "            if execution['success']:\n",
    "                print(f\"\\n🎯 執行結果: 成功返回 {execution['row_count']} 行數據\")\n",
    "                \n",
    "                if execution['data']:\n",
    "                    # 將結果格式化為表格\n",
    "                    df = pd.DataFrame(execution['data'])\n",
    "                    print(\"\\n📊 查詢結果:\")\n",
    "                    print(df.to_string(index=False))\n",
    "                else:\n",
    "                    print(\"\\n📊 查詢結果: 無數據返回\")\n",
    "            else:\n",
    "                print(f\"\\n❌ 執行失敗: {execution['error']}\")\n",
    "        \n",
    "        # 顯示檢索信息\n",
    "        retrieval = results['retrieval_results']\n",
    "        if retrieval['similar_examples']:\n",
    "            best_match = retrieval['similar_examples'][0]\n",
    "            print(f\"\\n🔍 最佳匹配示例: {best_match['text']}\")\n",
    "            print(f\"📈 相似度: {1-best_match['distance']:.3f}\")\n",
    "    \n",
    "    def _show_help(self):\n",
    "        \"\"\"顯示幫助信息\"\"\"\n",
    "        help_text = \"\"\"\n",
    "📚 使用幫助:\n",
    "\n",
    "基本查詢示例:\n",
    "  • \"顯示所有用戶\" - 查詢用戶表\n",
    "  • \"找出最貴的產品\" - 按價格排序\n",
    "  • \"統計每個類別的產品數量\" - 聚合查詢\n",
    "  • \"顯示張三的所有訂單\" - 聯合查詢\n",
    "\n",
    "特殊命令:\n",
    "  • help - 顯示此幫助\n",
    "  • history - 顯示查詢歷史\n",
    "  • schema - 顯示數據庫結構\n",
    "  • quit - 退出系統\n",
    "\n",
    "💡 提示: 盡量使用自然語言描述您的查詢需求！\n",
    "\"\"\"\n",
    "        print(help_text)\n",
    "    \n",
    "    def _show_history(self):\n",
    "        \"\"\"顯示查詢歷史\"\"\"\n",
    "        if not self.session_history:\n",
    "            print(\"\\n📝 暫無查詢歷史\")\n",
    "            return\n",
    "        \n",
    "        print(f\"\\n📝 查詢歷史 (共 {len(self.session_history)} 條):\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        for i, result in enumerate(self.session_history[-5:], 1):  # 只顯示最近5條\n",
    "            query = result['natural_language_query']\n",
    "            sql = result['generation_results']['generated_sql']\n",
    "            success = result['pipeline_success']\n",
    "            status = \"✅\" if success else \"❌\"\n",
    "            \n",
    "            print(f\"{i}. {status} {query}\")\n",
    "            print(f\"   SQL: {sql[:50]}{'...' if len(sql) > 50 else ''}\")\n",
    "            print()\n",
    "    \n",
    "    def _show_schema(self):\n",
    "        \"\"\"顯示數據庫結構\"\"\"\n",
    "        print(\"\\n🗃️ 數據庫結構:\")\n",
    "        print(self.pipeline.schema.get_schema_info())\n",
    "\n",
    "# 創建互動界面\n",
    "interface = InteractiveQueryInterface(pipeline)\n",
    "\n",
    "# 演示一些自動查詢（而不是真正的互動模式）\n",
    "print(\"\\n🎯 TextSQL RAG 系統演示\")\n",
    "print(\"以下是一些自動執行的查詢示例:\\n\")\n",
    "\n",
    "demo_queries = [\n",
    "    \"查看所有用戶的信息\",\n",
    "    \"找出價格最高的三個產品\",\n",
    "    \"統計每個用戶的訂單總數\"\n",
    "]\n",
    "\n",
    "for query in demo_queries:\n",
    "    print(f\"🔍 查詢: {query}\")\n",
    "    interface._process_interactive_query(query)\n",
    "    print(\"\\n\" + \"-\"*60 + \"\\n\")\n",
    "\n",
    "print(\"\\n✨ 演示完成！要開始真正的互動模式，請取消註釋下面的代碼：\")\n",
    "print(\"# interface.start_session()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 總結與優化建議\n",
    "\n",
    "### 學習總結\n",
    "\n",
    "通過本筆記本，我們完整實現了一個 TextSQL RAG 流水線，包括：\n",
    "\n",
    "1. **數據預處理**: 創建示例數據庫和訓練數據\n",
    "2. **向量化**: 使用句子嵌入模型將文本轉換為向量\n",
    "3. **檢索系統**: 基於語義相似度檢索相關示例\n",
    "4. **SQL生成**: 結合檢索結果和模板生成SQL\n",
    "5. **完整流水線**: 整合所有組件\n",
    "6. **評估系統**: 多維度評估性能\n",
    "7. **實際應用**: 互動式查詢界面\n",
    "\n",
    "### 優化建議\n",
    "\n",
    "1. **模型優化**:\n",
    "   - 使用更強大的嵌入模型（如 OpenAI embeddings）\n",
    "   - 實現微調機制以適應特定領域\n",
    "\n",
    "2. **檢索優化**:\n",
    "   - 實現混合檢索（語義+關鍵詞）\n",
    "   - 添加重排序機制\n",
    "\n",
    "3. **生成優化**:\n",
    "   - 集成大語言模型（如 GPT-4）\n",
    "   - 實現更複雜的SQL模板\n",
    "\n",
    "4. **系統優化**:\n",
    "   - 添加緩存機制\n",
    "   - 實現分佈式部署\n",
    "   - 增強錯誤處理\n",
    "\n",
    "### 後續學習方向\n",
    "\n",
    "1. 深入學習 Transformer 架構\n",
    "2. 探索更高級的 RAG 技術\n",
    "3. 學習數據庫優化技術\n",
    "4. 研究多模態 RAG 系統"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存流水線狀態和結果\n",
    "import pickle\n",
    "\n",
    "def save_pipeline_state():\n",
    "    \"\"\"保存流水線狀態\"\"\"\n",
    "    state = {\n",
    "        'query_history': pipeline.query_history,\n",
    "        'pipeline_stats': pipeline.get_pipeline_stats(),\n",
    "        'training_data': training_data,\n",
    "        'schema_info': schema.get_schema_info()\n",
    "    }\n",
    "    \n",
    "    with open('pipeline_state.pkl', 'wb') as f:\n",
    "        pickle.dump(state, f)\n",
    "    \n",
    "    # 也保存為JSON格式便於查看\n",
    "    json_state = {\n",
    "        'pipeline_stats': pipeline.get_pipeline_stats(),\n",
    "        'training_data': training_data,\n",
    "        'schema_info': schema.get_schema_info()\n",
    "    }\n",
    "    \n",
    "    with open('pipeline_summary.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(json_state, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(\"✅ 流水線狀態已保存\")\n",
    "\n",
    "# 保存狀態\n",
    "save_pipeline_state()\n",
    "\n",
    "print(\"\\n🎉 TextSQL RAG Pipeline 學習筆記完成！\")\n",
    "print(\"\\n📝 學習成果:\")\n",
    "print(f\"  • 處理了 {len(pipeline.query_history)} 個查詢\")\n",
    "print(f\"  • 成功率: {pipeline.get_pipeline_stats().get('success_rate', 0):.1%}\")\n",
    "print(f\"  • 平均檢索分數: {pipeline.get_pipeline_stats().get('avg_retrieval_score', 0):.3f}\")\n",
    "print(\"\\n🚀 您現在可以:\")\n",
    "print(\"  1. 在 Kaggle 環境中運行此筆記本\")\n",
    "print(\"  2. 修改數據庫結構和訓練數據\")\n",
    "print(\"  3. 優化檢索和生成算法\")\n",
    "print(\"  4. 集成更強大的語言模型\")\n",
    "print(\"\\n💡 建議下一步: 嘗試在真實數據集上測試此系統！\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}