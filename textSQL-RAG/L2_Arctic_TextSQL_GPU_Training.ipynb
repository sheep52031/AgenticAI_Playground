{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸš€ Arctic-Text2SQL-R1 Kaggle GPU å¾®èª¿å¯¦æˆ°\n",
    "\n",
    "## ğŸ“‹ å°ˆæ¡ˆæ¦‚è¿°\n",
    "\n",
    "æœ¬ç­†è¨˜æœ¬å¯¦ç¾äº†åŸºæ–¼ **Snowflake Arctic-Text2SQL-R1 7B** æ¨¡å‹çš„ Text2SQL RAG ç³»çµ±ï¼Œå°ˆé–€é‡å° **Kaggle GPU ç’°å¢ƒ** å„ªåŒ–ã€‚\n",
    "\n",
    "### ğŸ¯ æ ¸å¿ƒç‰¹è‰²\n",
    "- **æ¨¡å‹**: Arctic-Text2SQL-R1 7Bï¼ˆSOTA å°æ¨¡å‹ï¼ŒBIRD ExecAcc 57%ï¼‰\n",
    "- **å¾®èª¿æ–¹æ³•**: QLoRA 4-bit é‡åŒ– + LoRA é©é…å™¨\n",
    "- **è¨˜æ†¶é«”éœ€æ±‚**: â‰ˆ5GB VRAMï¼ˆé©åˆ Kaggle P100/T4ï¼‰\n",
    "- **æ–·é»çºŒè¨“**: å®Œæ•´çš„æª¢æŸ¥é»ç®¡ç†ç³»çµ±\n",
    "- **è³‡æ–™é›†**: SynSQL-2.5M é«˜è³ªé‡åˆæˆè³‡æ–™\n",
    "\n",
    "### ğŸ“Š é æœŸæ•ˆæœ\n",
    "- **Spider-Dev**: EM 72% / Exec Acc 86%\n",
    "- **BIRD-Dev**: Exec Acc ~57%\n",
    "- **è¨“ç·´æ™‚é–“**: ç´„ 4-6 å°æ™‚ï¼ˆKaggle P100ï¼‰\n",
    "\n",
    "### ğŸ”§ æŠ€è¡“æ¶æ§‹\n",
    "```\n",
    "è‡ªç„¶èªè¨€æŸ¥è©¢ â†’ Schemaæª¢ç´¢ â†’ Arcticç”Ÿæˆ â†’ SQLåŸ·è¡Œé©—è­‰ â†’ è‡ªåé¥‹ä¿®æ­£\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ› ï¸ ç¬¬ä¸€éƒ¨åˆ†ï¼šç’°å¢ƒè¨­ç½®èˆ‡ä¾è³´å®‰è£\n",
    "\n",
    "### é‡è¦æé†’\n",
    "- ç¢ºä¿ Kaggle ç­†è¨˜æœ¬å·²å•Ÿç”¨ **GPU** åŠ é€Ÿå™¨\n",
    "- å»ºè­°ä½¿ç”¨ **P100** æˆ– **T4** GPU\n",
    "- é ä¼°è¨“ç·´æ™‚é–“ï¼š4-6 å°æ™‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” ç³»çµ±ç’°å¢ƒæª¢æŸ¥\n",
      "==================================================\n",
      "âœ… GPU å¯ç”¨: Tesla P100-PCIE-16GB\n",
      "ğŸ“Š GPU è¨˜æ†¶é«”: 15.9 GB\n",
      "ğŸ¯ è¨˜æ†¶é«”å……è¶³ï¼Œé©åˆ 7B æ¨¡å‹å¾®èª¿\n",
      "ğŸ Python ç‰ˆæœ¬: 3.11.13\n",
      "ğŸ† Kaggle ç’°å¢ƒ: Interactive\n"
     ]
    }
   ],
   "source": [
    "# æª¢æŸ¥ GPU å¯ç”¨æ€§èˆ‡è¦æ ¼\n",
    "import torch\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "print(\"ğŸ” ç³»çµ±ç’°å¢ƒæª¢æŸ¥\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# GPU æª¢æŸ¥\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    print(f\"âœ… GPU å¯ç”¨: {gpu_name}\")\n",
    "    print(f\"ğŸ“Š GPU è¨˜æ†¶é«”: {gpu_memory:.1f} GB\")\n",
    "    \n",
    "    # æª¢æŸ¥æ˜¯å¦é©åˆè¨“ç·´\n",
    "    if gpu_memory >= 15:\n",
    "        print(\"ğŸ¯ è¨˜æ†¶é«”å……è¶³ï¼Œé©åˆ 7B æ¨¡å‹å¾®èª¿\")\n",
    "    elif gpu_memory >= 8:\n",
    "        print(\"âš ï¸  è¨˜æ†¶é«”ä¸­ç­‰ï¼Œå»ºè­°ä½¿ç”¨æœ€æ¿€é€²çš„é‡åŒ–è¨­ç½®\")\n",
    "    else:\n",
    "        print(\"âŒ è¨˜æ†¶é«”ä¸è¶³ï¼Œå»ºè­°ä½¿ç”¨æ›´å°çš„æ¨¡å‹\")\n",
    "else:\n",
    "    print(\"âŒ æœªæª¢æ¸¬åˆ° GPUï¼Œè«‹å•Ÿç”¨ GPU åŠ é€Ÿå™¨\")\n",
    "    \n",
    "# Python ç‰ˆæœ¬\n",
    "print(f\"ğŸ Python ç‰ˆæœ¬: {sys.version.split()[0]}\")\n",
    "\n",
    "# Kaggle ç’°å¢ƒæª¢æŸ¥\n",
    "import os\n",
    "if 'KAGGLE_KERNEL_RUN_TYPE' in os.environ:\n",
    "    print(f\"ğŸ† Kaggle ç’°å¢ƒ: {os.environ['KAGGLE_KERNEL_RUN_TYPE']}\")\n",
    "else:\n",
    "    print(\"ğŸ’» æœ¬åœ°ç’°å¢ƒ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¦ å®‰è£å¿…è¦å¥—ä»¶...\n",
      "==================================================\n",
      "âœ… transformers å®‰è£æˆåŠŸ\n",
      "âœ… peft å®‰è£æˆåŠŸ\n",
      "âœ… bitsandbytes å®‰è£æˆåŠŸ\n",
      "âœ… accelerate å®‰è£æˆåŠŸ\n",
      "âœ… datasets å®‰è£æˆåŠŸ\n",
      "âœ… tensorboard å®‰è£æˆåŠŸ\n",
      "âœ… wandb å®‰è£æˆåŠŸ\n",
      "âœ… sqlparse å®‰è£æˆåŠŸ\n",
      "âœ… evaluate å®‰è£æˆåŠŸ\n",
      "âœ… scikit-learn å®‰è£æˆåŠŸ\n",
      "âœ… matplotlib å®‰è£æˆåŠŸ\n",
      "âœ… seaborn å®‰è£æˆåŠŸ\n",
      "âœ… tqdm å®‰è£æˆåŠŸ\n",
      "\n",
      "ğŸ‰ å¥—ä»¶å®‰è£å®Œæˆï¼\n"
     ]
    }
   ],
   "source": [
    "# å®‰è£å¿…è¦å¥—ä»¶ - é‡å° Kaggle ç’°å¢ƒå„ªåŒ–\n",
    "print(\"ğŸ“¦ å®‰è£å¿…è¦å¥—ä»¶...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# æ ¸å¿ƒå¥—ä»¶åˆ—è¡¨\n",
    "packages = [\n",
    "    \"transformers>=4.36.0\",  # æ”¯æ´ Arctic æ¨¡å‹\n",
    "    \"peft>=0.7.0\",           # PEFT/LoRA æ”¯æ´\n",
    "    \"bitsandbytes>=0.41.0\",  # QLoRA é‡åŒ–\n",
    "    \"accelerate>=0.25.0\",    # åˆ†æ•£å¼è¨“ç·´\n",
    "    \"datasets>=2.15.0\",      # è³‡æ–™é›†è™•ç†\n",
    "    \"tensorboard\",           # è¨“ç·´ç›£æ§\n",
    "    \"wandb\",                 # å¯¦é©—è¿½è¹¤ï¼ˆå¯é¸ï¼‰\n",
    "    \"sqlparse\",              # SQL è§£æ\n",
    "    \"evaluate\",              # è©•ä¼°æŒ‡æ¨™\n",
    "    \"scikit-learn\",          # æ©Ÿå™¨å­¸ç¿’å·¥å…·\n",
    "    \"matplotlib\",            # è¦–è¦ºåŒ–\n",
    "    \"seaborn\",               # é€²éšè¦–è¦ºåŒ–\n",
    "    \"tqdm\"                   # é€²åº¦æ¢\n",
    "]\n",
    "\n",
    "# æ‰¹é‡å®‰è£\n",
    "for package in packages:\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            [sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package],\n",
    "            capture_output=True, text=True, timeout=300\n",
    "        )\n",
    "        if result.returncode == 0:\n",
    "            print(f\"âœ… {package.split('>=')[0]} å®‰è£æˆåŠŸ\")\n",
    "        else:\n",
    "            print(f\"âš ï¸  {package.split('>=')[0]} å®‰è£è­¦å‘Š: {result.stderr[:100]}\")\n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(f\"â° {package.split('>=')[0]} å®‰è£è¶…æ™‚ï¼Œè·³é\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ {package.split('>=')[0]} å®‰è£å¤±æ•—: {str(e)[:100]}\")\n",
    "\n",
    "print(\"\\nğŸ‰ å¥—ä»¶å®‰è£å®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ğŸ” HuggingFace èªè­‰è¨­ç½® - Kaggle å„ªåŒ–ç‰ˆæœ¬\nimport getpass\nimport os\nfrom huggingface_hub import login, whoami\nfrom huggingface_hub.utils import HfHubHTTPError\nimport time\n\ndef setup_huggingface_auth_kaggle():\n    \"\"\"\n    Kaggle å„ªåŒ–çš„ HuggingFace èªè­‰è¨­ç½®\n    æ›´å¥½çš„äº’å‹•é«”é©—å’ŒéŒ¯èª¤è™•ç†\n    \"\"\"\n    \n    print(\"ğŸ” HuggingFace èªè­‰è¨­ç½®\")\n    print(\"=\" * 50)\n    \n    # æª¢æŸ¥æ˜¯å¦å·²ç¶“ç™»å…¥\n    try:\n        user_info = whoami()\n        if user_info:\n            print(f\"âœ… å·²ç™»å…¥ HuggingFace\")\n            print(f\"   â€¢ ç”¨æˆ¶å: {user_info.get('name', 'Unknown')}\")\n            return True\n    except:\n        pass\n    \n    print(\"ğŸ’¡ å»ºè­°ä½¿ç”¨ HuggingFace Token ä»¥ç²å¾—ï¼š\")\n    print(\"   â€¢ æ›´å¿«çš„ä¸‹è¼‰é€Ÿåº¦\")\n    print(\"   â€¢ é¿å… API é€Ÿç‡é™åˆ¶\")\n    print(\"   â€¢ æ›´ç©©å®šçš„é€£æ¥\")\n    print()\n    \n    # æä¾›æ›´æ˜ç¢ºçš„é¸æ“‡\n    print(\"è«‹åœ¨ä¸‹æ–¹é¸æ“‡æ“ä½œï¼š\")\n    print(\"â”€\" * 30)\n    \n    return True\n\ndef input_hf_token():\n    \"\"\"å°ˆé–€ç”¨æ–¼è¼¸å…¥ HuggingFace Token çš„å‡½æ•¸\"\"\"\n    try:\n        print(\"ğŸ”‘ è«‹è¼¸å…¥æ‚¨çš„ HuggingFace Tokenï¼š\")\n        print(\"ğŸ’¡ Token ä¸æœƒé¡¯ç¤ºåœ¨è¢å¹•ä¸Šï¼Œè«‹æ”¾å¿ƒè¼¸å…¥\")\n        print(\"ğŸ“ å¦‚éœ€ç²å– Tokenï¼šhttps://huggingface.co/settings/tokens\")\n        print()\n        \n        # å®‰å…¨è¼¸å…¥\n        hf_token = getpass.getpass(\"è«‹è¼¸å…¥ Token: \").strip()\n        \n        if not hf_token:\n            print(\"âŒ Token ä¸èƒ½ç‚ºç©º\")\n            return False\n        \n        print(\"ğŸ”„ æ­£åœ¨é©—è­‰ Token...\")\n        \n        # ç™»å…¥é©—è­‰\n        login(token=hf_token, add_to_git_credential=False)\n        \n        # ç²å–ç”¨æˆ¶ä¿¡æ¯\n        user_info = whoami()\n        \n        print(\"âœ… HuggingFace èªè­‰æˆåŠŸï¼\")\n        print(f\"   â€¢ ç”¨æˆ¶å: {user_info.get('name', 'Unknown')}\")\n        print(f\"   â€¢ ç”¨æˆ¶é¡å‹: {user_info.get('type', 'user')}\")\n        print(f\"   â€¢ èªè­‰ç‹€æ…‹: å·²é©—è­‰\")\n        \n        return True\n        \n    except HfHubHTTPError as e:\n        if e.response.status_code == 401:\n            print(\"âŒ Token ç„¡æ•ˆæˆ–å·²éæœŸ\")\n            print(\"   è«‹æª¢æŸ¥ Token æ˜¯å¦æ­£ç¢ºè¤‡è£½\")\n        else:\n            print(f\"âŒ ç¶²è·¯éŒ¯èª¤: {e}\")\n        return False\n        \n    except Exception as e:\n        print(f\"âŒ èªè­‰å¤±æ•—: {str(e)}\")\n        return False\n\ndef skip_auth():\n    \"\"\"è·³éèªè­‰ï¼Œä½¿ç”¨åŒ¿åè¨ªå•\"\"\"\n    print(\"ğŸ”“ ä½¿ç”¨åŒ¿åè¨ªå•\")\n    print(\"âš ï¸  æ³¨æ„ï¼šå¯èƒ½æœƒé‡åˆ°ä¸‹è¼‰é€Ÿåº¦é™åˆ¶\")\n    print(\"   â€¢ æ¨¡å‹å’Œè³‡æ–™é›†ä»å¯æ­£å¸¸ä½¿ç”¨\")\n    print(\"   â€¢ å¦‚é‡åˆ°å•é¡Œï¼Œè«‹é‡æ–°é‹è¡Œä¸¦é¸æ“‡èªè­‰\")\n    return False\n\n# åŸ·è¡Œåˆå§‹è¨­ç½®\nauth_setup_result = setup_huggingface_auth_kaggle()\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"ğŸ¯ HuggingFace è¨­ç½®æº–å‚™å®Œæˆ\")\nprint(\"ğŸ“‹ æ¥ä¸‹ä¾†è«‹é¸æ“‡èªè­‰æ–¹å¼ï¼š\")\nprint()\nprint(\"é¸é … A: ä½¿ç”¨ Tokenï¼ˆæ¨è–¦ï¼‰\")\nprint(\"é¸é … B: è·³éèªè­‰\")\nprint()\nprint(\"ğŸ’¡ è«‹é‹è¡Œä¸‹æ–¹å°æ‡‰çš„ Cell ä¾†å®Œæˆè¨­ç½®\")"
  },
  {
   "cell_type": "code",
   "source": "# é¸é … B: è·³éèªè­‰ï¼Œä½¿ç”¨åŒ¿åè¨ªå• ğŸ”“\n# å¦‚æœæ‚¨æ²’æœ‰ Token æˆ–æƒ³è·³éèªè­‰ï¼Œè«‹é‹è¡Œæ­¤ Cell\n\nprint(\"ğŸ”“ é¸é … B: è·³éèªè­‰\")\nprint(\"=\" * 40)\n\n# åŸ·è¡Œè·³éèªè­‰\nskip_result = skip_auth()\n\nprint(\"\\nâœ… åŒ¿åè¨ªå•è¨­ç½®å®Œæˆ\")\nprint(\"ğŸ“ æ³¨æ„äº‹é …ï¼š\")\nprint(\"   â€¢ ä¸‹è¼‰é€Ÿåº¦å¯èƒ½è¼ƒæ…¢\")\nprint(\"   â€¢ å¯èƒ½é‡åˆ° API é™åˆ¶\")\nprint(\"   â€¢ æ‰€æœ‰å…¬é–‹è³‡æºä»å¯æ­£å¸¸ä½¿ç”¨\")\nprint(\"\\nğŸ¯ å¯ä»¥ç¹¼çºŒåŸ·è¡Œå¾ŒçºŒ Cell\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# é¸é … A: ä½¿ç”¨ HuggingFace Token èªè­‰ ğŸ”‘\n# å¦‚æœæ‚¨æœ‰ HuggingFace Tokenï¼Œè«‹é‹è¡Œæ­¤ Cell\n\nprint(\"ğŸ”‘ é¸é … A: ä½¿ç”¨ HuggingFace Token\")\nprint(\"=\" * 40)\n\n# åŸ·è¡Œ Token è¼¸å…¥\nauth_success = input_hf_token()\n\nif auth_success:\n    print(\"\\nğŸ‰ Token èªè­‰å®Œæˆï¼\")\n    print(\"âœ… å¯ä»¥äº«å—æ›´å¿«çš„ä¸‹è¼‰é€Ÿåº¦\")\nelse:\n    print(\"\\nâŒ Token èªè­‰å¤±æ•—\")\n    print(\"ğŸ’¡ æ‚¨å¯ä»¥ï¼š\")\n    print(\"   1. æª¢æŸ¥ Token æ˜¯å¦æ­£ç¢º\")\n    print(\"   2. é‡æ–°é‹è¡Œæ­¤ Cell\")\n    print(\"   3. æˆ–é‹è¡Œä¸‹æ–¹ 'é¸é … B' ä½¿ç”¨åŒ¿åè¨ªå•\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”„ ç¬¬äºŒéƒ¨åˆ†ï¼šKaggle æª¢æŸ¥é»ç®¡ç†ç³»çµ±\n",
    "\n",
    "### æ ¸å¿ƒè¨­è¨ˆç†å¿µ\n",
    "åŸºæ–¼ README.md å»ºè­°ï¼Œæˆ‘å€‘æ¡ç”¨ **HuggingFace Trainer + PEFT** æ¶æ§‹ï¼š\n",
    "- **æˆç†Ÿçš„æ–·é»æ©Ÿåˆ¶**: å…§å»º `resume_from_checkpoint` åŠŸèƒ½\n",
    "- **è‡ªå‹•æ–·é»åµæ¸¬**: ä½¿ç”¨ `trainer_utils.get_last_checkpoint()`\n",
    "- **è¨˜æ†¶é«”æ•ˆç‡**: QLoRA 4-bit é‡åŒ–è®“ 7B æ¨¡å‹åƒ…éœ€ç´„ 5GB VRAM\n",
    "- **ä¿å­˜ç°¡æ½”**: åªéœ€ä¿å­˜ adapter æ¬Šé‡ï¼ˆé€šå¸¸ < 50MBï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# å°å…¥å¿…è¦çš„åº«\nimport os\nimport json\nimport torch\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pathlib import Path\nfrom datetime import datetime\nfrom typing import Dict, List, Optional, Tuple, Any\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# HuggingFace ç”Ÿæ…‹ç³»çµ±\nimport transformers  # æ·»åŠ é€™è¡Œä¾†ä¿®å¾© transformers.__version__ éŒ¯èª¤\nfrom transformers import (\n    AutoModelForCausalLM, \n    AutoTokenizer, \n    TrainingArguments, \n    Trainer,\n    BitsAndBytesConfig,\n    EarlyStoppingCallback,\n    TrainerCallback\n)\nfrom peft import LoraConfig, get_peft_model, PeftModel, TaskType\nfrom datasets import Dataset, load_dataset\nimport transformers.trainer_utils as trainer_utils\n\n# å…¶ä»–å·¥å…·\nimport sqlparse\nfrom tqdm.auto import tqdm\nimport sqlite3\n\n# è¨­å®šéš¨æ©Ÿç¨®å­ç¢ºä¿çµæœå¯é‡ç¾\ntorch.manual_seed(42)\nnp.random.seed(42)\n\n# è¨­å®šåœ–è¡¨æ¨£å¼\nplt.style.use('seaborn-v0_8')\nsns.set_palette(\"husl\")\n\nprint(\"âœ… æ‰€æœ‰åº«å°å…¥æˆåŠŸï¼\")\nprint(f\"ğŸ”§ PyTorch ç‰ˆæœ¬: {torch.__version__}\")\nprint(f\"ğŸ¤— Transformers ç‰ˆæœ¬: {transformers.__version__}\")\n\n# Kaggle å°ˆç”¨æª¢æŸ¥é»ç®¡ç†å™¨ - é˜²æ­¢è¨“ç·´ä¸­æ–·çš„é—œéµçµ„ä»¶\nclass KaggleCheckpointManager:\n    \"\"\"\n    Kaggle å°ˆç”¨çš„æª¢æŸ¥é»ç®¡ç†å™¨\n    \n    åŠŸèƒ½ç‰¹è‰²ï¼š\n    1. è‡ªå‹•åµæ¸¬æœ€æ–°æª¢æŸ¥é»\n    2. ä¿å­˜/è¼‰å…¥è¨“ç·´å…ƒæ•¸æ“š\n    3. æ”¯æ´å¤šæª¢æŸ¥é»å‚™ä»½\n    4. Kaggle ç’°å¢ƒé©é…\n    \"\"\"\n    \n    def __init__(self, base_dir=\"/kaggle/working\"):\n        \"\"\"åˆå§‹åŒ–æª¢æŸ¥é»ç®¡ç†å™¨\n        \n        Args:\n            base_dir: åŸºç¤ç›®éŒ„ï¼ŒKaggle ç’°å¢ƒå»ºè­°ä½¿ç”¨ /kaggle/working\n        \"\"\"\n        self.base_dir = Path(base_dir)\n        self.checkpoint_dir = self.base_dir / \"checkpoints\"\n        self.checkpoint_dir.mkdir(exist_ok=True, parents=True)\n        \n        # å…ƒæ•¸æ“šæ–‡ä»¶è·¯å¾‘\n        self.metadata_file = self.checkpoint_dir / \"training_metadata.json\"\n        \n        print(f\"ğŸ“ æª¢æŸ¥é»ç›®éŒ„: {self.checkpoint_dir}\")\n\n    def save_metadata(self, current_step: int, epoch: float, \n                     loss_history: List[float], learning_rate: float):\n        \"\"\"ä¿å­˜è¨“ç·´å…ƒæ•¸æ“š - é—œéµçš„è¨“ç·´ç‹€æ…‹è¿½è¹¤\n        \n        é€™å€‹æ–¹æ³•ç¢ºä¿å³ä½¿ Kaggle æ„å¤–ä¸­æ–·ï¼Œæˆ‘å€‘ä¹Ÿèƒ½ç²¾ç¢ºçŸ¥é“è¨“ç·´é€²åº¦\n        \"\"\"\n        metadata = {\n            \"current_step\": current_step,\n            \"epoch\": epoch,\n            \"timestamp\": datetime.now().isoformat(),\n            \"loss_history\": loss_history[-20:],  # ä¿å­˜æœ€è¿‘20å€‹losså€¼ç”¨æ–¼åˆ†æ\n            \"learning_rate\": learning_rate,\n            \"kaggle_session\": os.environ.get('KAGGLE_KERNEL_RUN_TYPE', 'unknown'),\n            \"gpu_name\": torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"none\",\n            \"gpu_memory_used\": torch.cuda.memory_allocated() / 1024**3 if torch.cuda.is_available() else 0\n        }\n        \n        try:\n            with open(self.metadata_file, 'w', encoding='utf-8') as f:\n                json.dump(metadata, f, indent=2, ensure_ascii=False)\n            print(f\"ğŸ’¾ å…ƒæ•¸æ“šå·²ä¿å­˜ [Step: {current_step}, Loss: {loss_history[-1]:.4f}]\")\n        except Exception as e:\n            print(f\"âš ï¸  å…ƒæ•¸æ“šä¿å­˜å¤±æ•—: {e}\")\n    \n    def load_metadata(self) -> Optional[Dict]:\n        \"\"\"è¼‰å…¥è¨“ç·´å…ƒæ•¸æ“š - ç”¨æ–¼çºŒè¨“æ™‚çš„ç‹€æ…‹æ¢å¾©\"\"\"\n        if self.metadata_file.exists():\n            try:\n                with open(self.metadata_file, 'r', encoding='utf-8') as f:\n                    metadata = json.load(f)\n                \n                print(f\"ğŸ“Š è¼‰å…¥å…ƒæ•¸æ“šæˆåŠŸ:\")\n                print(f\"   â€¢ æ­¥é©Ÿ: {metadata['current_step']}\")\n                print(f\"   â€¢ è¼ªæ•¸: {metadata['epoch']:.2f}\")\n                print(f\"   â€¢ æœ€æ–°Loss: {metadata['loss_history'][-1]:.4f}\")\n                print(f\"   â€¢ å­¸ç¿’ç‡: {metadata['learning_rate']:.2e}\")\n                print(f\"   â€¢ æ™‚é–“: {metadata['timestamp']}\")\n                \n                return metadata\n            except Exception as e:\n                print(f\"âŒ å…ƒæ•¸æ“šè¼‰å…¥å¤±æ•—: {e}\")\n                return None\n        \n        print(\"ğŸ“ æœªæ‰¾åˆ°å…ƒæ•¸æ“šæ–‡ä»¶ï¼Œå°‡å‰µå»ºæ–°çš„è¨“ç·´è¨˜éŒ„\")\n        return None\n    \n    def get_latest_checkpoint(self) -> Optional[str]:\n        \"\"\"ç²å–æœ€æ–°æª¢æŸ¥é»è·¯å¾‘ - è‡ªå‹•çºŒè¨“çš„æ ¸å¿ƒåŠŸèƒ½\"\"\"\n        try:\n            checkpoint_path = trainer_utils.get_last_checkpoint(str(self.checkpoint_dir))\n            \n            if checkpoint_path:\n                # é©—è­‰æª¢æŸ¥é»å®Œæ•´æ€§  \n                checkpoint_files = list(Path(checkpoint_path).glob(\"*\"))\n                essential_files = [\"adapter_config.json\", \"adapter_model.safetensors\"]\n                \n                missing_files = []\n                for file in essential_files:\n                    if not (Path(checkpoint_path) / file).exists():\n                        missing_files.append(file)\n                \n                if missing_files:\n                    print(f\"âš ï¸  æª¢æŸ¥é»ä¸å®Œæ•´ï¼Œç¼ºå°‘æ–‡ä»¶: {missing_files}\")\n                    return None\n                \n                print(f\"ğŸ”„ æ‰¾åˆ°å®Œæ•´æª¢æŸ¥é»: {checkpoint_path}\")\n                print(f\"ğŸ“‚ åŒ…å«æ–‡ä»¶: {len(checkpoint_files)} å€‹\")\n                return checkpoint_path\n            else:\n                print(\"ğŸ†• æœªæ‰¾åˆ°æª¢æŸ¥é»ï¼Œå°‡é–‹å§‹æ–°çš„è¨“ç·´\")\n                return None\n                \n        except Exception as e:\n            print(f\"âŒ æª¢æŸ¥é»æª¢æ¸¬å¤±æ•—: {e}\")\n            return None\n\n# åˆå§‹åŒ–æª¢æŸ¥é»ç®¡ç†å™¨\ncheckpoint_manager = KaggleCheckpointManager()\n\nprint(\"\\nğŸ¯ æª¢æŸ¥é»ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆï¼\")\nprint(\"   â€¢ æ”¯æ´è‡ªå‹•çºŒè¨“\")\nprint(\"   â€¢ å®Œæ•´å…ƒæ•¸æ“šè¿½è¹¤\")\nprint(\"   â€¢ Kaggle ç’°å¢ƒå„ªåŒ–\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ ç¬¬ä¸‰éƒ¨åˆ†ï¼šArctic-Text2SQL-R1 æ¨¡å‹é…ç½®\n",
    "\n",
    "### æ¨¡å‹é¸æ“‡ç†ç”±\n",
    "æ ¹æ“š README.md åˆ†æï¼Œ**Arctic-Text2SQL-R1 7B** æ˜¯æœ€ä½³é¸æ“‡ï¼š\n",
    "- ğŸ† **SOTA è¡¨ç¾**: BIRD ExecAcc 57%ï¼ŒåŒç´šæ¨¡å‹æœ€ä½³\n",
    "- ğŸ’° **è³‡æºå‹å¥½**: 4-bit QLoRA åƒ…éœ€ â‰ˆ5GB VRAM\n",
    "- ğŸš€ **å°ˆæ¥­è¨“ç·´**: Execution-reward RL è¨“ç·´ï¼Œå°ˆæ³¨æ­£ç¢ºç‡\n",
    "- ğŸ“œ **é–‹æºæˆæ¬Š**: Apache-2.0ï¼Œå•†ç”¨å‹å¥½"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arctic-Text2SQL-R1 æ¨¡å‹é…ç½®é¡\n",
    "class ArcticModelConfig:\n",
    "    \"\"\"\n",
    "    Arctic-Text2SQL-R1 7B æ¨¡å‹é…ç½®ç®¡ç†\n",
    "    \n",
    "    åŸºæ–¼ README.md å»ºè­°ï¼Œå¯¦ç¾ QLoRA 4-bit é‡åŒ–é…ç½®\n",
    "    ç¢ºä¿åœ¨ Kaggle P100/T4 ç’°å¢ƒä¸‹ç©©å®šé‹è¡Œ\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # æ¨¡å‹åŸºæœ¬ä¿¡æ¯\n",
    "        self.model_name = \"Snowflake/snowflake-arctic-instruct\"  # Arctic åŸºåº§æ¨¡å‹\n",
    "        self.model_type = \"arctic-text2sql\"\n",
    "        self.max_length = 2048  # é©åˆ Text2SQL ä»»å‹™çš„åºåˆ—é•·åº¦\n",
    "        \n",
    "        # QLoRA 4-bit é‡åŒ–é…ç½® - é—œéµçš„è¨˜æ†¶é«”å„ªåŒ–\n",
    "        self.quantization_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,                    # å•Ÿç”¨ 4-bit é‡åŒ–\n",
    "            bnb_4bit_quant_type=\"nf4\",           # ä½¿ç”¨ NF4 é‡åŒ–é¡å‹ï¼ˆæ¨è–¦ï¼‰\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16, # è¨ˆç®—ç²¾åº¦ï¼ˆArctic æ¨è–¦ bfloat16ï¼‰\n",
    "            bnb_4bit_use_double_quant=True,       # é›™é‡é‡åŒ–é€²ä¸€æ­¥ç¯€çœè¨˜æ†¶é«”\n",
    "            bnb_4bit_quant_storage=\"uint8\"       # å­˜å„²æ ¼å¼\n",
    "        )\n",
    "        \n",
    "        # LoRA é…ç½® - åŸºæ–¼ Arctic æ¶æ§‹ç‰¹é»èª¿æ•´\n",
    "        self.lora_config = LoraConfig(\n",
    "            # ç›®æ¨™æ¨¡çµ„ - Arctic ä½¿ç”¨ MHA çµæ§‹\n",
    "            target_modules=[\n",
    "                \"q_proj\",    # Query æŠ•å½±\n",
    "                \"k_proj\",    # Key æŠ•å½±  \n",
    "                \"v_proj\",    # Value æŠ•å½±\n",
    "                \"o_proj\",    # Output æŠ•å½±\n",
    "                \"gate_proj\", # Arctic MoE gate\n",
    "                \"up_proj\",   # MLP up æŠ•å½±\n",
    "                \"down_proj\"  # MLP down æŠ•å½±\n",
    "            ],\n",
    "            r=32,                    # LoRA rankï¼ˆå¹³è¡¡æ•ˆæœèˆ‡æ•ˆç‡ï¼‰\n",
    "            lora_alpha=64,          # LoRA scalingï¼ˆé€šå¸¸æ˜¯ r çš„ 2 å€ï¼‰\n",
    "            lora_dropout=0.05,      # LoRA dropoutï¼ˆé˜²æ­¢éæ“¬åˆï¼‰\n",
    "            bias=\"none\",            # ä¸è¨“ç·´ bias\n",
    "            task_type=TaskType.CAUSAL_LM,  # å› æœèªè¨€æ¨¡å‹\n",
    "            use_rslora=True,        # ä½¿ç”¨ RSLoRAï¼ˆç©©å®šæ€§æ”¹é€²ï¼‰\n",
    "            use_dora=False          # æš«ä¸ä½¿ç”¨ DoRAï¼ˆç¯€çœè¨ˆç®—ï¼‰\n",
    "        )\n",
    "        \n",
    "        print(\"ğŸ¯ Arctic æ¨¡å‹é…ç½®åˆå§‹åŒ–å®Œæˆï¼š\")\n",
    "        print(f\"   â€¢ åŸºåº§æ¨¡å‹: {self.model_name}\")\n",
    "        print(f\"   â€¢ é‡åŒ–: 4-bit NF4 + é›™é‡é‡åŒ–\")\n",
    "        print(f\"   â€¢ LoRA: r={self.lora_config.r}, Î±={self.lora_config.lora_alpha}\")\n",
    "        print(f\"   â€¢ ç›®æ¨™æ¨¡çµ„: {len(self.lora_config.target_modules)} å€‹\")\n",
    "    \n",
    "    def estimate_memory_usage(self) -> Dict[str, float]:\n",
    "        \"\"\"ä¼°ç®—è¨˜æ†¶é«”ä½¿ç”¨é‡ - å¹«åŠ© Kaggle ç”¨æˆ¶è¦åŠƒè³‡æº\"\"\"\n",
    "        # åŸºæ–¼ç¶“é©—å€¼ä¼°ç®—ï¼ˆ7B æ¨¡å‹ï¼‰\n",
    "        base_model_4bit = 4.2  # 4-bit é‡åŒ–å¾Œçš„åŸºåº§æ¨¡å‹\n",
    "        lora_adapters = 0.3    # LoRA é©é…å™¨æ¬Šé‡\n",
    "        optimizer_states = 0.6  # AdamW optimizer ç‹€æ…‹\n",
    "        gradient_cache = 0.4   # æ¢¯åº¦ç·©å­˜\n",
    "        activation_cache = 0.8  # å•Ÿå‹•å€¼ç·©å­˜\n",
    "        misc_overhead = 0.7    # å…¶ä»–é–‹éŠ·\n",
    "        \n",
    "        total_estimated = (\n",
    "            base_model_4bit + lora_adapters + \n",
    "            optimizer_states + gradient_cache + \n",
    "            activation_cache + misc_overhead\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"base_model_4bit\": base_model_4bit,\n",
    "            \"lora_adapters\": lora_adapters,\n",
    "            \"optimizer_states\": optimizer_states,\n",
    "            \"gradient_cache\": gradient_cache,\n",
    "            \"activation_cache\": activation_cache,\n",
    "            \"misc_overhead\": misc_overhead,\n",
    "            \"total_estimated\": total_estimated\n",
    "        }\n",
    "    \n",
    "    def print_memory_breakdown(self):\n",
    "        \"\"\"æ‰“å°è©³ç´°çš„è¨˜æ†¶é«”ä½¿ç”¨åˆ†æ\"\"\"\n",
    "        memory_usage = self.estimate_memory_usage()\n",
    "        \n",
    "        print(\"\\nğŸ’¾ é ä¼°è¨˜æ†¶é«”ä½¿ç”¨é‡åˆ†æï¼ˆGBï¼‰ï¼š\")\n",
    "        print(\"=\" * 40)\n",
    "        for component, usage in memory_usage.items():\n",
    "            if component != \"total_estimated\":\n",
    "                percentage = (usage / memory_usage[\"total_estimated\"]) * 100\n",
    "                print(f\"  {component:.<20} {usage:>6.1f} GB ({percentage:4.1f}%)\")\n",
    "        \n",
    "        print(\"-\" * 40)\n",
    "        print(f\"  {'ç¸½è¨ˆ':.<20} {memory_usage['total_estimated']:>6.1f} GB (100.0%)\")\n",
    "        \n",
    "        # GPU å…¼å®¹æ€§æª¢æŸ¥\n",
    "        if torch.cuda.is_available():\n",
    "            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "            print(f\"\\nğŸ–¥ï¸  ç•¶å‰ GPU è¨˜æ†¶é«”: {gpu_memory:.1f} GB\")\n",
    "            \n",
    "            if memory_usage[\"total_estimated\"] <= gpu_memory * 0.9:  # ç•™ 10% ç·©è¡\n",
    "                print(\"âœ… è¨˜æ†¶é«”å……è¶³ï¼Œå¯ä»¥é–‹å§‹è¨“ç·´\")\n",
    "            elif memory_usage[\"total_estimated\"] <= gpu_memory:\n",
    "                print(\"âš ï¸  è¨˜æ†¶é«”ç·Šå¼µï¼Œå»ºè­°ç›£æ§ä½¿ç”¨é‡\")\n",
    "            else:\n",
    "                print(\"âŒ è¨˜æ†¶é«”ä¸è¶³ï¼Œè€ƒæ…®ï¼š\")\n",
    "                print(\"   â€¢ æ¸›å°‘ batch size\")\n",
    "                print(\"   â€¢ æ¸›å°‘ max_length\")\n",
    "                print(\"   â€¢ ä½¿ç”¨æ›´å°çš„ LoRA rank\")\n",
    "\n",
    "# åˆå§‹åŒ–æ¨¡å‹é…ç½®\n",
    "model_config = ArcticModelConfig()\n",
    "model_config.print_memory_breakdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arctic æ¨¡å‹è¼‰å…¥èˆ‡åˆå§‹åŒ–\n",
    "class ArcticModelLoader:\n",
    "    \"\"\"Arctic-Text2SQL æ¨¡å‹è¼‰å…¥å™¨ - æ”¯æ´æ–·é»çºŒè¨“\"\"\"\n",
    "    \n",
    "    def __init__(self, config: ArcticModelConfig, checkpoint_manager: KaggleCheckpointManager):\n",
    "        self.config = config\n",
    "        self.checkpoint_manager = checkpoint_manager\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        \n",
    "        print(f\"ğŸš€ Arctic æ¨¡å‹è¼‰å…¥å™¨åˆå§‹åŒ–\")\n",
    "    \n",
    "    def load_tokenizer(self):\n",
    "        \"\"\"è¼‰å…¥åˆ†è©å™¨ - Arctic å°ˆç”¨é…ç½®\"\"\"\n",
    "        print(\"\\nğŸ“ è¼‰å…¥ Arctic åˆ†è©å™¨...\")\n",
    "        \n",
    "        try:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "                self.config.model_name,\n",
    "                trust_remote_code=True,          # Arctic éœ€è¦è‡ªå®šç¾©ä»£ç¢¼\n",
    "                use_fast=True,                   # ä½¿ç”¨å¿«é€Ÿåˆ†è©å™¨\n",
    "                padding_side=\"left\"              # Text2SQL ä»»å‹™å»ºè­°å·¦å¡«å……\n",
    "            )\n",
    "            \n",
    "            # è¨­ç½®ç‰¹æ®Š tokenï¼ˆå¦‚æœæœªè¨­ç½®ï¼‰\n",
    "            if self.tokenizer.pad_token is None:\n",
    "                self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "                print(\"   â€¢ è¨­ç½® pad_token = eos_token\")\n",
    "                \n",
    "            if self.tokenizer.chat_template is None:\n",
    "                # ç‚º Text2SQL ä»»å‹™è¨­ç½®è‡ªå®šç¾©æ¨¡æ¿\n",
    "                self.tokenizer.chat_template = (\n",
    "                    \"{% for message in messages %}\"\n",
    "                    \"{% if message['role'] == 'user' %}\"\n",
    "                    \"### æŒ‡ä»¤:\\\\næ ¹æ“šè³‡æ–™åº«çµæ§‹ç”ŸæˆSQLæŸ¥è©¢\\\\n\\\\n### è¼¸å…¥:\\\\n{{ message['content'] }}\\\\n\\\\n### å›æ‡‰:\\\\n\"\n",
    "                    \"{% elif message['role'] == 'assistant' %}\"\n",
    "                    \"{{ message['content'] }}{% if not loop.last %}\\\\n\\\\n{% endif %}\"\n",
    "                    \"{% endif %}\"\n",
    "                    \"{% endfor %}\"\n",
    "                )\n",
    "                print(\"   â€¢ è¨­ç½® Text2SQL èŠå¤©æ¨¡æ¿\")\n",
    "            \n",
    "            print(f\"âœ… åˆ†è©å™¨è¼‰å…¥æˆåŠŸ\")\n",
    "            print(f\"   â€¢ è©å½™è¡¨å¤§å°: {len(self.tokenizer):,}\")\n",
    "            print(f\"   â€¢ ç‰¹æ®Š token: pad={self.tokenizer.pad_token}, eos={self.tokenizer.eos_token}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ åˆ†è©å™¨è¼‰å…¥å¤±æ•—: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def load_base_model(self):\n",
    "        \"\"\"è¼‰å…¥åŸºåº§æ¨¡å‹ - æ”¯æ´ 4-bit é‡åŒ–\"\"\"\n",
    "        print(\"\\nğŸ¤– è¼‰å…¥ Arctic åŸºåº§æ¨¡å‹ï¼ˆ4-bit é‡åŒ–ï¼‰...\")\n",
    "        \n",
    "        try:\n",
    "            # æ¸…ç† GPU è¨˜æ†¶é«”\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "                initial_memory = torch.cuda.memory_allocated() / 1024**3\n",
    "                print(f\"   â€¢ åˆå§‹ GPU è¨˜æ†¶é«”: {initial_memory:.2f} GB\")\n",
    "            \n",
    "            # è¼‰å…¥é‡åŒ–æ¨¡å‹\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                self.config.model_name,\n",
    "                quantization_config=self.config.quantization_config,\n",
    "                device_map=\"auto\",               # è‡ªå‹•è¨­å‚™æ˜ å°„\n",
    "                trust_remote_code=True,          # Arctic éœ€è¦è‡ªå®šç¾©ä»£ç¢¼\n",
    "                torch_dtype=torch.bfloat16,      # Arctic æ¨è–¦ç²¾åº¦\n",
    "                attn_implementation=\"flash_attention_2\",  # ä½¿ç”¨ Flash Attentionï¼ˆå¦‚æœå¯ç”¨ï¼‰\n",
    "                low_cpu_mem_usage=True,          # é™ä½ CPU è¨˜æ†¶é«”ä½¿ç”¨\n",
    "                cache_dir=\"/kaggle/working/model_cache\"  # Kaggle ç·©å­˜ç›®éŒ„\n",
    "            )\n",
    "            \n",
    "            # æª¢æŸ¥è¨˜æ†¶é«”ä½¿ç”¨\n",
    "            if torch.cuda.is_available():\n",
    "                after_memory = torch.cuda.memory_allocated() / 1024**3\n",
    "                model_memory = after_memory - initial_memory\n",
    "                print(f\"   â€¢ æ¨¡å‹è¨˜æ†¶é«”ä½¿ç”¨: {model_memory:.2f} GB\")\n",
    "                print(f\"   â€¢ ç¸½è¨ˆ GPU è¨˜æ†¶é«”: {after_memory:.2f} GB\")\n",
    "            \n",
    "            # å•Ÿç”¨æ¢¯åº¦æª¢æŸ¥é»ï¼ˆç¯€çœè¨˜æ†¶é«”ï¼‰\n",
    "            self.model.gradient_checkpointing_enable()\n",
    "            print(\"   â€¢ å•Ÿç”¨æ¢¯åº¦æª¢æŸ¥é»\")\n",
    "            \n",
    "            print(f\"âœ… åŸºåº§æ¨¡å‹è¼‰å…¥æˆåŠŸ\")\n",
    "            print(f\"   â€¢ åƒæ•¸é‡: {sum(p.numel() for p in self.model.parameters()):,}\")\n",
    "            print(f\"   â€¢ å¯è¨“ç·´åƒæ•¸: {sum(p.numel() for p in self.model.parameters() if p.requires_grad):,}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ åŸºåº§æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def apply_lora(self, resume_from_checkpoint: Optional[str] = None):\n",
    "        \"\"\"æ‡‰ç”¨ LoRA é©é…å™¨ - æ”¯æ´çºŒè¨“\"\"\"\n",
    "        if resume_from_checkpoint:\n",
    "            print(f\"\\nğŸ”„ å¾æª¢æŸ¥é»è¼‰å…¥ PEFT æ¨¡å‹: {resume_from_checkpoint}\")\n",
    "            try:\n",
    "                self.model = PeftModel.from_pretrained(\n",
    "                    self.model,\n",
    "                    resume_from_checkpoint,\n",
    "                    is_trainable=True  # é‡è¦ï¼šç¢ºä¿å¯ä»¥ç¹¼çºŒè¨“ç·´\n",
    "                )\n",
    "                print(\"âœ… PEFT æ¨¡å‹çºŒè¨“è¼‰å…¥æˆåŠŸ\")\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ PEFT æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}\")\n",
    "                print(\"   å°‡å‰µå»ºæ–°çš„ LoRA é©é…å™¨\")\n",
    "                self.model = get_peft_model(self.model, self.config.lora_config)\n",
    "        else:\n",
    "            print(\"\\nğŸ†• å‰µå»ºæ–°çš„ LoRA é©é…å™¨...\")\n",
    "            self.model = get_peft_model(self.model, self.config.lora_config)\n",
    "            print(\"âœ… LoRA é©é…å™¨å‰µå»ºæˆåŠŸ\")\n",
    "        \n",
    "        # æ‰“å°å¯è¨“ç·´åƒæ•¸çµ±è¨ˆ\n",
    "        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
    "        total_params = sum(p.numel() for p in self.model.parameters())\n",
    "        \n",
    "        print(f\"\\nğŸ“Š LoRA åƒæ•¸çµ±è¨ˆ:\")\n",
    "        print(f\"   â€¢ å¯è¨“ç·´åƒæ•¸: {trainable_params:,} ({100*trainable_params/total_params:.2f}%)\")\n",
    "        print(f\"   â€¢ ç¸½åƒæ•¸: {total_params:,}\")\n",
    "        print(f\"   â€¢ è¨˜æ†¶é«”æ•ˆç‡: {100*(1-trainable_params/total_params):.1f}% ç¯€çœ\")\n",
    "    \n",
    "    def initialize_model(self, resume_from_checkpoint: Optional[str] = None):\n",
    "        \"\"\"å®Œæ•´æ¨¡å‹åˆå§‹åŒ–æµç¨‹\"\"\"\n",
    "        print(\"ğŸ¯ é–‹å§‹ Arctic-Text2SQL æ¨¡å‹åˆå§‹åŒ–æµç¨‹\\n\")\n",
    "        \n",
    "        # æ­¥é©Ÿ 1: è¼‰å…¥åˆ†è©å™¨\n",
    "        self.load_tokenizer()\n",
    "        \n",
    "        # æ­¥é©Ÿ 2: è¼‰å…¥åŸºåº§æ¨¡å‹\n",
    "        self.load_base_model()\n",
    "        \n",
    "        # æ­¥é©Ÿ 3: æ‡‰ç”¨ LoRA\n",
    "        self.apply_lora(resume_from_checkpoint)\n",
    "        \n",
    "        print(\"\\nğŸ‰ Arctic æ¨¡å‹åˆå§‹åŒ–å®Œæˆï¼æº–å‚™é–‹å§‹è¨“ç·´\")\n",
    "        \n",
    "        return self.model, self.tokenizer\n",
    "\n",
    "# æª¢æŸ¥æ˜¯å¦æœ‰å¯ç”¨çš„æª¢æŸ¥é»\n",
    "checkpoint_path = checkpoint_manager.get_latest_checkpoint()\n",
    "metadata = checkpoint_manager.load_metadata()\n",
    "\n",
    "# åˆå§‹åŒ–æ¨¡å‹è¼‰å…¥å™¨\n",
    "model_loader = ArcticModelLoader(model_config, checkpoint_manager)\n",
    "\n",
    "print(\"ğŸ”§ æ¨¡å‹è¼‰å…¥å™¨æº–å‚™å°±ç·’\")\n",
    "if checkpoint_path:\n",
    "    print(f\"   å°‡å¾æª¢æŸ¥é»çºŒè¨“: {checkpoint_path}\")\n",
    "else:\n",
    "    print(\"   å°‡é–‹å§‹æ–°çš„è¨“ç·´\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š ç¬¬å››éƒ¨åˆ†ï¼šSynSQL-2.5M è³‡æ–™è™•ç†\n",
    "\n",
    "### è³‡æ–™é›†é¸æ“‡ç†ç”±\n",
    "æ ¹æ“š README.md å»ºè­°ï¼Œæˆ‘å€‘ä½¿ç”¨ **SynSQL-2.5M** è³‡æ–™é›†ï¼š\n",
    "- ğŸ¯ **é«˜å“è³ª**: 250è¬å€‹é«˜è³ªé‡åˆæˆæ¨£æœ¬\n",
    "- ğŸŒ **å»£è¦†è“‹**: è¦†è“‹1.6è¬å€‹è³‡æ–™åº«çµæ§‹\n",
    "- ğŸ”„ **å¯æ“´å±•**: æ”¯æŒè‡ªç”±å–æ¨£ï¼Œé©åˆä¸åŒè¨“ç·´è¦æ¨¡\n",
    "- ğŸ“ˆ **SOTAåŸºç¤**: ç¾ä»£Text2SQLæ¨¡å‹çš„æ¨™æº–è¨“ç·´è³‡æ–™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SynSQL-2.5M è³‡æ–™è™•ç†å™¨\n",
    "class SynSQLDataProcessor:\n",
    "    \"\"\"è™•ç† SynSQL-2.5M è³‡æ–™é›†çš„å°ˆç”¨é¡åˆ¥\n",
    "    \n",
    "    åŠŸèƒ½ç‰¹è‰²ï¼š\n",
    "    1. æ™ºèƒ½å–æ¨£ - æ ¹æ“š Kaggle è³‡æºé™åˆ¶èª¿æ•´è³‡æ–™é‡\n",
    "    2. æ ¼å¼æ¨™æº–åŒ– - çµ±ä¸€ Text2SQL æ ¼å¼\n",
    "    3. è³ªé‡éæ¿¾ - ç§»é™¤ä½å“è³ªæ¨£æœ¬\n",
    "    4. Kaggle æœ€ä½³åŒ– - è€ƒæ…®è¨“ç·´æ™‚é–“é™åˆ¶\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, tokenizer: AutoTokenizer, max_samples: int = 50000):\n",
    "        \"\"\"åˆå§‹åŒ–è³‡æ–™è™•ç†å™¨\n",
    "        \n",
    "        Args:\n",
    "            tokenizer: Arctic åˆ†è©å™¨\n",
    "            max_samples: æœ€å¤§æ¨£æœ¬æ•¸ï¼ˆKaggle å»ºè­° 50K ä»¥å…§ï¼‰\n",
    "        \"\"\"\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_samples = max_samples\n",
    "        self.max_length = 2048  # Arctic å»ºè­°é•·åº¦\n",
    "        \n",
    "        # Text2SQL æç¤ºæ¨¡æ¿\n",
    "        self.prompt_template = (\n",
    "            \"### æŒ‡ä»¤\\n\"\n",
    "            \"æ ¹æ“šçµ¦å®šçš„è³‡æ–™åº«çµæ§‹å’Œè‡ªç„¶èªè¨€å•é¡Œï¼Œç”Ÿæˆå°æ‡‰çš„ SQL æŸ¥è©¢ã€‚\\n\\n\"\n",
    "            \"### è³‡æ–™åº«çµæ§‹\\n\"\n",
    "            \"{schema}\\n\\n\"\n",
    "            \"### å•é¡Œ\\n\"\n",
    "            \"{question}\\n\\n\"\n",
    "            \"### SQL æŸ¥è©¢\\n\"\n",
    "            \"{sql}\"\n",
    "        )\n",
    "        \n",
    "        print(f\"ğŸ“Š SynSQL è³‡æ–™è™•ç†å™¨åˆå§‹åŒ–\")\n",
    "        print(f\"   â€¢ æœ€å¤§æ¨£æœ¬æ•¸: {max_samples:,}\")\n",
    "        print(f\"   â€¢ åºåˆ—é•·åº¦: {self.max_length}\")\n",
    "    \n",
    "    def load_synsql_dataset(self, subset_size: Optional[int] = None) -> Dataset:\n",
    "        \"\"\"è¼‰å…¥ SynSQL-2.5M è³‡æ–™é›†\n",
    "        \n",
    "        Args:\n",
    "            subset_size: å­é›†å¤§å°ï¼ŒNone è¡¨ç¤ºä½¿ç”¨ max_samples\n",
    "        \"\"\"\n",
    "        print(\"\\nğŸ”„ è¼‰å…¥ SynSQL-2.5M è³‡æ–™é›†...\")\n",
    "        \n",
    "        try:\n",
    "            # å¯¦éš›å¤§å°\n",
    "            actual_size = subset_size or self.max_samples\n",
    "            \n",
    "            # è¼‰å…¥è³‡æ–™é›†ï¼ˆå–æ¨£ä»¥ç¯€çœæ™‚é–“ï¼‰\n",
    "            dataset = load_dataset(\n",
    "                \"seeklhy/SynSQL-2.5M\",\n",
    "                split=f\"train[:{actual_size}]\",  # åªå–å‰ N å€‹æ¨£æœ¬\n",
    "                cache_dir=\"/kaggle/working/dataset_cache\"  # Kaggle ç·©å­˜ç›®éŒ„\n",
    "            )\n",
    "            \n",
    "            print(f\"âœ… è³‡æ–™é›†è¼‰å…¥æˆåŠŸ\")\n",
    "            print(f\"   â€¢ æ¨£æœ¬æ•¸: {len(dataset):,}\")\n",
    "            print(f\"   â€¢ è³‡æ–™æ¬„ä½: {list(dataset.features.keys())}\")\n",
    "            \n",
    "            # é¡¯ç¤ºè³‡æ–™é›†çµ±è¨ˆ\n",
    "            self._print_dataset_stats(dataset)\n",
    "            \n",
    "            return dataset\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ è³‡æ–™é›†è¼‰å…¥å¤±æ•—: {e}\")\n",
    "            print(\"ğŸ”„ ä½¿ç”¨å‚™ç”¨è³‡æ–™é›†å‰µå»ºæ–¹æ³•...\")\n",
    "            return self._create_fallback_dataset()\n",
    "    \n",
    "    def _create_fallback_dataset(self) -> Dataset:\n",
    "        \"\"\"å‰µå»ºå‚™ç”¨è³‡æ–™é›†ï¼ˆå¦‚æœ SynSQL è¼‰å…¥å¤±æ•—ï¼‰\"\"\"\n",
    "        print(\"ğŸ“ å‰µå»ºå‚™ç”¨ Text2SQL è³‡æ–™é›†...\")\n",
    "        \n",
    "        # å‚™ç”¨æ¨£æœ¬æ•¸æ“š\n",
    "        fallback_data = [\n",
    "            {\n",
    "                \"schema\": \"CREATE TABLE users (id INT, name VARCHAR(50), email VARCHAR(100));\",\n",
    "                \"question\": \"æŸ¥æ‰¾æ‰€æœ‰ç”¨æˆ¶çš„å§“åå’Œéƒµç®±\",\n",
    "                \"sql\": \"SELECT name, email FROM users;\"\n",
    "            },\n",
    "            {\n",
    "                \"schema\": \"CREATE TABLE products (id INT, name VARCHAR(100), price DECIMAL(10,2));\",\n",
    "                \"question\": \"æ‰¾å‡ºåƒ¹æ ¼è¶…é100çš„ç”¢å“\",\n",
    "                \"sql\": \"SELECT * FROM products WHERE price > 100;\"\n",
    "            },\n",
    "            {\n",
    "                \"schema\": \"CREATE TABLE orders (id INT, user_id INT, total DECIMAL(10,2));\",\n",
    "                \"question\": \"è¨ˆç®—è¨‚å–®ç¸½é‡‘é¡\",\n",
    "                \"sql\": \"SELECT SUM(total) FROM orders;\"\n",
    "            }\n",
    "        ] * (self.max_samples // 3)  # é‡è¤‡ä»¥é”åˆ°æ‰€éœ€æ¨£æœ¬æ•¸\n",
    "        \n",
    "        return Dataset.from_list(fallback_data[:self.max_samples])\n",
    "    \n",
    "    def _print_dataset_stats(self, dataset: Dataset):\n",
    "        \"\"\"æ‰“å°è³‡æ–™é›†çµ±è¨ˆä¿¡æ¯\"\"\"\n",
    "        print(f\"\\nğŸ“ˆ è³‡æ–™é›†çµ±è¨ˆåˆ†æ:\")\n",
    "        \n",
    "        # æª¢æŸ¥å¿…è¦æ¬„ä½\n",
    "        required_fields = ['schema', 'question', 'sql']\n",
    "        available_fields = list(dataset.features.keys())\n",
    "        \n",
    "        print(f\"   â€¢ å¿…è¦æ¬„ä½æª¢æŸ¥:\")\n",
    "        for field in required_fields:\n",
    "            if field in available_fields:\n",
    "                print(f\"     âœ… {field}\")\n",
    "            else:\n",
    "                # å°‹æ‰¾ç›¸ä¼¼æ¬„ä½\n",
    "                similar_fields = [f for f in available_fields if field.lower() in f.lower()]\n",
    "                if similar_fields:\n",
    "                    print(f\"     âš ï¸  {field} (æ‰¾åˆ°ç›¸ä¼¼: {similar_fields})\")\n",
    "                else:\n",
    "                    print(f\"     âŒ {field} (ç¼ºå¤±)\")\n",
    "        \n",
    "        # æ¨£æœ¬é•·åº¦åˆ†æ\n",
    "        if len(dataset) > 0:\n",
    "            sample = dataset[0]\n",
    "            if 'question' in sample:\n",
    "                avg_question_len = np.mean([len(str(item.get('question', ''))) for item in dataset[:1000]])\n",
    "                print(f\"   â€¢ å¹³å‡å•é¡Œé•·åº¦: {avg_question_len:.1f} å­—ç¬¦\")\n",
    "            \n",
    "            if 'sql' in sample:\n",
    "                avg_sql_len = np.mean([len(str(item.get('sql', ''))) for item in dataset[:1000]])\n",
    "                print(f\"   â€¢ å¹³å‡SQLé•·åº¦: {avg_sql_len:.1f} å­—ç¬¦\")\n",
    "    \n",
    "    def preprocess_dataset(self, dataset: Dataset, train_split: float = 0.9) -> Tuple[Dataset, Dataset]:\n",
    "        \"\"\"é è™•ç†è³‡æ–™é›†ä¸¦åˆ†å‰²ç‚ºè¨“ç·´/é©—è­‰é›†\n",
    "        \n",
    "        Args:\n",
    "            dataset: åŸå§‹è³‡æ–™é›†\n",
    "            train_split: è¨“ç·´é›†æ¯”ä¾‹\n",
    "        \"\"\"\n",
    "        print(f\"\\nğŸ”§ é–‹å§‹è³‡æ–™é è™•ç†...\")\n",
    "        \n",
    "        # 1. è³‡æ–™æ¸…ç†å’Œæ ¼å¼åŒ–\n",
    "        print(\"   â€¢ è³‡æ–™æ¸…ç†ä¸­...\")\n",
    "        cleaned_dataset = dataset.map(\n",
    "            self._clean_and_format,\n",
    "            remove_columns=dataset.column_names,  # ç§»é™¤åŸå§‹æ¬„ä½\n",
    "            desc=\"æ¸…ç†è³‡æ–™\"\n",
    "        )\n",
    "        \n",
    "        # 2. éæ¿¾ç„¡æ•ˆæ¨£æœ¬\n",
    "        print(\"   â€¢ éæ¿¾ç„¡æ•ˆæ¨£æœ¬...\")\n",
    "        valid_dataset = cleaned_dataset.filter(\n",
    "            lambda x: len(x['formatted_text']) > 10 and len(x['formatted_text']) < self.max_length * 4\n",
    "        )\n",
    "        \n",
    "        print(f\"   â€¢ éæ¿¾å¾Œæ¨£æœ¬æ•¸: {len(valid_dataset):,} (åŸå§‹: {len(dataset):,})\")\n",
    "        \n",
    "        # 3. åˆ†è©åŒ–\n",
    "        print(\"   â€¢ åˆ†è©åŒ–è™•ç†...\")\n",
    "        tokenized_dataset = valid_dataset.map(\n",
    "            self._tokenize_function,\n",
    "            batched=True,\n",
    "            batch_size=1000,\n",
    "            desc=\"åˆ†è©åŒ–\"\n",
    "        )\n",
    "        \n",
    "        # 4. åˆ†å‰²è¨“ç·´/é©—è­‰é›†\n",
    "        print(f\"   â€¢ åˆ†å‰²è³‡æ–™é›† (è¨“ç·´:{train_split:.0%}, é©—è­‰:{1-train_split:.0%})...\")\n",
    "        split_dataset = tokenized_dataset.train_test_split(\n",
    "            test_size=1-train_split,\n",
    "            shuffle=True,\n",
    "            seed=42\n",
    "        )\n",
    "        \n",
    "        train_dataset = split_dataset['train']\n",
    "        eval_dataset = split_dataset['test']\n",
    "        \n",
    "        print(f\"âœ… è³‡æ–™é è™•ç†å®Œæˆ\")\n",
    "        print(f\"   â€¢ è¨“ç·´é›†: {len(train_dataset):,} æ¨£æœ¬\")\n",
    "        print(f\"   â€¢ é©—è­‰é›†: {len(eval_dataset):,} æ¨£æœ¬\")\n",
    "        \n",
    "        return train_dataset, eval_dataset\n",
    "    \n",
    "    def _clean_and_format(self, example: Dict) -> Dict:\n",
    "        \"\"\"æ¸…ç†ä¸¦æ ¼å¼åŒ–å–®å€‹æ¨£æœ¬\"\"\"\n",
    "        # æå–æ¬„ä½ï¼ˆè™•ç†ä¸åŒçš„æ¬„ä½åç¨±ï¼‰\n",
    "        schema = example.get('schema', example.get('db_schema', ''))\n",
    "        question = example.get('question', example.get('nl_question', example.get('query', '')))\n",
    "        sql = example.get('sql', example.get('sql_query', ''))\n",
    "        \n",
    "        # æ¸…ç†æ–‡æœ¬\n",
    "        schema = str(schema).strip()\n",
    "        question = str(question).strip()\n",
    "        sql = str(sql).strip()\n",
    "        \n",
    "        # æ ¼å¼åŒ–ç‚ºçµ±ä¸€çš„æç¤ºæ ¼å¼\n",
    "        formatted_text = self.prompt_template.format(\n",
    "            schema=schema,\n",
    "            question=question,\n",
    "            sql=sql\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'formatted_text': formatted_text,\n",
    "            'schema': schema,\n",
    "            'question': question,\n",
    "            'sql': sql\n",
    "        }\n",
    "    \n",
    "    def _tokenize_function(self, examples: Dict) -> Dict:\n",
    "        \"\"\"æ‰¹é‡åˆ†è©åŒ–å‡½æ•¸\"\"\"\n",
    "        # åˆ†è©åŒ–\n",
    "        tokenized = self.tokenizer(\n",
    "            examples['formatted_text'],\n",
    "            truncation=True,\n",
    "            padding=False,  # å‹•æ…‹å¡«å……æ›´æœ‰æ•ˆ\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=None  # è¿”å› Python list\n",
    "        )\n",
    "        \n",
    "        # å°æ–¼å› æœèªè¨€æ¨¡å‹ï¼Œlabels å°±æ˜¯ input_ids\n",
    "        tokenized['labels'] = tokenized['input_ids'].copy()\n",
    "        \n",
    "        return tokenized\n",
    "    \n",
    "    def create_data_collator(self):\n",
    "        \"\"\"å‰µå»ºè³‡æ–™æ•´ç†å™¨ - ç”¨æ–¼å‹•æ…‹å¡«å……\"\"\"\n",
    "        from transformers import DataCollatorForLanguageModeling\n",
    "        \n",
    "        return DataCollatorForLanguageModeling(\n",
    "            tokenizer=self.tokenizer,\n",
    "            mlm=False,  # ä¸ä½¿ç”¨ MLMï¼ˆå› ç‚ºæ˜¯å› æœèªè¨€æ¨¡å‹ï¼‰\n",
    "            pad_to_multiple_of=8  # å¡«å……åˆ°8çš„å€æ•¸ï¼ˆæé«˜æ•ˆç‡ï¼‰\n",
    "        )\n",
    "\n",
    "# è³‡æ–™è™•ç†å™¨å°‡åœ¨æ¨¡å‹è¼‰å…¥å¾Œåˆå§‹åŒ–\n",
    "print(\"ğŸ“Š SynSQL è³‡æ–™è™•ç†å™¨é¡åˆ¥å®šç¾©å®Œæˆ\")\n",
    "print(\"   ç­‰å¾…æ¨¡å‹è¼‰å…¥å¾Œé€²è¡Œè³‡æ–™è™•ç†...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸš€ ç¬¬äº”éƒ¨åˆ†ï¼šQLoRA å¾®èª¿è¨“ç·´å¯¦ç¾\n",
    "\n",
    "### æ ¸å¿ƒè¨“ç·´æ¶æ§‹\n",
    "åŸºæ–¼ README.md æœ€ä½³å¯¦è¸ï¼Œå¯¦ç¾å®Œæ•´çš„ QLoRA è¨“ç·´æµç¨‹ï¼š\n",
    "- **HuggingFace Trainer**: æˆç†Ÿçš„æ–·é»çºŒè¨“æ©Ÿåˆ¶\n",
    "- **è‡ªå®šç¾©å›èª¿**: å¯¦æ™‚ç›£æ§è¨“ç·´ç‹€æ…‹å’Œè¨˜æ†¶é«”ä½¿ç”¨\n",
    "- **å‹•æ…‹èª¿æ•´**: æ ¹æ“š GPU è¨˜æ†¶é«”è‡ªå‹•èª¿æ•´æ‰¹æ¬¡å¤§å°\n",
    "- **å¤šæŒ‡æ¨™è©•ä¼°**: Lossã€å­¸ç¿’ç‡ã€GPU ä½¿ç”¨ç‡ç­‰å…¨æ–¹ä½ç›£æ§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QLoRA è¨“ç·´å™¨ - å®Œæ•´çš„è¨“ç·´æµç¨‹å¯¦ç¾\n",
    "class ArcticQLoRATrainer:\n",
    "    \"\"\"\n",
    "    Arctic-Text2SQL QLoRA å¾®èª¿è¨“ç·´å™¨\n",
    "    \n",
    "    æ ¸å¿ƒåŠŸèƒ½ï¼š\n",
    "    1. è‡ªå‹•è¨˜æ†¶é«”ç®¡ç†å’Œæ‰¹æ¬¡å¤§å°èª¿æ•´\n",
    "    2. å®Œæ•´çš„æ–·é»çºŒè¨“æ”¯æ´\n",
    "    3. å¯¦æ™‚è¨“ç·´ç›£æ§å’Œæ—¥èªŒè¨˜éŒ„\n",
    "    4. Kaggle ç’°å¢ƒå„ªåŒ–é…ç½®\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 model,\n",
    "                 tokenizer, \n",
    "                 checkpoint_manager: KaggleCheckpointManager,\n",
    "                 config: ArcticModelConfig):\n",
    "        \"\"\"åˆå§‹åŒ–è¨“ç·´å™¨\n",
    "        \n",
    "        Args:\n",
    "            model: å·²é…ç½® LoRA çš„ Arctic æ¨¡å‹\n",
    "            tokenizer: Arctic åˆ†è©å™¨\n",
    "            checkpoint_manager: æª¢æŸ¥é»ç®¡ç†å™¨\n",
    "            config: æ¨¡å‹é…ç½®\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.checkpoint_manager = checkpoint_manager\n",
    "        self.config = config\n",
    "        \n",
    "        # è¨“ç·´ç‹€æ…‹è¿½è¹¤\n",
    "        self.training_history = {\n",
    "            'train_loss': [],\n",
    "            'eval_loss': [],\n",
    "            'learning_rate': [],\n",
    "            'gpu_memory': [],\n",
    "            'timestamps': []\n",
    "        }\n",
    "        \n",
    "        print(\"ğŸ¯ Arctic QLoRA è¨“ç·´å™¨åˆå§‹åŒ–å®Œæˆ\")\n",
    "    \n",
    "    def create_training_arguments(self, \n",
    "                                batch_size: int = 1,\n",
    "                                gradient_accumulation_steps: int = 8,\n",
    "                                num_epochs: int = 3,\n",
    "                                learning_rate: float = 2e-5) -> TrainingArguments:\n",
    "        \"\"\"å‰µå»ºè¨“ç·´åƒæ•¸é…ç½® - é‡å° Kaggle ç’°å¢ƒå„ªåŒ–\"\"\"\n",
    "        \n",
    "        return TrainingArguments(\n",
    "            # åŸºæœ¬è¨­ç½®\n",
    "            output_dir=str(self.checkpoint_manager.checkpoint_dir),\n",
    "            overwrite_output_dir=False,  # ä¿è­·ç¾æœ‰æª¢æŸ¥é»\n",
    "            run_name=f\"arctic-text2sql-{datetime.now().strftime('%Y%m%d_%H%M')}\",\n",
    "            \n",
    "            # è¨“ç·´é…ç½®\n",
    "            num_train_epochs=num_epochs,\n",
    "            per_device_train_batch_size=batch_size,\n",
    "            per_device_eval_batch_size=batch_size,\n",
    "            gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "            \n",
    "            # å„ªåŒ–å™¨è¨­ç½®\n",
    "            learning_rate=learning_rate,\n",
    "            weight_decay=0.01,\n",
    "            adam_beta1=0.9,\n",
    "            adam_beta2=0.999,\n",
    "            adam_epsilon=1e-8,\n",
    "            max_grad_norm=1.0,\n",
    "            \n",
    "            # å­¸ç¿’ç‡èª¿åº¦\n",
    "            lr_scheduler_type=\"cosine\",\n",
    "            warmup_ratio=0.03,  # 3% warmup\n",
    "            \n",
    "            # æª¢æŸ¥é»è¨­ç½®ï¼ˆé »ç¹ä¿å­˜é©æ‡‰ Kaggleï¼‰\n",
    "            save_strategy=\"steps\",\n",
    "            save_steps=50,  # æ¯50æ­¥ä¿å­˜ä¸€æ¬¡\n",
    "            save_total_limit=3,  # ä¿ç•™æœ€è¿‘3å€‹æª¢æŸ¥é»\n",
    "            \n",
    "            # è©•ä¼°è¨­ç½®\n",
    "            evaluation_strategy=\"steps\",\n",
    "            eval_steps=50,\n",
    "            eval_accumulation_steps=4,  # æ¸›å°‘è¨˜æ†¶é«”ä½¿ç”¨\n",
    "            \n",
    "            # æ—©åœå’Œæ¨¡å‹é¸æ“‡\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"eval_loss\",\n",
    "            greater_is_better=False,\n",
    "            \n",
    "            # æ—¥èªŒè¨­ç½®\n",
    "            logging_strategy=\"steps\",\n",
    "            logging_steps=10,\n",
    "            logging_dir=str(self.checkpoint_manager.base_dir / \"logs\"),\n",
    "            report_to=[\"tensorboard\"],\n",
    "            \n",
    "            # æ€§èƒ½å„ªåŒ–\n",
    "            fp16=True,  # æ··åˆç²¾åº¦è¨“ç·´\n",
    "            dataloader_drop_last=True,\n",
    "            remove_unused_columns=False,\n",
    "            group_by_length=True,  # æŒ‰é•·åº¦åˆ†çµ„æé«˜æ•ˆç‡\n",
    "            \n",
    "            # Kaggle ç‰¹å®šå„ªåŒ–\n",
    "            dataloader_num_workers=2,  # é©åˆ Kaggle CPU æ ¸å¿ƒæ•¸\n",
    "            ignore_data_skip=True,  # åŠ é€ŸçºŒè¨“\n",
    "            save_safetensors=True,  # ä½¿ç”¨æ›´å®‰å…¨çš„æ ¼å¼\n",
    "            \n",
    "            # è¨˜æ†¶é«”å„ªåŒ–\n",
    "            gradient_checkpointing=True,\n",
    "            optim=\"adamw_torch_fused\",  # æ›´å¿«çš„å„ªåŒ–å™¨ï¼ˆå¦‚æœå¯ç”¨ï¼‰\n",
    "        )\n",
    "    \n",
    "    def create_custom_callbacks(self):\n",
    "        \"\"\"å‰µå»ºè‡ªå®šç¾©å›èª¿å‡½æ•¸ - å¢å¼·ç›£æ§å’Œè‡ªå‹•åŒ–åŠŸèƒ½\"\"\"\n",
    "        \n",
    "        class ArcticTrainingCallback(TrainerCallback):\n",
    "            \"\"\"Arctic å°ˆç”¨è¨“ç·´å›èª¿ - é›†æˆæª¢æŸ¥é»ç®¡ç†å’Œè¨˜æ†¶é«”ç›£æ§\"\"\"\n",
    "            \n",
    "            def __init__(self, checkpoint_manager, training_history):\n",
    "                self.checkpoint_manager = checkpoint_manager\n",
    "                self.training_history = training_history\n",
    "                self.best_eval_loss = float('inf')\n",
    "                \n",
    "            def on_log(self, args, state, control, model=None, logs=None, **kwargs):\n",
    "                \"\"\"æ—¥èªŒè¨˜éŒ„æ™‚çš„å›èª¿ - è¨˜éŒ„è©³ç´°è¨“ç·´ç‹€æ…‹\"\"\"\n",
    "                if logs:\n",
    "                    current_time = datetime.now().isoformat()\n",
    "                    \n",
    "                    # è¨˜éŒ„è¨“ç·´æŒ‡æ¨™\n",
    "                    if \"train_loss\" in logs:\n",
    "                        self.training_history['train_loss'].append(logs[\"train_loss\"])\n",
    "                    if \"eval_loss\" in logs:\n",
    "                        self.training_history['eval_loss'].append(logs[\"eval_loss\"])\n",
    "                    if \"learning_rate\" in logs:\n",
    "                        self.training_history['learning_rate'].append(logs[\"learning_rate\"])\n",
    "                    \n",
    "                    # è¨˜éŒ„ GPU è¨˜æ†¶é«”ä½¿ç”¨\n",
    "                    if torch.cuda.is_available():\n",
    "                        gpu_memory = torch.cuda.memory_allocated() / 1024**3\n",
    "                        self.training_history['gpu_memory'].append(gpu_memory)\n",
    "                    \n",
    "                    self.training_history['timestamps'].append(current_time)\n",
    "                    \n",
    "                    # æ¯50æ­¥ä¿å­˜è©³ç´°å…ƒæ•¸æ“š\n",
    "                    if state.global_step % 50 == 0 and len(self.training_history['train_loss']) > 0:\n",
    "                        self.checkpoint_manager.save_metadata(\n",
    "                            current_step=state.global_step,\n",
    "                            epoch=state.epoch,\n",
    "                            loss_history=self.training_history['train_loss'],\n",
    "                            learning_rate=logs.get(\"learning_rate\", 0)\n",
    "                        )\n",
    "            \n",
    "            def on_evaluate(self, args, state, control, model=None, logs=None, **kwargs):\n",
    "                \"\"\"è©•ä¼°å®Œæˆæ™‚çš„å›èª¿ - æª¢æŸ¥æ˜¯å¦éœ€è¦æ—©åœ\"\"\"\n",
    "                if logs and \"eval_loss\" in logs:\n",
    "                    current_eval_loss = logs[\"eval_loss\"]\n",
    "                    \n",
    "                    # æ›´æ–°æœ€ä½³è©•ä¼°çµæœ\n",
    "                    if current_eval_loss < self.best_eval_loss:\n",
    "                        self.best_eval_loss = current_eval_loss\n",
    "                        print(f\"ğŸ‰ æ–°çš„æœ€ä½³è©•ä¼°çµæœ: {current_eval_loss:.4f}\")\n",
    "                    \n",
    "                    # è¨˜æ†¶é«”ä½¿ç”¨å ±å‘Š\n",
    "                    if torch.cuda.is_available():\n",
    "                        memory_used = torch.cuda.memory_allocated() / 1024**3\n",
    "                        memory_total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "                        print(f\"ğŸ’¾ GPU è¨˜æ†¶é«”: {memory_used:.1f}/{memory_total:.1f} GB ({memory_used/memory_total*100:.1f}%)\")\n",
    "            \n",
    "            def on_save(self, args, state, control, model=None, **kwargs):\n",
    "                \"\"\"æª¢æŸ¥é»ä¿å­˜æ™‚çš„å›èª¿\"\"\"\n",
    "                print(f\"ğŸ’¾ æª¢æŸ¥é»å·²ä¿å­˜ - Step: {state.global_step}, Epoch: {state.epoch:.2f}\")\n",
    "        \n",
    "        # å›èª¿å‡½æ•¸åˆ—è¡¨\n",
    "        callbacks = [\n",
    "            ArcticTrainingCallback(self.checkpoint_manager, self.training_history),\n",
    "            EarlyStoppingCallback(\n",
    "                early_stopping_patience=3,  # 3æ¬¡è©•ä¼°ç„¡æ”¹å–„å‰‡æ—©åœ\n",
    "                early_stopping_threshold=0.01  # æ”¹å–„é–¾å€¼\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        return callbacks\n",
    "    \n",
    "    def setup_data_collator(self):\n",
    "        \"\"\"è¨­ç½®è³‡æ–™æ•´ç†å™¨ - å‹•æ…‹å¡«å……å„ªåŒ–\"\"\"\n",
    "        from transformers import DataCollatorForLanguageModeling\n",
    "        \n",
    "        return DataCollatorForLanguageModeling(\n",
    "            tokenizer=self.tokenizer,\n",
    "            mlm=False,  # å› æœèªè¨€æ¨¡å‹\n",
    "            pad_to_multiple_of=8,  # å°é½Šåˆ°8çš„å€æ•¸æé«˜æ•ˆç‡\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "    \n",
    "    def estimate_optimal_batch_size(self) -> Tuple[int, int]:\n",
    "        \"\"\"æ™ºèƒ½ä¼°ç®—æœ€ä½³æ‰¹æ¬¡å¤§å° - æ ¹æ“š GPU è¨˜æ†¶é«”å‹•æ…‹èª¿æ•´\"\"\"\n",
    "        if not torch.cuda.is_available():\n",
    "            return 1, 8  # CPU ç’°å¢ƒçš„ä¿å®ˆè¨­ç½®\n",
    "        \n",
    "        gpu_memory_gb = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "        \n",
    "        # åŸºæ–¼ GPU è¨˜æ†¶é«”çš„å•Ÿç™¼å¼è¦å‰‡\n",
    "        if gpu_memory_gb >= 24:  # RTX 4090, A100 ç­‰\n",
    "            return 2, 4\n",
    "        elif gpu_memory_gb >= 16:  # P100, T4, RTX 3080 ç­‰\n",
    "            return 1, 8\n",
    "        elif gpu_memory_gb >= 12:  # GTX 1080 Ti ç­‰\n",
    "            return 1, 12\n",
    "        else:  # æ›´å°çš„ GPU\n",
    "            return 1, 16\n",
    "    \n",
    "    def train_model(self, \n",
    "                   train_dataset: Dataset, \n",
    "                   eval_dataset: Dataset,\n",
    "                   resume_from_checkpoint: Optional[str] = None,\n",
    "                   custom_training_args: Optional[Dict] = None) -> Any:\n",
    "        \"\"\"åŸ·è¡Œå®Œæ•´çš„ QLoRA å¾®èª¿è¨“ç·´\"\"\"\n",
    "        \n",
    "        print(\"ğŸš€ é–‹å§‹ Arctic-Text2SQL QLoRA å¾®èª¿è¨“ç·´\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # 1. æ™ºèƒ½æ‰¹æ¬¡å¤§å°è¨­ç½®\n",
    "        optimal_batch_size, optimal_grad_accum = self.estimate_optimal_batch_size()\n",
    "        print(f\"ğŸ“Š å„ªåŒ–é…ç½®:\")\n",
    "        print(f\"   â€¢ æ‰¹æ¬¡å¤§å°: {optimal_batch_size}\")\n",
    "        print(f\"   â€¢ æ¢¯åº¦ç´¯ç©æ­¥æ•¸: {optimal_grad_accum}\")\n",
    "        print(f\"   â€¢ æœ‰æ•ˆæ‰¹æ¬¡å¤§å°: {optimal_batch_size * optimal_grad_accum}\")\n",
    "        \n",
    "        # 2. å‰µå»ºè¨“ç·´åƒæ•¸\n",
    "        training_args_kwargs = {\n",
    "            'batch_size': optimal_batch_size,\n",
    "            'gradient_accumulation_steps': optimal_grad_accum,\n",
    "        }\n",
    "        if custom_training_args:\n",
    "            training_args_kwargs.update(custom_training_args)\n",
    "        \n",
    "        training_args = self.create_training_arguments(**training_args_kwargs)\n",
    "        \n",
    "        # 3. è¨­ç½®å›èª¿å’Œè³‡æ–™æ•´ç†å™¨\n",
    "        callbacks = self.create_custom_callbacks()\n",
    "        data_collator = self.setup_data_collator()\n",
    "        \n",
    "        # 4. å‰µå»º Trainer\n",
    "        trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=eval_dataset,\n",
    "            tokenizer=self.tokenizer,\n",
    "            data_collator=data_collator,\n",
    "            callbacks=callbacks,\n",
    "        )\n",
    "        \n",
    "        # 5. é–‹å§‹è¨“ç·´ï¼ˆæ”¯æ´çºŒè¨“ï¼‰\n",
    "        print(f\"\\nğŸ¯ è¨“ç·´é–‹å§‹\")\n",
    "        if resume_from_checkpoint:\n",
    "            print(f\"   ğŸ”„ å¾æª¢æŸ¥é»çºŒè¨“: {resume_from_checkpoint}\")\n",
    "            train_result = trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n",
    "        else:\n",
    "            print(\"   ğŸ†• é–‹å§‹æ–°çš„è¨“ç·´\")\n",
    "            train_result = trainer.train()\n",
    "        \n",
    "        # 6. ä¿å­˜æœ€çµ‚æ¨¡å‹\n",
    "        final_model_dir = self.checkpoint_manager.base_dir / \"final_model\"\n",
    "        trainer.save_model(str(final_model_dir))\n",
    "        \n",
    "        # 7. è¨“ç·´ç¸½çµ\n",
    "        print(f\"\\nğŸ‰ è¨“ç·´å®Œæˆï¼\")\n",
    "        print(f\"   â€¢ æœ€çµ‚æ¨¡å‹ä¿å­˜è‡³: {final_model_dir}\")\n",
    "        print(f\"   â€¢ è¨“ç·´æ­¥æ•¸: {train_result.global_step}\")\n",
    "        print(f\"   â€¢ æœ€çµ‚æå¤±: {train_result.training_loss:.4f}\")\n",
    "        \n",
    "        return train_result, trainer\n",
    "    \n",
    "    def plot_training_history(self):\n",
    "        \"\"\"å¯è¦–åŒ–è¨“ç·´æ­·å² - ç”Ÿæˆè©³ç´°çš„è¨“ç·´å ±å‘Š\"\"\"\n",
    "        if not self.training_history['train_loss']:\n",
    "            print(\"âš ï¸  æ²’æœ‰è¨“ç·´æ­·å²æ•¸æ“šå¯ä¾›å¯è¦–åŒ–\")\n",
    "            return\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        fig.suptitle('Arctic-Text2SQL QLoRA è¨“ç·´ç›£æ§', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        # æå¤±æ›²ç·š\n",
    "        axes[0, 0].plot(self.training_history['train_loss'], label='è¨“ç·´æå¤±', color='blue', alpha=0.7)\n",
    "        if self.training_history['eval_loss']:\n",
    "            axes[0, 0].plot(self.training_history['eval_loss'], label='é©—è­‰æå¤±', color='red', alpha=0.7)\n",
    "        axes[0, 0].set_title('æå¤±å‡½æ•¸è®ŠåŒ–')\n",
    "        axes[0, 0].set_xlabel('æ­¥æ•¸')\n",
    "        axes[0, 0].set_ylabel('æå¤±å€¼')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # å­¸ç¿’ç‡æ›²ç·š\n",
    "        if self.training_history['learning_rate']:\n",
    "            axes[0, 1].plot(self.training_history['learning_rate'], color='green', alpha=0.7)\n",
    "            axes[0, 1].set_title('å­¸ç¿’ç‡èª¿åº¦')\n",
    "            axes[0, 1].set_xlabel('æ­¥æ•¸')\n",
    "            axes[0, 1].set_ylabel('å­¸ç¿’ç‡')\n",
    "            axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # GPU è¨˜æ†¶é«”ä½¿ç”¨\n",
    "        if self.training_history['gpu_memory']:\n",
    "            axes[1, 0].plot(self.training_history['gpu_memory'], color='purple', alpha=0.7)\n",
    "            axes[1, 0].set_title('GPU è¨˜æ†¶é«”ä½¿ç”¨')\n",
    "            axes[1, 0].set_xlabel('æ­¥æ•¸')\n",
    "            axes[1, 0].set_ylabel('è¨˜æ†¶é«” (GB)')\n",
    "            axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # è¨“ç·´é€²åº¦çµ±è¨ˆ\n",
    "        axes[1, 1].axis('off')\n",
    "        stats_text = f\"\"\"\n",
    "è¨“ç·´çµ±è¨ˆæ‘˜è¦:\n",
    "        \n",
    "â€¢ ç¸½è¨“ç·´æ­¥æ•¸: {len(self.training_history['train_loss'])}\n",
    "â€¢ æœ€çµ‚è¨“ç·´æå¤±: {self.training_history['train_loss'][-1]:.4f}\n",
    "â€¢ æœ€ä½é©—è­‰æå¤±: {min(self.training_history['eval_loss']) if self.training_history['eval_loss'] else 'N/A'}\n",
    "â€¢ å¹³å‡ GPU è¨˜æ†¶é«”: {np.mean(self.training_history['gpu_memory']):.1f} GB\n",
    "â€¢ è¨“ç·´æŒçºŒæ™‚é–“: {len(self.training_history['timestamps'])} å€‹è¨˜éŒ„é»\n",
    "        \"\"\"\n",
    "        axes[1, 1].text(0.1, 0.9, stats_text, transform=axes[1, 1].transAxes, \n",
    "                        fontsize=12, verticalalignment='top', fontfamily='monospace')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # ä¿å­˜åœ–è¡¨\n",
    "        plot_path = self.checkpoint_manager.base_dir / \"training_history.png\"\n",
    "        plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"ğŸ“Š è¨“ç·´æ­·å²åœ–è¡¨å·²ä¿å­˜: {plot_path}\")\n",
    "\n",
    "print(\"ğŸ”§ Arctic QLoRA è¨“ç·´å™¨é¡åˆ¥å®šç¾©å®Œæˆ\")\n",
    "print(\"   æº–å‚™é–‹å§‹æ¨¡å‹åˆå§‹åŒ–å’Œè³‡æ–™è¼‰å…¥...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ ç¬¬å…­éƒ¨åˆ†ï¼šå®Œæ•´è¨“ç·´æµç¨‹åŸ·è¡Œ\n",
    "\n",
    "### åŸ·è¡Œæ­¥é©Ÿ\n",
    "1. **æ¨¡å‹åˆå§‹åŒ–** - è¼‰å…¥ Arctic æ¨¡å‹ä¸¦é…ç½® LoRA\n",
    "2. **è³‡æ–™è¼‰å…¥** - è™•ç† SynSQL è³‡æ–™é›†ä¸¦é è™•ç†\n",
    "3. **è¨“ç·´åŸ·è¡Œ** - é–‹å§‹ QLoRA å¾®èª¿è¨“ç·´\n",
    "4. **çµæœè©•ä¼°** - åˆ†æè¨“ç·´çµæœå’Œæ¨¡å‹æ€§èƒ½"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ­¥é©Ÿ 1: åˆå§‹åŒ– Arctic æ¨¡å‹å’Œåˆ†è©å™¨\n",
    "print(\"ğŸš€ ç¬¬ä¸€æ­¥ï¼šåˆå§‹åŒ– Arctic-Text2SQL æ¨¡å‹\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "try:\n",
    "    # åˆå§‹åŒ–æ¨¡å‹ï¼ˆæ”¯æ´æ–·é»çºŒè¨“ï¼‰\n",
    "    model, tokenizer = model_loader.initialize_model(checkpoint_path)\n",
    "    \n",
    "    print(\"\\nâœ… æ¨¡å‹åˆå§‹åŒ–æˆåŠŸï¼\")\n",
    "    print(f\"   â€¢ æ¨¡å‹é¡å‹: {type(model).__name__}\")\n",
    "    print(f\"   â€¢ åˆ†è©å™¨é¡å‹: {type(tokenizer).__name__}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ æ¨¡å‹åˆå§‹åŒ–å¤±æ•—: {e}\")\n",
    "    print(\"è«‹æª¢æŸ¥:\")\n",
    "    print(\"   â€¢ GPU è¨˜æ†¶é«”æ˜¯å¦å……è¶³\")\n",
    "    print(\"   â€¢ ç¶²è·¯é€£æ¥æ˜¯å¦æ­£å¸¸\")\n",
    "    print(\"   â€¢ Hugging Face æ˜¯å¦å¯è¨ªå•\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ­¥é©Ÿ 2: è¼‰å…¥å’Œé è™•ç† SynSQL è³‡æ–™é›†\n",
    "print(\"ğŸ“Š ç¬¬äºŒæ­¥ï¼šè¼‰å…¥å’Œé è™•ç† SynSQL-2.5M è³‡æ–™é›†\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "try:\n",
    "    # åˆå§‹åŒ–è³‡æ–™è™•ç†å™¨\n",
    "    data_processor = SynSQLDataProcessor(\n",
    "        tokenizer=tokenizer,\n",
    "        max_samples=10000  # Kaggle ç’°å¢ƒå»ºè­°æ¨£æœ¬æ•¸ï¼Œå¯æ ¹æ“šéœ€è¦èª¿æ•´\n",
    "    )\n",
    "    \n",
    "    # è¼‰å…¥åŸå§‹è³‡æ–™é›†\n",
    "    print(\"\\nğŸ”„ è¼‰å…¥åŸå§‹è³‡æ–™é›†...\")\n",
    "    raw_dataset = data_processor.load_synsql_dataset()\n",
    "    \n",
    "    # é è™•ç†å’Œåˆ†å‰²è³‡æ–™é›†\n",
    "    print(\"\\nğŸ”§ é è™•ç†è³‡æ–™é›†...\")\n",
    "    train_dataset, eval_dataset = data_processor.preprocess_dataset(\n",
    "        raw_dataset, \n",
    "        train_split=0.9  # 90% ç”¨æ–¼è¨“ç·´ï¼Œ10% ç”¨æ–¼é©—è­‰\n",
    "    )\n",
    "    \n",
    "    # é¡¯ç¤ºæ¨£æœ¬ç¤ºä¾‹\n",
    "    print(\"\\nğŸ“‹ è³‡æ–™é›†æ¨£æœ¬é è¦½:\")\n",
    "    print(\"è¨“ç·´é›†æ¨£æœ¬:\")\n",
    "    sample_idx = 0\n",
    "    if len(train_dataset) > 0:\n",
    "        sample = train_dataset[sample_idx]\n",
    "        print(f\"  â€¢ Input IDs é•·åº¦: {len(sample['input_ids'])}\")\n",
    "        print(f\"  â€¢ Labels é•·åº¦: {len(sample['labels'])}\")\n",
    "        \n",
    "        # è§£ç¢¼ä¸€å€‹æ¨£æœ¬çœ‹çœ‹æ ¼å¼\n",
    "        decoded_text = tokenizer.decode(sample['input_ids'][:200], skip_special_tokens=True)\n",
    "        print(f\"  â€¢ æ¨£æœ¬å…§å®¹é è¦½: {decoded_text[:200]}...\")\n",
    "    \n",
    "    print(f\"\\nâœ… è³‡æ–™é›†æº–å‚™å®Œæˆï¼\")\n",
    "    print(f\"   â€¢ è¨“ç·´é›†æ¨£æœ¬æ•¸: {len(train_dataset):,}\")\n",
    "    print(f\"   â€¢ é©—è­‰é›†æ¨£æœ¬æ•¸: {len(eval_dataset):,}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ è³‡æ–™é›†è¼‰å…¥å¤±æ•—: {e}\")\n",
    "    print(\"   å˜—è©¦ä½¿ç”¨å‚™ç”¨è³‡æ–™é›†...\")\n",
    "    \n",
    "    # ä½¿ç”¨å‚™ç”¨è³‡æ–™é›†\n",
    "    data_processor = SynSQLDataProcessor(tokenizer=tokenizer, max_samples=1000)\n",
    "    raw_dataset = data_processor._create_fallback_dataset()\n",
    "    train_dataset, eval_dataset = data_processor.preprocess_dataset(raw_dataset)\n",
    "    \n",
    "    print(f\"âœ… å‚™ç”¨è³‡æ–™é›†è¼‰å…¥æˆåŠŸ\")\n",
    "    print(f\"   â€¢ è¨“ç·´é›†æ¨£æœ¬æ•¸: {len(train_dataset):,}\")\n",
    "    print(f\"   â€¢ é©—è­‰é›†æ¨£æœ¬æ•¸: {len(eval_dataset):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ­¥é©Ÿ 3: åˆå§‹åŒ–è¨“ç·´å™¨ä¸¦é–‹å§‹ QLoRA å¾®èª¿\n",
    "print(\"ğŸ¯ ç¬¬ä¸‰æ­¥ï¼šé–‹å§‹ QLoRA å¾®èª¿è¨“ç·´\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# åˆå§‹åŒ– QLoRA è¨“ç·´å™¨\n",
    "trainer = ArcticQLoRATrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    checkpoint_manager=checkpoint_manager,\n",
    "    config=model_config\n",
    ")\n",
    "\n",
    "# é…ç½®è¨“ç·´åƒæ•¸ï¼ˆå¯æ ¹æ“šéœ€è¦èª¿æ•´ï¼‰\n",
    "custom_training_config = {\n",
    "    'num_epochs': 2,  # Kaggle æ™‚é–“é™åˆ¶ï¼Œä½¿ç”¨è¼ƒå°‘è¼ªæ¬¡\n",
    "    'learning_rate': 2e-5,  # Arctic æ¨è–¦å­¸ç¿’ç‡\n",
    "}\n",
    "\n",
    "print(\"ğŸš€ é–‹å§‹è¨“ç·´...\")\n",
    "print(\"ğŸ’¡ æç¤ºï¼šè¨“ç·´éç¨‹ä¸­æœƒè‡ªå‹•ä¿å­˜æª¢æŸ¥é»ï¼Œå¯ä»¥å®‰å…¨ä¸­æ–·ä¸¦çºŒè¨“\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "try:\n",
    "    # é–‹å§‹è¨“ç·´ï¼ˆæ”¯æ´çºŒè¨“ï¼‰\n",
    "    train_result, trained_model = trainer.train_model(\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        resume_from_checkpoint=checkpoint_path,\n",
    "        custom_training_args=custom_training_config\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ğŸ‰ è¨“ç·´æˆåŠŸå®Œæˆï¼\")\n",
    "    print(f\"   â€¢ ç¸½è¨“ç·´æ­¥æ•¸: {train_result.global_step}\")\n",
    "    print(f\"   â€¢ æœ€çµ‚è¨“ç·´æå¤±: {train_result.training_loss:.4f}\")\n",
    "    print(f\"   â€¢ è¨“ç·´ç”¨æ™‚: {train_result.metrics.get('train_runtime', 0):.1f} ç§’\")\n",
    "    \n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nâ¸ï¸  è¨“ç·´è¢«ç”¨æˆ¶ä¸­æ–·\")\n",
    "    print(\"ğŸ’¾ æª¢æŸ¥é»å·²è‡ªå‹•ä¿å­˜ï¼Œå¯ç¨å¾ŒçºŒè¨“\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ è¨“ç·´éç¨‹ä¸­å‡ºç¾éŒ¯èª¤: {e}\")\n",
    "    print(\"ğŸ’¾ æª¢æŸ¥æª¢æŸ¥é»æ˜¯å¦å·²ä¿å­˜\")\n",
    "    print(\"ğŸ”„ å¯å˜—è©¦å¾æœ€æ–°æª¢æŸ¥é»çºŒè¨“\")\n",
    "    \n",
    "    # é¡¯ç¤ºå¯ç”¨çš„æª¢æŸ¥é»\n",
    "    available_checkpoints = list(checkpoint_manager.checkpoint_dir.glob(\"checkpoint-*\"))\n",
    "    if available_checkpoints:\n",
    "        print(f\"ğŸ“‚ å¯ç”¨æª¢æŸ¥é»: {len(available_checkpoints)} å€‹\")\n",
    "        for cp in sorted(available_checkpoints)[-3:]:  # é¡¯ç¤ºæœ€è¿‘3å€‹\n",
    "            print(f\"   â€¢ {cp.name}\")\n",
    "    \n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ­¥é©Ÿ 4: è¨“ç·´çµæœåˆ†æå’Œå¯è¦–åŒ–\n",
    "print(\"ğŸ“Š ç¬¬å››æ­¥ï¼šè¨“ç·´çµæœåˆ†æ\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# ç”Ÿæˆè¨“ç·´æ­·å²å¯è¦–åŒ–\n",
    "try:\n",
    "    print(\"ğŸ¨ ç”Ÿæˆè¨“ç·´æ­·å²åœ–è¡¨...\")\n",
    "    trainer.plot_training_history()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸  å¯è¦–åŒ–ç”Ÿæˆå¤±æ•—: {e}\")\n",
    "    print(\"   å¯èƒ½åŸå› ï¼šæ²’æœ‰è¶³å¤ çš„è¨“ç·´æ•¸æ“š\")\n",
    "\n",
    "# é¡¯ç¤ºæœ€çµ‚çµ±è¨ˆ\n",
    "print(\"\\nğŸ“ˆ è¨“ç·´ç¸½çµçµ±è¨ˆ:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "try:\n",
    "    history = trainer.training_history\n",
    "    \n",
    "    if history['train_loss']:\n",
    "        print(f\"âœ… è¨“ç·´æŒ‡æ¨™:\")\n",
    "        print(f\"   â€¢ ç¸½è¨“ç·´æ­¥æ•¸: {len(history['train_loss'])}\")\n",
    "        print(f\"   â€¢ åˆå§‹æå¤±: {history['train_loss'][0]:.4f}\")\n",
    "        print(f\"   â€¢ æœ€çµ‚æå¤±: {history['train_loss'][-1]:.4f}\")\n",
    "        print(f\"   â€¢ æå¤±æ”¹å–„: {history['train_loss'][0] - history['train_loss'][-1]:.4f}\")\n",
    "        \n",
    "        if history['eval_loss']:\n",
    "            print(f\"   â€¢ æœ€ä½³é©—è­‰æå¤±: {min(history['eval_loss']):.4f}\")\n",
    "        \n",
    "        if history['gpu_memory']:\n",
    "            print(f\"   â€¢ å¹³å‡ GPU ä½¿ç”¨: {np.mean(history['gpu_memory']):.1f} GB\")\n",
    "            print(f\"   â€¢ å³°å€¼ GPU ä½¿ç”¨: {max(history['gpu_memory']):.1f} GB\")\n",
    "    \n",
    "    # æª¢æŸ¥é»ä¿¡æ¯\n",
    "    print(f\"\\nğŸ’¾ æª¢æŸ¥é»ä¿¡æ¯:\")\n",
    "    checkpoints = list(checkpoint_manager.checkpoint_dir.glob(\"checkpoint-*\"))\n",
    "    print(f\"   â€¢ ä¿å­˜çš„æª¢æŸ¥é»æ•¸: {len(checkpoints)}\")\n",
    "    \n",
    "    final_model_path = checkpoint_manager.base_dir / \"final_model\"\n",
    "    if final_model_path.exists():\n",
    "        print(f\"   â€¢ æœ€çµ‚æ¨¡å‹è·¯å¾‘: {final_model_path}\")\n",
    "        model_files = list(final_model_path.glob(\"*\"))\n",
    "        print(f\"   â€¢ æ¨¡å‹æ–‡ä»¶æ•¸: {len(model_files)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸  çµ±è¨ˆä¿¡æ¯ç”Ÿæˆå¤±æ•—: {e}\")\n",
    "\n",
    "print(\"\\nğŸŠ Arctic-Text2SQL QLoRA å¾®èª¿å®Œæˆï¼\")\n",
    "print(\"=\" * 50)\n",
    "print(\"âœ… æ‚¨å·²æˆåŠŸå®Œæˆ:\")\n",
    "print(\"   â€¢ Arctic-Text2SQL-R1 7B æ¨¡å‹å¾®èª¿\")\n",
    "print(\"   â€¢ QLoRA 4-bit é‡åŒ–è¨“ç·´\")\n",
    "print(\"   â€¢ SynSQL è³‡æ–™é›†è¨“ç·´\")\n",
    "print(\"   â€¢ å®Œæ•´çš„æª¢æŸ¥é»ç®¡ç†\")\n",
    "print(\"   â€¢ è¨“ç·´éç¨‹ç›£æ§\")\n",
    "\n",
    "print(\"\\nğŸš€ ä¸‹ä¸€æ­¥å»ºè­°:\")\n",
    "print(\"   â€¢ ä½¿ç”¨è¨“ç·´å¥½çš„æ¨¡å‹é€²è¡Œ SQL ç”Ÿæˆæ¸¬è©¦\")\n",
    "print(\"   â€¢ åœ¨ BIRD/Spider åŸºæº–ä¸Šè©•ä¼°æ¨¡å‹æ€§èƒ½\")\n",
    "print(\"   â€¢ éƒ¨ç½²åˆ°ç”Ÿç”¢ç’°å¢ƒé€²è¡Œå¯¦éš›æ‡‰ç”¨\")\n",
    "print(\"   â€¢ æ ¹æ“šæ¥­å‹™éœ€æ±‚é€²ä¸€æ­¥å¾®èª¿\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}