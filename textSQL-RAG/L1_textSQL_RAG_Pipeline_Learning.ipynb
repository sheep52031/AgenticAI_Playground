{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TextSQL RAG Pipeline å­¸ç¿’ç­†è¨˜\n",
    "\n",
    "æœ¬ç­†è¨˜æœ¬å°‡è©³ç´°ä»‹ç´¹å¦‚ä½•æ§‹å»ºä¸€å€‹å®Œæ•´çš„ textSQL RAG (Retrieval-Augmented Generation) æµæ°´ç·šã€‚\n",
    "\n",
    "## ç›®éŒ„\n",
    "1. [RAG æ¦‚å¿µä»‹ç´¹](#1-rag-æ¦‚å¿µä»‹ç´¹)\n",
    "2. [TextSQL åŸºç¤](#2-textsql-åŸºç¤)\n",
    "3. [ç’°å¢ƒè¨­ç½®](#3-ç’°å¢ƒè¨­ç½®)\n",
    "4. [æ•¸æ“šé è™•ç†](#4-æ•¸æ“šé è™•ç†)\n",
    "5. [å‘é‡åŒ–èˆ‡ç´¢å¼•](#5-å‘é‡åŒ–èˆ‡ç´¢å¼•)\n",
    "6. [æª¢ç´¢ç³»çµ±](#6-æª¢ç´¢ç³»çµ±)\n",
    "7. [SQL ç”Ÿæˆ](#7-sql-ç”Ÿæˆ)\n",
    "8. [å®Œæ•´æµæ°´ç·š](#8-å®Œæ•´æµæ°´ç·š)\n",
    "9. [è©•ä¼°èˆ‡å„ªåŒ–](#9-è©•ä¼°èˆ‡å„ªåŒ–)\n",
    "10. [å¯¦éš›æ‡‰ç”¨æ¡ˆä¾‹](#10-å¯¦éš›æ‡‰ç”¨æ¡ˆä¾‹)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. RAG æ¦‚å¿µä»‹ç´¹\n",
    "\n",
    "### ä»€éº¼æ˜¯ RAGï¼Ÿ\n",
    "RAGï¼ˆRetrieval-Augmented Generationï¼‰æ˜¯ä¸€ç¨®çµåˆäº†æª¢ç´¢å’Œç”Ÿæˆçš„æ¶æ§‹ï¼š\n",
    "1. **æª¢ç´¢éšæ®µ**ï¼šå¾çŸ¥è­˜åº«ä¸­æ‰¾åˆ°ç›¸é—œä¿¡æ¯\n",
    "2. **ç”Ÿæˆéšæ®µ**ï¼šåŸºæ–¼æª¢ç´¢åˆ°çš„ä¿¡æ¯ç”Ÿæˆå›ç­”\n",
    "\n",
    "### TextSQL RAG çš„ç‰¹é»\n",
    "- å°ˆæ³¨æ–¼è‡ªç„¶èªè¨€åˆ° SQL æŸ¥è©¢çš„è½‰æ›\n",
    "- çµåˆæ•¸æ“šåº«æ¨¡å¼ä¿¡æ¯\n",
    "- æ”¯æŒè¤‡é›œçš„æ•¸æ“šåº«æŸ¥è©¢ç”Ÿæˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG ç¤ºä¾‹çµæœ:\n",
      "åŸºæ–¼ä¸Šä¸‹æ–‡ [] å°æŸ¥è©¢ 'å¦‚ä½•æŸ¥è©¢ç”¨æˆ¶çš„è¨‚å–®ä¿¡æ¯ï¼Ÿ' çš„å›ç­”\n"
     ]
    }
   ],
   "source": [
    "# åŸºæœ¬æ¦‚å¿µç¤ºä¾‹\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# RAG æµç¨‹ç¤ºæ„\n",
    "class SimpleRAGConcept:\n",
    "    def __init__(self):\n",
    "        self.knowledge_base = []\n",
    "        self.query_history = []\n",
    "    \n",
    "    def retrieve(self, query: str) -> List[str]:\n",
    "        \"\"\"æª¢ç´¢ç›¸é—œä¿¡æ¯\"\"\"\n",
    "        # é€™è£¡æ˜¯ç°¡åŒ–çš„æª¢ç´¢é‚è¼¯\n",
    "        relevant_docs = [doc for doc in self.knowledge_base \n",
    "                        if any(word.lower() in doc.lower() for word in query.split())]\n",
    "        return relevant_docs[:3]  # è¿”å›å‰3å€‹ç›¸é—œæ–‡æª”\n",
    "    \n",
    "    def generate(self, query: str, context: List[str]) -> str:\n",
    "        \"\"\"åŸºæ–¼ä¸Šä¸‹æ–‡ç”Ÿæˆå›ç­”\"\"\"\n",
    "        # é€™è£¡æ˜¯ç°¡åŒ–çš„ç”Ÿæˆé‚è¼¯\n",
    "        return f\"åŸºæ–¼ä¸Šä¸‹æ–‡ {context} å°æŸ¥è©¢ '{query}' çš„å›ç­”\"\n",
    "    \n",
    "    def rag_pipeline(self, query: str) -> str:\n",
    "        \"\"\"å®Œæ•´çš„RAGæµæ°´ç·š\"\"\"\n",
    "        # 1. æª¢ç´¢\n",
    "        relevant_context = self.retrieve(query)\n",
    "        # 2. ç”Ÿæˆ\n",
    "        response = self.generate(query, relevant_context)\n",
    "        # 3. è¨˜éŒ„æŸ¥è©¢æ­·å²\n",
    "        self.query_history.append({'query': query, 'response': response})\n",
    "        return response\n",
    "\n",
    "# ç¤ºä¾‹ä½¿ç”¨\n",
    "rag_demo = SimpleRAGConcept()\n",
    "rag_demo.knowledge_base = [\n",
    "    \"ç”¨æˆ¶è¡¨åŒ…å«ç”¨æˆ¶IDã€å§“åã€éƒµç®±ç­‰å­—æ®µ\",\n",
    "    \"è¨‚å–®è¡¨è¨˜éŒ„äº†æ‰€æœ‰çš„è³¼è²·ä¿¡æ¯\",\n",
    "    \"ç”¢å“è¡¨å­˜å„²ç”¢å“çš„è©³ç´°ä¿¡æ¯\"\n",
    "]\n",
    "\n",
    "result = rag_demo.rag_pipeline(\"å¦‚ä½•æŸ¥è©¢ç”¨æˆ¶çš„è¨‚å–®ä¿¡æ¯ï¼Ÿ\")\n",
    "print(\"RAG ç¤ºä¾‹çµæœ:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. TextSQL åŸºç¤\n",
    "\n",
    "### æ ¸å¿ƒæ¦‚å¿µ\n",
    "TextSQL æ˜¯å°‡è‡ªç„¶èªè¨€æŸ¥è©¢è½‰æ›ç‚º SQL èªå¥çš„éç¨‹ã€‚é—œéµçµ„ä»¶åŒ…æ‹¬ï¼š\n",
    "- **Schema Understanding**: ç†è§£æ•¸æ“šåº«çµæ§‹\n",
    "- **Intent Recognition**: è­˜åˆ¥ç”¨æˆ¶æ„åœ–\n",
    "- **SQL Generation**: ç”Ÿæˆå°æ‡‰çš„SQLèªå¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ•¸æ“šåº«æ¨¡å¼ä¿¡æ¯:\n",
      "è¡¨ users: {'user_id': 'INT PRIMARY KEY', 'name': 'VARCHAR(100)', 'email': 'VARCHAR(100)', 'created_at': 'DATETIME'}\n",
      "è¡¨ orders: {'order_id': 'INT PRIMARY KEY', 'user_id': 'INT', 'product_id': 'INT', 'quantity': 'INT', 'order_date': 'DATETIME'}\n",
      "è¡¨ products: {'product_id': 'INT PRIMARY KEY', 'name': 'VARCHAR(100)', 'price': 'DECIMAL(10,2)', 'category': 'VARCHAR(50)'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TextSQL åŸºç¤çµ„ä»¶\n",
    "class DatabaseSchema:\n",
    "    \"\"\"æ•¸æ“šåº«æ¨¡å¼é¡\"\"\"\n",
    "    def __init__(self):\n",
    "        self.tables = {}\n",
    "        self.relationships = []\n",
    "    \n",
    "    def add_table(self, table_name: str, columns: Dict[str, str]):\n",
    "        \"\"\"æ·»åŠ è¡¨çµæ§‹\"\"\"\n",
    "        self.tables[table_name] = columns\n",
    "    \n",
    "    def add_relationship(self, table1: str, column1: str, table2: str, column2: str):\n",
    "        \"\"\"æ·»åŠ è¡¨é—œä¿‚\"\"\"\n",
    "        self.relationships.append({\n",
    "            'from_table': table1,\n",
    "            'from_column': column1,\n",
    "            'to_table': table2,\n",
    "            'to_column': column2\n",
    "        })\n",
    "    \n",
    "    def get_schema_info(self) -> str:\n",
    "        \"\"\"ç²å–æ¨¡å¼ä¿¡æ¯\"\"\"\n",
    "        schema_info = \"æ•¸æ“šåº«æ¨¡å¼ä¿¡æ¯:\\n\"\n",
    "        for table, columns in self.tables.items():\n",
    "            schema_info += f\"è¡¨ {table}: {columns}\\n\"\n",
    "        return schema_info\n",
    "\n",
    "# å‰µå»ºç¤ºä¾‹æ•¸æ“šåº«æ¨¡å¼\n",
    "schema = DatabaseSchema()\n",
    "schema.add_table('users', {\n",
    "    'user_id': 'INT PRIMARY KEY',\n",
    "    'name': 'VARCHAR(100)',\n",
    "    'email': 'VARCHAR(100)',\n",
    "    'created_at': 'DATETIME'\n",
    "})\n",
    "\n",
    "schema.add_table('orders', {\n",
    "    'order_id': 'INT PRIMARY KEY',\n",
    "    'user_id': 'INT',\n",
    "    'product_id': 'INT',\n",
    "    'quantity': 'INT',\n",
    "    'order_date': 'DATETIME'\n",
    "})\n",
    "\n",
    "schema.add_table('products', {\n",
    "    'product_id': 'INT PRIMARY KEY',\n",
    "    'name': 'VARCHAR(100)',\n",
    "    'price': 'DECIMAL(10,2)',\n",
    "    'category': 'VARCHAR(50)'\n",
    "})\n",
    "\n",
    "schema.add_relationship('orders', 'user_id', 'users', 'user_id')\n",
    "schema.add_relationship('orders', 'product_id', 'products', 'product_id')\n",
    "\n",
    "print(schema.get_schema_info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ç’°å¢ƒè¨­ç½®\n",
    "\n",
    "### å®‰è£å¿…è¦çš„åº«"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# åœ¨ Kaggle ç’°å¢ƒä¸­å®‰è£å¿…è¦çš„åŒ…\nimport sys\nimport subprocess\n\ndef install_package(package):\n    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n\n# é¦–å…ˆä¿®å¾©protobufç‰ˆæœ¬è¡çªå•é¡Œ\nprint(\"ğŸ”§ ä¿®å¾©protobufç‰ˆæœ¬è¡çª...\")\ntry:\n    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"protobuf\"])\n    print(\"âœ“ protobuf ç‰ˆæœ¬å·²ä¿®å¾©\")\nexcept Exception as e:\n    print(f\"âš ï¸ protobuf ä¿®å¾©å¤±æ•—ï¼Œå°‡ä½¿ç”¨å‚™ç”¨æ–¹æ¡ˆ: {e}\")\n\n# æ ¸å¿ƒåŒ…åˆ—è¡¨ (sqlite3 æ˜¯Pythonæ¨™æº–åº«ï¼Œä¸éœ€è¦å®‰è£)\npackages = [\n    'transformers',\n    'torch',\n    'sentence-transformers',\n    'faiss-cpu',\n    'sqlparse',\n    'datasets'  # ç”¨æ–¼åŠ è¼‰ Hugging Face æ•¸æ“šé›†\n]\n\n# å¯é¸åŒ…åˆ—è¡¨ï¼ˆå¦‚æœå®‰è£å¤±æ•—ä¸å½±éŸ¿æ ¸å¿ƒåŠŸèƒ½ï¼‰\noptional_packages = [\n    'chromadb',\n    'langchain',\n    'openai'\n]\n\nprint(\"\\næ­£åœ¨å®‰è£æ ¸å¿ƒåŒ…...\")\nfor package in packages:\n    try:\n        install_package(package)\n        print(f\"âœ“ {package} å®‰è£æˆåŠŸ\")\n    except Exception as e:\n        print(f\"âœ— {package} å®‰è£å¤±æ•—: {e}\")\n\nprint(\"\\næ­£åœ¨å®‰è£å¯é¸åŒ…...\")\nfor package in optional_packages:\n    try:\n        install_package(package)\n        print(f\"âœ“ {package} å®‰è£æˆåŠŸ\")\n    except Exception as e:\n        print(f\"âš ï¸ {package} å®‰è£å¤±æ•—ï¼ˆä¸å½±éŸ¿æ ¸å¿ƒåŠŸèƒ½ï¼‰: {e}\")\n\nprint(\"\\nç’°å¢ƒè¨­ç½®å®Œæˆï¼\")\nprint(\"ğŸ“ æ³¨æ„ï¼šsqlite3 æ˜¯ Python å…§å»ºåº«ï¼Œç„¡éœ€å®‰è£\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# å°å…¥å¿…è¦çš„åº«ï¼ˆå¸¶éŒ¯èª¤è™•ç†ï¼‰\nimport os\nimport json\nimport sqlite3\nimport pandas as pd\nimport numpy as np\nfrom typing import List, Dict, Any, Tuple\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# æ ¸å¿ƒå°å…¥\nprint(\"å°å…¥æ ¸å¿ƒåº«...\")\ntry:\n    import sqlparse\n    print(\"âœ“ sqlparse å°å…¥æˆåŠŸ\")\nexcept ImportError as e:\n    print(f\"âš ï¸ sqlparse å°å…¥å¤±æ•—: {e}\")\n    # å‰µå»ºç°¡å–®çš„å‚™ç”¨è§£æå™¨\n    class MockSQLParse:\n        @staticmethod\n        def parse(sql):\n            return [type('Token', (), {'tokens': [sql] if sql.strip() else []})]\n    sqlparse = MockSQLParse()\n\n# å‘é‡åŒ–å’Œæª¢ç´¢ï¼ˆå¸¶å‚™ç”¨æ–¹æ¡ˆï¼‰\nprint(\"å°å…¥å‘é‡åŒ–åº«...\")\ntry:\n    from sentence_transformers import SentenceTransformer\n    print(\"âœ“ sentence_transformers å°å…¥æˆåŠŸ\")\n    SENTENCE_TRANSFORMERS_AVAILABLE = True\nexcept ImportError as e:\n    print(f\"âš ï¸ sentence_transformers å°å…¥å¤±æ•—: {e}\")\n    SENTENCE_TRANSFORMERS_AVAILABLE = False\n    # å°‡åœ¨TextEmbedderä¸­ä½¿ç”¨å‚™ç”¨æ–¹æ¡ˆ\n\ntry:\n    import faiss\n    print(\"âœ“ faiss å°å…¥æˆåŠŸ\")\n    FAISS_AVAILABLE = True\nexcept ImportError as e:\n    print(f\"âš ï¸ faiss å°å…¥å¤±æ•—: {e}\")\n    FAISS_AVAILABLE = False\n    # å°‡ä½¿ç”¨ç°¡å–®çš„å‘é‡æœç´¢å‚™ç”¨æ–¹æ¡ˆ\n\n# è‡ªç„¶èªè¨€è™•ç†ï¼ˆå¯é¸ï¼‰\nprint(\"å°å…¥NLPåº«...\")\ntry:\n    from transformers import AutoTokenizer, AutoModel\n    import torch\n    print(\"âœ“ transformers å’Œ torch å°å…¥æˆåŠŸ\")\n    TRANSFORMERS_AVAILABLE = True\nexcept ImportError as e:\n    print(f\"âš ï¸ transformers/torch å°å…¥å¤±æ•—: {e}\")\n    TRANSFORMERS_AVAILABLE = False\n\nprint(\"\\nâœ… åº«å°å…¥å®Œæˆï¼\")\nprint(f\"ğŸ“Š åŠŸèƒ½ç‹€æ…‹:\")\nprint(f\"  â€¢ å¥å­åµŒå…¥: {'å¯ç”¨' if SENTENCE_TRANSFORMERS_AVAILABLE else 'ä½¿ç”¨å‚™ç”¨æ–¹æ¡ˆ'}\")\nprint(f\"  â€¢ å‘é‡ç´¢å¼•: {'å¯ç”¨' if FAISS_AVAILABLE else 'ä½¿ç”¨å‚™ç”¨æ–¹æ¡ˆ'}\")\nprint(f\"  â€¢ é«˜ç´šNLP: {'å¯ç”¨' if TRANSFORMERS_AVAILABLE else 'åŸºç¤åŠŸèƒ½'}\")\nprint(f\"  â€¢ SQLè§£æ: å¯ç”¨\")\nprint(f\"  â€¢ æ•¸æ“šåº«: å¯ç”¨ (SQLite3)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. æ•¸æ“šé è™•ç†\n",
    "\n",
    "### å‰µå»ºç¤ºä¾‹æ•¸æ“šåº«å’Œæ•¸æ“š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å‰µå»ºç¤ºä¾‹ SQLite æ•¸æ“šåº«\n",
    "def create_sample_database():\n",
    "    \"\"\"å‰µå»ºç¤ºä¾‹æ•¸æ“šåº«\"\"\"\n",
    "    conn = sqlite3.connect('sample_ecommerce.db')\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # å‰µå»ºç”¨æˆ¶è¡¨\n",
    "    cursor.execute('''\n",
    "    CREATE TABLE IF NOT EXISTS users (\n",
    "        user_id INTEGER PRIMARY KEY,\n",
    "        name TEXT NOT NULL,\n",
    "        email TEXT UNIQUE NOT NULL,\n",
    "        created_at DATETIME DEFAULT CURRENT_TIMESTAMP\n",
    "    )\n",
    "    ''')\n",
    "    \n",
    "    # å‰µå»ºç”¢å“è¡¨\n",
    "    cursor.execute('''\n",
    "    CREATE TABLE IF NOT EXISTS products (\n",
    "        product_id INTEGER PRIMARY KEY,\n",
    "        name TEXT NOT NULL,\n",
    "        price DECIMAL(10,2) NOT NULL,\n",
    "        category TEXT NOT NULL,\n",
    "        description TEXT\n",
    "    )\n",
    "    ''')\n",
    "    \n",
    "    # å‰µå»ºè¨‚å–®è¡¨\n",
    "    cursor.execute('''\n",
    "    CREATE TABLE IF NOT EXISTS orders (\n",
    "        order_id INTEGER PRIMARY KEY,\n",
    "        user_id INTEGER,\n",
    "        product_id INTEGER,\n",
    "        quantity INTEGER NOT NULL,\n",
    "        order_date DATETIME DEFAULT CURRENT_TIMESTAMP,\n",
    "        FOREIGN KEY (user_id) REFERENCES users (user_id),\n",
    "        FOREIGN KEY (product_id) REFERENCES products (product_id)\n",
    "    )\n",
    "    ''')\n",
    "    \n",
    "    # æ’å…¥ç¤ºä¾‹æ•¸æ“š\n",
    "    users_data = [\n",
    "        (1, 'å¼µä¸‰', 'zhang@example.com'),\n",
    "        (2, 'æå››', 'li@example.com'),\n",
    "        (3, 'ç‹äº”', 'wang@example.com')\n",
    "    ]\n",
    "    \n",
    "    products_data = [\n",
    "        (1, 'iPhone 15', 999.99, 'é›»å­ç”¢å“', 'æœ€æ–°æ¬¾æ™ºèƒ½æ‰‹æ©Ÿ'),\n",
    "        (2, 'MacBook Pro', 1299.99, 'é›»å­ç”¢å“', 'å°ˆæ¥­ç­†è¨˜æœ¬é›»è…¦'),\n",
    "        (3, 'å’–å•¡æ©Ÿ', 199.99, 'å®¶é›»', 'å…¨è‡ªå‹•å’–å•¡æ©Ÿ'),\n",
    "        (4, 'æ›¸ç±ï¼šPythonç·¨ç¨‹', 29.99, 'åœ–æ›¸', 'Pythonç·¨ç¨‹å…¥é–€æ•™ç¨‹')\n",
    "    ]\n",
    "    \n",
    "    orders_data = [\n",
    "        (1, 1, 1, 1),\n",
    "        (2, 1, 3, 1),\n",
    "        (3, 2, 2, 1),\n",
    "        (4, 3, 4, 2)\n",
    "    ]\n",
    "    \n",
    "    cursor.executemany('INSERT OR REPLACE INTO users (user_id, name, email) VALUES (?, ?, ?)', users_data)\n",
    "    cursor.executemany('INSERT OR REPLACE INTO products (product_id, name, price, category, description) VALUES (?, ?, ?, ?, ?)', products_data)\n",
    "    cursor.executemany('INSERT OR REPLACE INTO orders (order_id, user_id, product_id, quantity) VALUES (?, ?, ?, ?)', orders_data)\n",
    "    \n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    print(\"ç¤ºä¾‹æ•¸æ“šåº«å‰µå»ºå®Œæˆï¼\")\n",
    "\n",
    "# å‰µå»ºæ•¸æ“šåº«\n",
    "create_sample_database()\n",
    "\n",
    "# é©—è­‰æ•¸æ“š\n",
    "def verify_database():\n",
    "    conn = sqlite3.connect('sample_ecommerce.db')\n",
    "    \n",
    "    print(\"ç”¨æˆ¶è¡¨:\")\n",
    "    users_df = pd.read_sql_query('SELECT * FROM users', conn)\n",
    "    print(users_df)\n",
    "    \n",
    "    print(\"\\nç”¢å“è¡¨:\")\n",
    "    products_df = pd.read_sql_query('SELECT * FROM products', conn)\n",
    "    print(products_df)\n",
    "    \n",
    "    print(\"\\nè¨‚å–®è¡¨:\")\n",
    "    orders_df = pd.read_sql_query('SELECT * FROM orders', conn)\n",
    "    print(orders_df)\n",
    "    \n",
    "    conn.close()\n",
    "\n",
    "verify_database()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æº–å‚™è¨“ç·´æ•¸æ“šé›†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# æº–å‚™è¨“ç·´æ•¸æ“šé›† - ä½¿ç”¨å°ˆæ¥­æ•¸æ“šé›†å’Œè‡ªè£½æ•¸æ“šé›†çµåˆ\n\nprint(\"ğŸ” ç²å–å°ˆæ¥­ Text2SQL æ•¸æ“šé›†...\")\n\n# æ–¹æ¡ˆ1: ä½¿ç”¨ Hugging Face å°ˆæ¥­æ•¸æ“šé›†\ndef load_professional_datasets():\n    \"\"\"åŠ è¼‰å°ˆæ¥­ Text2SQL æ•¸æ“šé›†\"\"\"\n    professional_data = []\n    \n    try:\n        # å˜—è©¦ä½¿ç”¨ datasets åº«åŠ è¼‰å°ˆæ¥­æ•¸æ“šé›†\n        from datasets import load_dataset\n        \n        print(\"ğŸ“¦ å˜—è©¦åŠ è¼‰ Hugging Face æ•¸æ“šé›†...\")\n        \n        # åŠ è¼‰ä¸€äº›çŸ¥åçš„ Text2SQL æ•¸æ“šé›†\n        dataset_configs = [\n            (\"gretelai/synthetic_text_to_sql\", \"éƒ¨åˆ†åˆæˆæ•¸æ“šé›†\"),\n            (\"NumbersStation/NSText2SQL\", \"å°ˆæ¥­åŸºæº–æ•¸æ“šé›†\"),\n            (\"spider\", \"å­¸è¡“æ¨™æº–æ•¸æ“šé›†\")\n        ]\n        \n        for dataset_name, description in dataset_configs:\n            try:\n                print(f\"  â€¢ å˜—è©¦åŠ è¼‰: {dataset_name}\")\n                dataset = load_dataset(dataset_name, split=\"train[:100]\")  # åªåŠ è¼‰å‰100æ¢\n                \n                for item in dataset:\n                    # çµ±ä¸€æ•¸æ“šæ ¼å¼\n                    if 'question' in item and 'query' in item:\n                        professional_data.append({\n                            \"natural_language\": item['question'],\n                            \"sql\": item['query'],\n                            \"explanation\": f\"ä¾†è‡ª{description}\",\n                            \"source\": dataset_name\n                        })\n                \n                print(f\"    âœ“ æˆåŠŸåŠ è¼‰ {len([d for d in professional_data if d['source'] == dataset_name])} æ¢æ•¸æ“š\")\n                break  # æˆåŠŸåŠ è¼‰ä¸€å€‹æ•¸æ“šé›†å³å¯\n                \n            except Exception as e:\n                print(f\"    âš ï¸ {dataset_name} åŠ è¼‰å¤±æ•—: {str(e)[:50]}...\")\n                continue\n                \n    except ImportError:\n        print(\"âš ï¸ datasets åº«ä¸å¯ç”¨ï¼Œä½¿ç”¨æœ¬åœ°æ•¸æ“šé›†\")\n    \n    return professional_data\n\n# å˜—è©¦åŠ è¼‰å°ˆæ¥­æ•¸æ“šé›†\nprofessional_data = load_professional_datasets()\n\n# æ–¹æ¡ˆ2: è‡ªè£½åŸºç¤æ•¸æ“šé›†ï¼ˆç”¨æ–¼å­¸ç¿’å’Œå‚™ç”¨ï¼‰\nprint(\"\\nğŸ“ æº–å‚™åŸºç¤è¨“ç·´æ•¸æ“šé›†...\")\nbasic_training_data = [\n    {\n        \"natural_language\": \"é¡¯ç¤ºæ‰€æœ‰ç”¨æˆ¶çš„ä¿¡æ¯\",\n        \"sql\": \"SELECT * FROM users;\",\n        \"explanation\": \"æŸ¥è©¢ç”¨æˆ¶è¡¨ä¸­çš„æ‰€æœ‰è¨˜éŒ„\",\n        \"source\": \"basic_examples\"\n    },\n    {\n        \"natural_language\": \"æ‰¾å‡ºåƒ¹æ ¼è¶…é500å…ƒçš„ç”¢å“\",\n        \"sql\": \"SELECT * FROM products WHERE price > 500;\",\n        \"explanation\": \"ä½¿ç”¨WHEREå­å¥éæ¿¾åƒ¹æ ¼å¤§æ–¼500çš„ç”¢å“\",\n        \"source\": \"basic_examples\"\n    },\n    {\n        \"natural_language\": \"çµ±è¨ˆæ¯å€‹ç”¨æˆ¶çš„è¨‚å–®æ•¸é‡\",\n        \"sql\": \"SELECT u.name, COUNT(o.order_id) as order_count FROM users u LEFT JOIN orders o ON u.user_id = o.user_id GROUP BY u.user_id, u.name;\",\n        \"explanation\": \"ä½¿ç”¨JOINå’ŒGROUP BYçµ±è¨ˆæ¯å€‹ç”¨æˆ¶çš„è¨‚å–®æ•¸é‡\",\n        \"source\": \"basic_examples\"\n    },\n    {\n        \"natural_language\": \"æŸ¥æ‰¾æœ€è²´çš„ç”¢å“\",\n        \"sql\": \"SELECT * FROM products ORDER BY price DESC LIMIT 1;\",\n        \"explanation\": \"ä½¿ç”¨ORDER BYå’ŒLIMITæ‰¾åˆ°åƒ¹æ ¼æœ€é«˜çš„ç”¢å“\",\n        \"source\": \"basic_examples\"\n    },\n    {\n        \"natural_language\": \"é¡¯ç¤ºç”¨æˆ¶å¼µä¸‰çš„æ‰€æœ‰è¨‚å–®\",\n        \"sql\": \"SELECT o.*, p.name as product_name FROM orders o JOIN users u ON o.user_id = u.user_id JOIN products p ON o.product_id = p.product_id WHERE u.name = 'å¼µä¸‰';\",\n        \"explanation\": \"ä½¿ç”¨å¤šè¡¨JOINæŸ¥è©¢ç‰¹å®šç”¨æˆ¶çš„è¨‚å–®ä¿¡æ¯\",\n        \"source\": \"basic_examples\"\n    },\n    {\n        \"natural_language\": \"è¨ˆç®—æ¯å€‹é¡åˆ¥çš„ç”¢å“å¹³å‡åƒ¹æ ¼\",\n        \"sql\": \"SELECT category, AVG(price) as avg_price FROM products GROUP BY category;\",\n        \"explanation\": \"ä½¿ç”¨GROUP BYå’ŒAVGå‡½æ•¸è¨ˆç®—å„é¡åˆ¥çš„å¹³å‡åƒ¹æ ¼\",\n        \"source\": \"basic_examples\"\n    },\n    {\n        \"natural_language\": \"æ‰¾å‡ºæ²’æœ‰ä¸‹éè¨‚å–®çš„ç”¨æˆ¶\",\n        \"sql\": \"SELECT u.* FROM users u LEFT JOIN orders o ON u.user_id = o.user_id WHERE o.user_id IS NULL;\",\n        \"explanation\": \"ä½¿ç”¨LEFT JOINå’ŒIS NULLæ‰¾å‡ºæ²’æœ‰è¨‚å–®çš„ç”¨æˆ¶\",\n        \"source\": \"basic_examples\"\n    },\n    {\n        \"natural_language\": \"é¡¯ç¤ºéŠ·é‡æœ€é«˜çš„ç”¢å“\",\n        \"sql\": \"SELECT p.name, SUM(o.quantity) as total_sold FROM products p JOIN orders o ON p.product_id = o.product_id GROUP BY p.product_id, p.name ORDER BY total_sold DESC LIMIT 1;\",\n        \"explanation\": \"çµ±è¨ˆç”¢å“éŠ·é‡ä¸¦æ‰¾å‡ºéŠ·é‡æœ€é«˜çš„ç”¢å“\",\n        \"source\": \"basic_examples\"\n    }\n]\n\n# æ–¹æ¡ˆ3: æ•´åˆæ•¸æ“šé›†\nprint(\"\\nğŸ”„ æ•´åˆæ•¸æ“šé›†...\")\nif professional_data:\n    training_data = professional_data + basic_training_data\n    print(f\"âœ“ ä½¿ç”¨æ··åˆæ•¸æ“šé›†: {len(professional_data)} æ¢å°ˆæ¥­æ•¸æ“š + {len(basic_training_data)} æ¢åŸºç¤æ•¸æ“š\")\nelse:\n    training_data = basic_training_data\n    print(f\"âœ“ ä½¿ç”¨åŸºç¤æ•¸æ“šé›†: {len(basic_training_data)} æ¢è‡ªè£½æ•¸æ“š\")\n\n# æ•¸æ“šé›†çµ±è¨ˆ\nsources = {}\nfor item in training_data:\n    source = item.get('source', 'unknown')\n    sources[source] = sources.get(source, 0) + 1\n\nprint(f\"\\nğŸ“Š æ•¸æ“šé›†çµ±è¨ˆ (ç¸½è¨ˆ {len(training_data)} æ¢):\")\nfor source, count in sources.items():\n    print(f\"  â€¢ {source}: {count} æ¢\")\n\n# ä¿å­˜å®Œæ•´æ•¸æ“šé›†\nwith open('training_data.json', 'w', encoding='utf-8') as f:\n    json.dump(training_data, f, ensure_ascii=False, indent=2)\n\nprint(f\"\\nğŸ’¾ æ•¸æ“šé›†å·²ä¿å­˜åˆ° training_data.json\")\n\n# é¡¯ç¤ºæ•¸æ“šé›†ç¤ºä¾‹\nprint(f\"\\nğŸ“‹ æ•¸æ“šé›†ç¤ºä¾‹:\")\nfor i, item in enumerate(training_data[:3]):\n    print(f\"\\næ¨£æœ¬ {i+1} ({item.get('source', 'unknown')}):\")\n    print(f\"  è‡ªç„¶èªè¨€: {item['natural_language']}\")\n    print(f\"  SQL: {item['sql']}\")\n    print(f\"  èªªæ˜: {item['explanation']}\")\n\nprint(f\"\\nğŸ’¡ æ•¸æ“šé›†ä¾†æºèªªæ˜:\")\nprint(f\"  â€¢ å°ˆæ¥­æ•¸æ“šé›†: ä¾†è‡ª Hugging Faceï¼Œè³ªé‡é«˜ï¼Œè¦†è“‹é¢å»£\")\nprint(f\"  â€¢ åŸºç¤æ•¸æ“šé›†: é‡å°ç¤ºä¾‹æ•¸æ“šåº«è¨­è¨ˆï¼Œä¾¿æ–¼å­¸ç¿’ç†è§£\")\nprint(f\"  â€¢ æ··åˆç­–ç•¥: çµåˆå°ˆæ¥­æ€§å’Œé‡å°æ€§ï¼Œæå‡RAGæ•ˆæœ\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. å‘é‡åŒ–èˆ‡ç´¢å¼•\n",
    "\n",
    "### ä½¿ç”¨å¥å­åµŒå…¥æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# åˆå§‹åŒ–å¥å­åµŒå…¥æ¨¡å‹ï¼ˆå¢å¼·ç‰ˆï¼Œæ”¯æ´å‚™ç”¨æ–¹æ¡ˆï¼‰\nclass TextEmbedder:\n    def __init__(self, model_name='all-MiniLM-L6-v2'):\n        \"\"\"åˆå§‹åŒ–æ–‡æœ¬åµŒå…¥å™¨\"\"\"\n        self.model = None\n        self.use_simple_encoding = False\n        \n        if SENTENCE_TRANSFORMERS_AVAILABLE:\n            try:\n                self.model = SentenceTransformer(model_name)\n                print(f\"âœ“ æˆåŠŸåŠ è¼‰æ¨¡å‹: {model_name}\")\n            except Exception as e:\n                print(f\"âš ï¸ æ¨¡å‹åŠ è¼‰å¤±æ•—: {e}, ä½¿ç”¨å‚™ç”¨æ–¹æ¡ˆ\")\n                self.use_simple_encoding = True\n        else:\n            print(\"âš ï¸ sentence_transformers ä¸å¯ç”¨ï¼Œä½¿ç”¨ç°¡å–®ç·¨ç¢¼æ–¹æ¡ˆ\")\n            self.use_simple_encoding = True\n    \n    def encode(self, texts: List[str]) -> np.ndarray:\n        \"\"\"å°‡æ–‡æœ¬ç·¨ç¢¼ç‚ºå‘é‡\"\"\"\n        if self.model and not self.use_simple_encoding:\n            try:\n                return self.model.encode(texts)\n            except Exception as e:\n                print(f\"âš ï¸ æ¨¡å‹ç·¨ç¢¼å¤±æ•—: {e}, åˆ‡æ›åˆ°å‚™ç”¨æ–¹æ¡ˆ\")\n                self.use_simple_encoding = True\n                return self._simple_encode(texts)\n        else:\n            return self._simple_encode(texts)\n    \n    def _simple_encode(self, texts: List[str]) -> np.ndarray:\n        \"\"\"ç°¡å–®çš„æ–‡æœ¬ç·¨ç¢¼å‚™ç”¨æ–¹æ¡ˆï¼ˆåŸºæ–¼TF-IDFæ¦‚å¿µï¼‰\"\"\"\n        # æ§‹å»ºè©å½™è¡¨\n        all_words = set()\n        for text in texts:\n            words = text.lower().replace('ï¼Œ', ' ').replace('ã€‚', ' ').split()\n            all_words.update(words)\n        \n        vocab = {word: idx for idx, word in enumerate(sorted(all_words))}\n        \n        # å‰µå»ºå‘é‡\n        vectors = []\n        for text in texts:\n            vector = np.zeros(len(vocab))\n            words = text.lower().replace('ï¼Œ', ' ').replace('ã€‚', ' ').split()\n            \n            # ç°¡å–®çš„è©é »çµ±è¨ˆ\n            word_count = {}\n            for word in words:\n                word_count[word] = word_count.get(word, 0) + 1\n            \n            # å¡«å……å‘é‡\n            for word, count in word_count.items():\n                if word in vocab:\n                    vector[vocab[word]] = count\n            \n            # æ­£è¦åŒ–\n            if np.linalg.norm(vector) > 0:\n                vector = vector / np.linalg.norm(vector)\n            \n            vectors.append(vector)\n        \n        return np.array(vectors)\n\n# åˆå§‹åŒ–åµŒå…¥å™¨\nembedder = TextEmbedder()\n\n# æ¸¬è©¦åµŒå…¥åŠŸèƒ½\ntest_texts = [\n    \"æŸ¥è©¢æ‰€æœ‰ç”¨æˆ¶ä¿¡æ¯\",\n    \"é¡¯ç¤ºç”¢å“è©³ç´°ä¿¡æ¯\",\n    \"çµ±è¨ˆè¨‚å–®æ•¸é‡\"\n]\n\nembeddings = embedder.encode(test_texts)\nprint(f\"\\nğŸ“Š æ–‡æœ¬åµŒå…¥æ¸¬è©¦:\")\nprint(f\"  â€¢ åµŒå…¥ç¶­åº¦: {embeddings.shape}\")\nprint(f\"  â€¢ ç·¨ç¢¼æ–¹å¼: {'ç¥ç¶“ç¶²çµ¡æ¨¡å‹' if not embedder.use_simple_encoding else 'ç°¡å–®TF-IDF'}\")\nprint(f\"  â€¢ ç¤ºä¾‹å‘é‡å‰5ç¶­: {embeddings[0][:5]}\")\n\n# æ¸¬è©¦ç›¸ä¼¼åº¦\nif len(embeddings) >= 2:\n    similarity = np.dot(embeddings[0], embeddings[1])\n    print(f\"  â€¢ å‰å…©å€‹æ–‡æœ¬ç›¸ä¼¼åº¦: {similarity:.3f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æ§‹å»ºå‘é‡ç´¢å¼•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# æ§‹å»ºå‘é‡ç´¢å¼•ï¼ˆå¢å¼·ç‰ˆï¼Œæ”¯æ´FAISSå‚™ç”¨æ–¹æ¡ˆï¼‰\nclass VectorIndex:\n    def __init__(self, dimension: int):\n        \"\"\"åˆå§‹åŒ–å‘é‡ç´¢å¼•\"\"\"\n        self.dimension = dimension\n        self.texts = []  # å­˜å„²åŸå§‹æ–‡æœ¬\n        self.metadata = []  # å­˜å„²å…ƒæ•¸æ“š\n        self.vectors = []  # å‚™ç”¨æ–¹æ¡ˆï¼šå­˜å„²æ‰€æœ‰å‘é‡\n        self.use_faiss = FAISS_AVAILABLE\n        \n        if self.use_faiss:\n            try:\n                self.index = faiss.IndexFlatL2(dimension)  # L2è·é›¢ç´¢å¼•\n                print(f\"âœ“ ä½¿ç”¨ FAISS ç´¢å¼•ï¼Œç¶­åº¦: {dimension}\")\n            except Exception as e:\n                print(f\"âš ï¸ FAISS åˆå§‹åŒ–å¤±æ•—: {e}, ä½¿ç”¨å‚™ç”¨ç´¢å¼•\")\n                self.use_faiss = False\n        \n        if not self.use_faiss:\n            print(f\"âœ“ ä½¿ç”¨ç°¡å–®å‘é‡ç´¢å¼•ï¼Œç¶­åº¦: {dimension}\")\n    \n    def add_vectors(self, vectors: np.ndarray, texts: List[str], metadata: List[Dict]):\n        \"\"\"æ·»åŠ å‘é‡åˆ°ç´¢å¼•\"\"\"\n        if self.use_faiss:\n            try:\n                self.index.add(vectors.astype('float32'))\n            except Exception as e:\n                print(f\"âš ï¸ FAISS æ·»åŠ å‘é‡å¤±æ•—: {e}, åˆ‡æ›åˆ°å‚™ç”¨æ–¹æ¡ˆ\")\n                self.use_faiss = False\n                self.vectors = vectors.tolist()  # è½‰æ›ç‚ºåˆ—è¡¨å­˜å„²\n        \n        if not self.use_faiss:\n            self.vectors.extend(vectors.tolist())\n        \n        self.texts.extend(texts)\n        self.metadata.extend(metadata)\n        print(f\"âœ“ å·²æ·»åŠ  {len(vectors)} å€‹å‘é‡åˆ°ç´¢å¼•\")\n    \n    def search(self, query_vector: np.ndarray, k: int = 5) -> List[Dict]:\n        \"\"\"æœç´¢æœ€ç›¸ä¼¼çš„å‘é‡\"\"\"\n        if self.use_faiss:\n            return self._faiss_search(query_vector, k)\n        else:\n            return self._simple_search(query_vector, k)\n    \n    def _faiss_search(self, query_vector: np.ndarray, k: int) -> List[Dict]:\n        \"\"\"ä½¿ç”¨ FAISS æœç´¢\"\"\"\n        try:\n            query_vector = query_vector.reshape(1, -1).astype('float32')\n            distances, indices = self.index.search(query_vector, k)\n            \n            results = []\n            for i, (distance, idx) in enumerate(zip(distances[0], indices[0])):\n                if idx < len(self.texts):\n                    results.append({\n                        'text': self.texts[idx],\n                        'metadata': self.metadata[idx],\n                        'distance': float(distance),\n                        'rank': i + 1\n                    })\n            return results\n        except Exception as e:\n            print(f\"âš ï¸ FAISS æœç´¢å¤±æ•—: {e}, åˆ‡æ›åˆ°å‚™ç”¨æœç´¢\")\n            self.use_faiss = False\n            return self._simple_search(query_vector, k)\n    \n    def _simple_search(self, query_vector: np.ndarray, k: int) -> List[Dict]:\n        \"\"\"ç°¡å–®çš„å‘é‡æœç´¢ï¼ˆé¤˜å¼¦ç›¸ä¼¼åº¦ï¼‰\"\"\"\n        if not self.vectors:\n            return []\n        \n        # è¨ˆç®—èˆ‡æ‰€æœ‰å‘é‡çš„ç›¸ä¼¼åº¦\n        similarities = []\n        query_norm = np.linalg.norm(query_vector)\n        \n        for i, stored_vector in enumerate(self.vectors):\n            stored_vector = np.array(stored_vector)\n            stored_norm = np.linalg.norm(stored_vector)\n            \n            if query_norm > 0 and stored_norm > 0:\n                # é¤˜å¼¦ç›¸ä¼¼åº¦\n                similarity = np.dot(query_vector, stored_vector) / (query_norm * stored_norm)\n                # è½‰æ›ç‚ºè·é›¢ï¼ˆè·é›¢è¶Šå°è¶Šç›¸ä¼¼ï¼‰\n                distance = 1 - similarity\n            else:\n                distance = float('inf')\n            \n            similarities.append((distance, i))\n        \n        # æ’åºä¸¦è¿”å›å‰kå€‹\n        similarities.sort(key=lambda x: x[0])\n        \n        results = []\n        for rank, (distance, idx) in enumerate(similarities[:k]):\n            if idx < len(self.texts):\n                results.append({\n                    'text': self.texts[idx],\n                    'metadata': self.metadata[idx],\n                    'distance': float(distance),\n                    'rank': rank + 1\n                })\n        \n        return results\n    \n    def get_stats(self) -> Dict:\n        \"\"\"ç²å–ç´¢å¼•çµ±è¨ˆä¿¡æ¯\"\"\"\n        total_vectors = len(self.vectors) if not self.use_faiss else (self.index.ntotal if hasattr(self, 'index') else 0)\n        return {\n            'total_vectors': total_vectors,\n            'dimension': self.dimension,\n            'index_type': 'FAISS-FlatL2' if self.use_faiss else 'Simple-Cosine'\n        }\n\n# ç‚ºè¨“ç·´æ•¸æ“šå‰µå»ºå‘é‡ç´¢å¼•\nnatural_language_queries = [item['natural_language'] for item in training_data]\nquery_embeddings = embedder.encode(natural_language_queries)\n\n# åˆå§‹åŒ–å‘é‡ç´¢å¼•\nvector_index = VectorIndex(query_embeddings.shape[1])\n\n# æ·»åŠ å‘é‡åˆ°ç´¢å¼•\nvector_index.add_vectors(\n    query_embeddings,\n    natural_language_queries,\n    training_data\n)\n\nprint(f\"\\nğŸ“Š å‘é‡ç´¢å¼•çµ±è¨ˆ:\")\nstats = vector_index.get_stats()\nfor key, value in stats.items():\n    print(f\"  â€¢ {key}: {value}\")\n\n# æ¸¬è©¦æœç´¢åŠŸèƒ½\ntest_query = \"æŸ¥è©¢ç”¨æˆ¶ä¿¡æ¯\"\ntest_vector = embedder.encode([test_query])[0]\nsearch_results = vector_index.search(test_vector, k=3)\n\nprint(f\"\\nğŸ” æœç´¢æ¸¬è©¦: '{test_query}'\")\nfor result in search_results:\n    print(f\"  â€¢ åŒ¹é…: {result['text']} (è·é›¢: {result['distance']:.3f})\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. æª¢ç´¢ç³»çµ±\n",
    "\n",
    "### å¯¦ç¾èªç¾©æª¢ç´¢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯¦ç¾æª¢ç´¢ç³»çµ±\n",
    "class SemanticRetriever:\n",
    "    def __init__(self, embedder: TextEmbedder, vector_index: VectorIndex, schema: DatabaseSchema):\n",
    "        \"\"\"åˆå§‹åŒ–èªç¾©æª¢ç´¢å™¨\"\"\"\n",
    "        self.embedder = embedder\n",
    "        self.vector_index = vector_index\n",
    "        self.schema = schema\n",
    "    \n",
    "    def retrieve(self, query: str, k: int = 3) -> List[Dict]:\n",
    "        \"\"\"æª¢ç´¢ç›¸é—œçš„SQLç¤ºä¾‹\"\"\"\n",
    "        # 1. å°‡æŸ¥è©¢ç·¨ç¢¼ç‚ºå‘é‡\n",
    "        query_vector = self.embedder.encode([query])[0]\n",
    "        \n",
    "        # 2. åœ¨å‘é‡ç´¢å¼•ä¸­æœç´¢\n",
    "        similar_examples = self.vector_index.search(query_vector, k)\n",
    "        \n",
    "        # 3. æ·»åŠ æ•¸æ“šåº«æ¨¡å¼ä¿¡æ¯\n",
    "        schema_info = self.schema.get_schema_info()\n",
    "        \n",
    "        # 4. æ§‹å»ºæª¢ç´¢çµæœ\n",
    "        retrieval_results = {\n",
    "            'query': query,\n",
    "            'schema_info': schema_info,\n",
    "            'similar_examples': similar_examples,\n",
    "            'retrieval_score': self._calculate_retrieval_score(similar_examples)\n",
    "        }\n",
    "        \n",
    "        return retrieval_results\n",
    "    \n",
    "    def _calculate_retrieval_score(self, examples: List[Dict]) -> float:\n",
    "        \"\"\"è¨ˆç®—æª¢ç´¢è³ªé‡åˆ†æ•¸\"\"\"\n",
    "        if not examples:\n",
    "            return 0.0\n",
    "        \n",
    "        # åŸºæ–¼è·é›¢è¨ˆç®—åˆ†æ•¸ï¼ˆè·é›¢è¶Šå°ï¼Œåˆ†æ•¸è¶Šé«˜ï¼‰\n",
    "        distances = [ex['distance'] for ex in examples]\n",
    "        avg_distance = sum(distances) / len(distances)\n",
    "        score = max(0, 1 - avg_distance / 10)  # ç°¡å–®çš„è©•åˆ†å…¬å¼\n",
    "        return score\n",
    "    \n",
    "    def explain_retrieval(self, results: Dict) -> str:\n",
    "        \"\"\"è§£é‡‹æª¢ç´¢éç¨‹\"\"\"\n",
    "        explanation = f\"\\næª¢ç´¢çµæœè§£é‡‹ï¼š\\n\"\n",
    "        explanation += f\"æŸ¥è©¢: {results['query']}\\n\"\n",
    "        explanation += f\"æª¢ç´¢åˆ†æ•¸: {results['retrieval_score']:.3f}\\n\"\n",
    "        explanation += f\"æ‰¾åˆ° {len(results['similar_examples'])} å€‹ç›¸ä¼¼ç¤ºä¾‹:\\n\"\n",
    "        \n",
    "        for i, example in enumerate(results['similar_examples']):\n",
    "            explanation += f\"\\n{i+1}. ç›¸ä¼¼åº¦: {1-example['distance']:.3f}\\n\"\n",
    "            explanation += f\"   æŸ¥è©¢: {example['text']}\\n\"\n",
    "            explanation += f\"   SQL: {example['metadata']['sql']}\\n\"\n",
    "        \n",
    "        return explanation\n",
    "\n",
    "# åˆå§‹åŒ–æª¢ç´¢å™¨\n",
    "retriever = SemanticRetriever(embedder, vector_index, schema)\n",
    "\n",
    "# æ¸¬è©¦æª¢ç´¢åŠŸèƒ½\n",
    "test_query = \"æˆ‘è¦æŸ¥çœ‹æ‰€æœ‰ç”¢å“çš„è©³ç´°ä¿¡æ¯\"\n",
    "retrieval_results = retriever.retrieve(test_query)\n",
    "\n",
    "print(\"=== æª¢ç´¢æ¸¬è©¦ ===\")\n",
    "print(retriever.explain_retrieval(retrieval_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. SQL ç”Ÿæˆ\n",
    "\n",
    "### åŸºæ–¼æª¢ç´¢çµæœç”ŸæˆSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# SQLç”Ÿæˆå™¨ (2025å¹´å¢å¼·ç‰ˆ - æ”¯æ´ç¾ä»£æ¨¡å‹)\nimport re\nfrom datetime import datetime\n\nclass SQLGenerator:\n    def __init__(self, schema: DatabaseSchema, model_approach=\"rag\"):\n        \"\"\"åˆå§‹åŒ–SQLç”Ÿæˆå™¨\n        \n        Args:\n            schema: æ•¸æ“šåº«æ¨¡å¼\n            model_approach: 'rag' (æª¢ç´¢å¢å¼·) æˆ– 'llm' (å¤§èªè¨€æ¨¡å‹)\n        \"\"\"\n        self.schema = schema\n        self.model_approach = model_approach\n        self.templates = self._load_sql_templates()\n        \n        # 2025å¹´æ¨è–¦çš„æ¨¡å‹é…ç½®\n        self.recommended_models = {\n            \"kaggle_friendly\": {\n                \"name\": \"Salesforce/codet5p-220m\",\n                \"size\": \"220M\",\n                \"memory\": \"~1GB\",\n                \"description\": \"æœ€é©åˆKaggleï¼Œå°ˆç‚ºä»£ç¢¼ç”Ÿæˆè¨­è¨ˆ\"\n            },\n            \"balanced\": {\n                \"name\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", \n                \"size\": \"1.1B\",\n                \"memory\": \"~3GB\",\n                \"description\": \"å¹³è¡¡æ•ˆèƒ½èˆ‡è³‡æºï¼Œæ”¯æ´QLoRA\"\n            },\n            \"production\": {\n                \"name\": \"mistralai/Mistral-7B-Instruct-v0.3\",\n                \"size\": \"7B\", \n                \"memory\": \"~14GB\",\n                \"description\": \"ç”Ÿç”¢ç´šåˆ¥ï¼Œéœ€è¦è¼ƒå¤šè³‡æº\"\n            }\n        }\n        \n        # å˜—è©¦è¼‰å…¥ç¾ä»£LLMï¼ˆå¦‚æœå¯ç”¨ï¼‰\n        self.llm_available = self._try_load_llm()\n        \n        print(f\"âœ“ SQLç”Ÿæˆå™¨åˆå§‹åŒ–å®Œæˆ\")\n        print(f\"  â€¢ ç”Ÿæˆæ–¹æ³•: {model_approach.upper()}\")\n        print(f\"  â€¢ LLMå¯ç”¨æ€§: {'æ˜¯' if self.llm_available else 'å¦ (ä½¿ç”¨æ¨¡æ¿æ–¹æ³•)'}\")\n    \n    def _try_load_llm(self):\n        \"\"\"å˜—è©¦è¼‰å…¥LLMæ¨¡å‹\"\"\"\n        if self.model_approach == \"rag\":\n            return False  # RAGæ–¹æ³•ä¸éœ€è¦è¼‰å…¥LLM\n        \n        try:\n            if TRANSFORMERS_AVAILABLE:\n                # é€™è£¡å¯ä»¥å¯¦éš›è¼‰å…¥æ¨¡å‹ï¼Œä½†ç‚ºäº†æ¼”ç¤ºå…ˆè·³é\n                print(\"  ğŸ’¡ æç¤º: è¦ä½¿ç”¨LLMç”Ÿæˆï¼Œè«‹å®‰è£ä¸¦è¼‰å…¥æ¨è–¦æ¨¡å‹\")\n                return True\n            return False\n        except Exception as e:\n            print(f\"  âš ï¸ LLMè¼‰å…¥å¤±æ•—: {e}\")\n            return False\n    \n    def get_model_recommendations(self) -> str:\n        \"\"\"ç²å–2025å¹´æ¨¡å‹æ¨è–¦\"\"\"\n        recommendations = \"\\nğŸ† 2025å¹´TextSQLæ¨¡å‹æ¨è–¦:\\n\"\n        recommendations += \"=\" * 50 + \"\\n\"\n        \n        for category, info in self.recommended_models.items():\n            recommendations += f\"\\nğŸ“± {category.upper()}:\\n\"\n            recommendations += f\"  â€¢ æ¨¡å‹: {info['name']}\\n\"\n            recommendations += f\"  â€¢ å¤§å°: {info['size']}\\n\" \n            recommendations += f\"  â€¢ è¨˜æ†¶é«”: {info['memory']}\\n\"\n            recommendations += f\"  â€¢ æè¿°: {info['description']}\\n\"\n        \n        recommendations += f\"\\nğŸ’¡ å¾®èª¿å»ºè­°:\\n\"\n        recommendations += f\"  â€¢ é¦–é¸: QLoRA (4-bité‡åŒ– + LoRA)\\n\"\n        recommendations += f\"  â€¢ å‚™é¸: LoRA (8-bité‡åŒ–)\\n\"\n        recommendations += f\"  â€¢ é«˜ç«¯: Full Fine-tuning\\n\"\n        \n        recommendations += f\"\\nğŸ¤” æ˜¯å¦éœ€è¦å¾®èª¿?\\n\"\n        recommendations += f\"  â€¢ RAGæ–¹æ³•: âŒ ç„¡éœ€å¾®èª¿ï¼Œæˆæœ¬ä½\\n\"\n        recommendations += f\"  â€¢ ç‰¹å®šé ˜åŸŸ: âœ… å¾®èª¿æå‡ç²¾åº¦\\n\"\n        recommendations += f\"  â€¢ é€šç”¨å ´æ™¯: âŒ é è¨“ç·´æ¨¡å‹å·²è¶³å¤ \\n\"\n        \n        return recommendations\n    \n    def _load_sql_templates(self) -> Dict[str, str]:\n        \"\"\"åŠ è¼‰SQLæ¨¡æ¿\"\"\"\n        return {\n            'select_all': \"SELECT * FROM {table};\",\n            'select_where': \"SELECT * FROM {table} WHERE {condition};\",\n            'count': \"SELECT COUNT(*) FROM {table};\",\n            'join': \"SELECT {columns} FROM {table1} t1 JOIN {table2} t2 ON {join_condition};\",\n            'group_by': \"SELECT {columns}, {aggregate} FROM {table} GROUP BY {group_columns};\",\n            'order_by': \"SELECT * FROM {table} ORDER BY {column} {direction};\",\n            'avg': \"SELECT AVG({column}) as avg_{column} FROM {table};\",\n            'sum': \"SELECT SUM({column}) as total_{column} FROM {table};\",\n            'max': \"SELECT MAX({column}) as max_{column} FROM {table};\",\n            'min': \"SELECT MIN({column}) as min_{column} FROM {table};\"\n        }\n    \n    def generate_sql(self, query: str, retrieval_results: Dict) -> Dict:\n        \"\"\"åŸºæ–¼æª¢ç´¢çµæœç”ŸæˆSQL\"\"\"\n        timestamp = datetime.now().isoformat()\n        \n        # 1. åˆ†ææŸ¥è©¢æ„åœ–\n        intent = self._analyze_intent(query)\n        \n        # 2. å¾æª¢ç´¢çµæœä¸­æå–æœ€ä½³åŒ¹é…\n        best_match = retrieval_results['similar_examples'][0] if retrieval_results['similar_examples'] else None\n        \n        # 3. æ ¹æ“šæ–¹æ³•ç”ŸæˆSQL\n        if self.model_approach == \"llm\" and self.llm_available:\n            generated_sql, method = self._llm_generation(query, retrieval_results, intent)\n        else:\n            generated_sql, method = self._rag_generation(query, retrieval_results, intent, best_match)\n        \n        # 4. é©—è­‰SQL\n        is_valid, validation_error = self._validate_sql(generated_sql)\n        \n        # 5. å¾Œè™•ç†\n        generated_sql = self._post_process_sql(generated_sql)\n        \n        return {\n            'query': query,\n            'generated_sql': generated_sql,\n            'method': method,\n            'intent': intent,\n            'is_valid': is_valid,\n            'validation_error': validation_error,\n            'best_match': best_match,\n            'timestamp': timestamp,\n            'model_approach': self.model_approach\n        }\n    \n    def _llm_generation(self, query: str, retrieval_results: Dict, intent: Dict) -> tuple:\n        \"\"\"ä½¿ç”¨LLMç”ŸæˆSQL (2025å¹´æ–¹æ³•)\"\"\"\n        # æ§‹å»ºæç¤ºè©\n        prompt = self._build_llm_prompt(query, retrieval_results, intent)\n        \n        # é€™è£¡æ‡‰è©²èª¿ç”¨å¯¦éš›çš„LLM API\n        # generated_sql = llm_model.generate(prompt)\n        \n        # ç‚ºæ¼”ç¤ºç›®çš„ï¼Œè¿”å›åŸºç¤ç”Ÿæˆçµæœ\n        generated_sql, _ = self._rag_generation(query, retrieval_results, intent, None)\n        return generated_sql, \"llm_enhanced\"\n    \n    def _rag_generation(self, query: str, retrieval_results: Dict, intent: Dict, best_match) -> tuple:\n        \"\"\"ä½¿ç”¨RAGæ–¹æ³•ç”ŸæˆSQL\"\"\"\n        if best_match and best_match['distance'] < 0.5:  # é«˜ç›¸ä¼¼åº¦\n            generated_sql = self._adapt_sql(best_match['metadata']['sql'], query, intent)\n            method = 'retrieval_based'\n        else:\n            generated_sql = self._template_based_generation(query, intent)\n            method = 'template_based'\n        \n        return generated_sql, method\n    \n    def _build_llm_prompt(self, query: str, retrieval_results: Dict, intent: Dict) -> str:\n        \"\"\"æ§‹å»ºLLMæç¤ºè© (2025å¹´æœ€ä½³å¯¦è¸)\"\"\"\n        prompt = f\"\"\"ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„SQLç”ŸæˆåŠ©æ‰‹ã€‚æ ¹æ“šä»¥ä¸‹ä¿¡æ¯ç”Ÿæˆæº–ç¢ºçš„SQLæŸ¥è©¢ã€‚\n\næ•¸æ“šåº«æ¶æ§‹:\n{self.schema.get_schema_info()}\n\nç”¨æˆ¶æŸ¥è©¢: {query}\n\nç›¸ä¼¼ç¤ºä¾‹:\n\"\"\"\n        \n        for i, example in enumerate(retrieval_results['similar_examples'][:2]):\n            prompt += f\"{i+1}. å•é¡Œ: {example['text']}\\n\"\n            prompt += f\"   SQL: {example['metadata']['sql']}\\n\\n\"\n        \n        prompt += \"\"\"è«‹ç”Ÿæˆå°æ‡‰çš„SQLæŸ¥è©¢ã€‚è¦æ±‚:\n1. èªæ³•æ­£ç¢º\n2. ç¬¦åˆæ•¸æ“šåº«æ¶æ§‹\n3. å›ç­”ç”¨æˆ¶å•é¡Œ\n4. åªè¿”å›SQLèªå¥ï¼Œä¸è¦é¡å¤–è§£é‡‹\n\nSQL:\"\"\"\n        \n        return prompt\n    \n    def _analyze_intent(self, query: str) -> Dict:\n        \"\"\"å¢å¼·çš„æŸ¥è©¢æ„åœ–åˆ†æ\"\"\"\n        query_lower = query.lower()\n        \n        intent = {\n            'action': 'select',\n            'tables': [],\n            'conditions': [],\n            'aggregation': None,\n            'sorting': None,\n            'limit': None,\n            'join_type': None\n        }\n        \n        # è­˜åˆ¥è¡¨å\n        for table in self.schema.tables.keys():\n            if table in query_lower or self._table_synonyms(table, query_lower):\n                intent['tables'].append(table)\n        \n        # è­˜åˆ¥èšåˆæ“ä½œ\n        aggregation_keywords = {\n            'çµ±è¨ˆ': 'count', 'è¨ˆç®—': 'count', 'ç¸½å’Œ': 'sum', \n            'å¹³å‡': 'avg', 'æœ€å¤§': 'max', 'æœ€å°': 'min',\n            'æ•¸é‡': 'count'\n        }\n        for keyword, agg_type in aggregation_keywords.items():\n            if keyword in query_lower:\n                intent['aggregation'] = agg_type\n                break\n        \n        # è­˜åˆ¥æ’åº\n        if any(word in query_lower for word in ['æœ€é«˜', 'æœ€å¤§', 'æœ€è²´', 'é™åº']):\n            intent['sorting'] = 'desc'\n        elif any(word in query_lower for word in ['æœ€ä½', 'æœ€å°', 'æœ€ä¾¿å®œ', 'å‡åº']):\n            intent['sorting'] = 'asc'\n        \n        # è­˜åˆ¥é™åˆ¶\n        if any(word in query_lower for word in ['å‰', 'æœ€', 'ç¬¬ä¸€']):\n            intent['limit'] = 1\n        \n        return intent\n    \n    def _table_synonyms(self, table: str, query: str) -> bool:\n        \"\"\"æ“´å±•çš„è¡¨ååŒç¾©è©\"\"\"\n        synonyms = {\n            'users': ['ç”¨æˆ¶', 'ä½¿ç”¨è€…', 'æœƒå“¡', 'å®¢æˆ¶', 'ç”¨æˆ·'],\n            'products': ['ç”¢å“', 'å•†å“', 'ç‰©å“', 'è²¨å“', 'äº§å“'],\n            'orders': ['è¨‚å–®', 'è¨‚è³¼', 'è³¼è²·', 'äº¤æ˜“', 'è®¢å•']\n        }\n        \n        if table in synonyms:\n            return any(syn in query for syn in synonyms[table])\n        return False\n    \n    def _adapt_sql(self, base_sql: str, query: str, intent: Dict) -> str:\n        \"\"\"æ™ºèƒ½SQLé©é…\"\"\"\n        adapted_sql = base_sql\n        \n        # æ ¹æ“šæ„åœ–èª¿æ•´SQL\n        if intent['limit'] and 'LIMIT' not in adapted_sql.upper():\n            adapted_sql = adapted_sql.rstrip(';') + f\" LIMIT {intent['limit']};\"\n        \n        return adapted_sql\n    \n    def _template_based_generation(self, query: str, intent: Dict) -> str:\n        \"\"\"å¢å¼·çš„æ¨¡æ¿ç”Ÿæˆ\"\"\"\n        if not intent['tables']:\n            return \"SELECT 'No table identified' as error;\"\n        \n        table = intent['tables'][0]\n        \n        # èšåˆæŸ¥è©¢\n        if intent['aggregation']:\n            agg_func = intent['aggregation'].upper()\n            if agg_func == 'COUNT':\n                sql = f\"SELECT COUNT(*) as total_count FROM {table};\"\n            else:\n                numeric_columns = self._get_numeric_columns(table)\n                if numeric_columns:\n                    column = numeric_columns[0]\n                    sql = f\"SELECT {agg_func}({column}) as {agg_func.lower()}_{column} FROM {table};\"\n                else:\n                    sql = f\"SELECT COUNT(*) as total_count FROM {table};\"\n        \n        # æ’åºæŸ¥è©¢\n        elif intent['sorting']:\n            numeric_columns = self._get_numeric_columns(table)\n            if numeric_columns:\n                column = numeric_columns[0]\n                direction = intent['sorting'].upper()\n                sql = f\"SELECT * FROM {table} ORDER BY {column} {direction};\"\n                if intent['limit']:\n                    sql = sql.rstrip(';') + f\" LIMIT {intent['limit']};\"\n            else:\n                sql = f\"SELECT * FROM {table};\"\n        \n        # åŸºæœ¬æŸ¥è©¢\n        else:\n            sql = f\"SELECT * FROM {table};\"\n            if intent['limit']:\n                sql = sql.rstrip(';') + f\" LIMIT {intent['limit']};\"\n        \n        return sql\n    \n    def _get_numeric_columns(self, table: str) -> List[str]:\n        \"\"\"ç²å–æ•¸å€¼åˆ—\"\"\"\n        if table not in self.schema.tables:\n            return []\n        \n        numeric_types = ['INT', 'DECIMAL', 'FLOAT', 'DOUBLE', 'NUMERIC']\n        numeric_columns = []\n        \n        for column, column_type in self.schema.tables[table].items():\n            if any(num_type in column_type.upper() for num_type in numeric_types):\n                numeric_columns.append(column)\n        \n        return numeric_columns\n    \n    def _post_process_sql(self, sql: str) -> str:\n        \"\"\"SQLå¾Œè™•ç†\"\"\"\n        # ç¢ºä¿ä»¥åˆ†è™Ÿçµå°¾\n        sql = sql.strip()\n        if not sql.endswith(';'):\n            sql += ';'\n        \n        # æ ¼å¼åŒ–\n        sql = re.sub(r'\\s+', ' ', sql)  # çµ±ä¸€ç©ºæ ¼\n        \n        return sql\n    \n    def _validate_sql(self, sql: str) -> Tuple[bool, str]:\n        \"\"\"å¢å¼·çš„SQLé©—è­‰\"\"\"\n        try:\n            parsed = sqlparse.parse(sql)[0]\n            if parsed.tokens:\n                # åŸºæœ¬èªæ³•æª¢æŸ¥\n                sql_upper = sql.upper()\n                if not any(keyword in sql_upper for keyword in ['SELECT', 'INSERT', 'UPDATE', 'DELETE']):\n                    return False, \"ä¸åŒ…å«æœ‰æ•ˆçš„SQLé—œéµå­—\"\n                \n                return True, \"\"\n            else:\n                return False, \"ç©ºçš„SQLèªå¥\"\n        except Exception as e:\n            return False, str(e)\n\n# åˆå§‹åŒ–å¢å¼·ç‰ˆSQLç”Ÿæˆå™¨\nsql_generator = SQLGenerator(schema, model_approach=\"rag\")\n\n# é¡¯ç¤ºæ¨¡å‹æ¨è–¦\nprint(sql_generator.get_model_recommendations())\n\n# æ¸¬è©¦å¢å¼·ç‰ˆSQLç”Ÿæˆ\ntest_queries = [\n    \"é¡¯ç¤ºæ‰€æœ‰ç”¢å“ä¿¡æ¯\",\n    \"æ‰¾å‡ºæœ€è²´çš„ç”¢å“\", \n    \"çµ±è¨ˆç”¨æˆ¶æ•¸é‡\",\n    \"è¨ˆç®—ç”¢å“å¹³å‡åƒ¹æ ¼\",\n    \"é¡¯ç¤ºå‰3å€‹æœ€è²´çš„ç”¢å“\"\n]\n\nprint(f\"\\nğŸ§ª å¢å¼·ç‰ˆSQLç”Ÿæˆæ¸¬è©¦:\")\nprint(\"=\" * 50)\n\nfor query in test_queries:\n    retrieval_results = retriever.retrieve(query)\n    generation_results = sql_generator.generate_sql(query, retrieval_results)\n    \n    print(f\"\\nğŸ” æŸ¥è©¢: {query}\")\n    print(f\"ğŸ“ ç”ŸæˆSQL: {generation_results['generated_sql']}\")\n    print(f\"ğŸ”§ æ–¹æ³•: {generation_results['method']}\")\n    print(f\"ğŸ¯ æ„åœ–: {generation_results['intent']}\")\n    print(f\"âœ… æœ‰æ•ˆ: {generation_results['is_valid']}\")\n    \n    if generation_results['validation_error']:\n        print(f\"âŒ éŒ¯èª¤: {generation_results['validation_error']}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. å®Œæ•´æµæ°´ç·š\n",
    "\n",
    "### æ•´åˆæ‰€æœ‰çµ„ä»¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®Œæ•´çš„TextSQL RAGæµæ°´ç·š\n",
    "class TextSQLRAGPipeline:\n",
    "    def __init__(self, \n",
    "                 embedder: TextEmbedder,\n",
    "                 vector_index: VectorIndex,\n",
    "                 schema: DatabaseSchema,\n",
    "                 retriever: SemanticRetriever,\n",
    "                 sql_generator: SQLGenerator):\n",
    "        \"\"\"åˆå§‹åŒ–å®Œæ•´çš„RAGæµæ°´ç·š\"\"\"\n",
    "        self.embedder = embedder\n",
    "        self.vector_index = vector_index\n",
    "        self.schema = schema\n",
    "        self.retriever = retriever\n",
    "        self.sql_generator = sql_generator\n",
    "        self.query_history = []\n",
    "    \n",
    "    def process_query(self, natural_language_query: str, execute_sql: bool = False) -> Dict:\n",
    "        \"\"\"è™•ç†è‡ªç„¶èªè¨€æŸ¥è©¢çš„å®Œæ•´æµç¨‹\"\"\"\n",
    "        print(f\"\\nè™•ç†æŸ¥è©¢: {natural_language_query}\")\n",
    "        \n",
    "        # ç¬¬1æ­¥ï¼šæª¢ç´¢ç›¸é—œç¤ºä¾‹\n",
    "        print(\"ç¬¬1æ­¥ï¼šæª¢ç´¢ç›¸é—œç¤ºä¾‹...\")\n",
    "        retrieval_results = self.retriever.retrieve(natural_language_query)\n",
    "        \n",
    "        # ç¬¬2æ­¥ï¼šç”ŸæˆSQL\n",
    "        print(\"ç¬¬2æ­¥ï¼šç”ŸæˆSQL...\")\n",
    "        generation_results = self.sql_generator.generate_sql(natural_language_query, retrieval_results)\n",
    "        \n",
    "        # ç¬¬3æ­¥ï¼šåŸ·è¡ŒSQLï¼ˆå¯é¸ï¼‰\n",
    "        execution_results = None\n",
    "        if execute_sql and generation_results['is_valid']:\n",
    "            print(\"ç¬¬3æ­¥ï¼šåŸ·è¡ŒSQL...\")\n",
    "            execution_results = self._execute_sql(generation_results['generated_sql'])\n",
    "        \n",
    "        # ç¬¬4æ­¥ï¼šæ§‹å»ºå®Œæ•´çµæœ\n",
    "        complete_results = {\n",
    "            'natural_language_query': natural_language_query,\n",
    "            'retrieval_results': retrieval_results,\n",
    "            'generation_results': generation_results,\n",
    "            'execution_results': execution_results,\n",
    "            'pipeline_success': generation_results['is_valid'],\n",
    "            'timestamp': pd.Timestamp.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        # ç¬¬5æ­¥ï¼šè¨˜éŒ„æŸ¥è©¢æ­·å²\n",
    "        self.query_history.append(complete_results)\n",
    "        \n",
    "        return complete_results\n",
    "    \n",
    "    def _execute_sql(self, sql: str) -> Dict:\n",
    "        \"\"\"åŸ·è¡ŒSQLæŸ¥è©¢\"\"\"\n",
    "        try:\n",
    "            conn = sqlite3.connect('sample_ecommerce.db')\n",
    "            \n",
    "            # åŸ·è¡ŒæŸ¥è©¢\n",
    "            result_df = pd.read_sql_query(sql, conn)\n",
    "            \n",
    "            conn.close()\n",
    "            \n",
    "            return {\n",
    "                'success': True,\n",
    "                'data': result_df.to_dict('records'),\n",
    "                'row_count': len(result_df),\n",
    "                'columns': list(result_df.columns),\n",
    "                'error': None\n",
    "            }\n",
    "        \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'success': False,\n",
    "                'data': None,\n",
    "                'row_count': 0,\n",
    "                'columns': [],\n",
    "                'error': str(e)\n",
    "            }\n",
    "    \n",
    "    def explain_results(self, results: Dict) -> str:\n",
    "        \"\"\"è§£é‡‹è™•ç†çµæœ\"\"\"\n",
    "        explanation = f\"\\n{'='*50}\\n\"\n",
    "        explanation += f\"æŸ¥è©¢: {results['natural_language_query']}\\n\"\n",
    "        explanation += f\"{'='*50}\\n\"\n",
    "        \n",
    "        # æª¢ç´¢éšæ®µ\n",
    "        explanation += \"\\nğŸ” æª¢ç´¢éšæ®µ:\\n\"\n",
    "        retrieval = results['retrieval_results']\n",
    "        explanation += f\"æ‰¾åˆ° {len(retrieval['similar_examples'])} å€‹ç›¸ä¼¼ç¤ºä¾‹\\n\"\n",
    "        explanation += f\"æª¢ç´¢åˆ†æ•¸: {retrieval['retrieval_score']:.3f}\\n\"\n",
    "        \n",
    "        if retrieval['similar_examples']:\n",
    "            best_match = retrieval['similar_examples'][0]\n",
    "            explanation += f\"æœ€ä½³åŒ¹é…: {best_match['text']} (ç›¸ä¼¼åº¦: {1-best_match['distance']:.3f})\\n\"\n",
    "        \n",
    "        # ç”Ÿæˆéšæ®µ\n",
    "        explanation += \"\\nâš™ï¸ ç”Ÿæˆéšæ®µ:\\n\"\n",
    "        generation = results['generation_results']\n",
    "        explanation += f\"ç”Ÿæˆæ–¹æ³•: {generation['method']}\\n\"\n",
    "        explanation += f\"ç”Ÿæˆçš„SQL: {generation['generated_sql']}\\n\"\n",
    "        explanation += f\"SQLæœ‰æ•ˆæ€§: {generation['is_valid']}\\n\"\n",
    "        \n",
    "        # åŸ·è¡Œéšæ®µ\n",
    "        if results['execution_results']:\n",
    "            explanation += \"\\nğŸš€ åŸ·è¡Œéšæ®µ:\\n\"\n",
    "            execution = results['execution_results']\n",
    "            if execution['success']:\n",
    "                explanation += f\"åŸ·è¡ŒæˆåŠŸï¼Œè¿”å› {execution['row_count']} è¡Œæ•¸æ“š\\n\"\n",
    "                if execution['data']:\n",
    "                    explanation += \"å‰å¹¾è¡Œæ•¸æ“š:\\n\"\n",
    "                    for i, row in enumerate(execution['data'][:3]):\n",
    "                        explanation += f\"  {i+1}: {row}\\n\"\n",
    "            else:\n",
    "                explanation += f\"åŸ·è¡Œå¤±æ•—: {execution['error']}\\n\"\n",
    "        \n",
    "        explanation += f\"\\nâœ… æµæ°´ç·šæˆåŠŸ: {results['pipeline_success']}\\n\"\n",
    "        \n",
    "        return explanation\n",
    "    \n",
    "    def get_pipeline_stats(self) -> Dict:\n",
    "        \"\"\"ç²å–æµæ°´ç·šçµ±è¨ˆä¿¡æ¯\"\"\"\n",
    "        if not self.query_history:\n",
    "            return {'total_queries': 0}\n",
    "        \n",
    "        successful_queries = sum(1 for q in self.query_history if q['pipeline_success'])\n",
    "        \n",
    "        return {\n",
    "            'total_queries': len(self.query_history),\n",
    "            'successful_queries': successful_queries,\n",
    "            'success_rate': successful_queries / len(self.query_history),\n",
    "            'avg_retrieval_score': np.mean([q['retrieval_results']['retrieval_score'] \n",
    "                                          for q in self.query_history])\n",
    "        }\n",
    "\n",
    "# åˆå§‹åŒ–å®Œæ•´æµæ°´ç·š\n",
    "pipeline = TextSQLRAGPipeline(\n",
    "    embedder=embedder,\n",
    "    vector_index=vector_index,\n",
    "    schema=schema,\n",
    "    retriever=retriever,\n",
    "    sql_generator=sql_generator\n",
    ")\n",
    "\n",
    "print(\"âœ… TextSQL RAG æµæ°´ç·šåˆå§‹åŒ–å®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æµæ°´ç·šæ¼”ç¤º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æµæ°´ç·šæ¼”ç¤º\n",
    "demo_queries = [\n",
    "    \"é¡¯ç¤ºæ‰€æœ‰ç”¨æˆ¶çš„åŸºæœ¬ä¿¡æ¯\",\n",
    "    \"æŸ¥æ‰¾åƒ¹æ ¼æœ€é«˜çš„ç”¢å“\",\n",
    "    \"çµ±è¨ˆæ¯å€‹é¡åˆ¥æœ‰å¤šå°‘ç”¢å“\",\n",
    "    \"é¡¯ç¤ºå¼µä¸‰è³¼è²·çš„æ‰€æœ‰å•†å“\",\n",
    "    \"æ‰¾å‡ºé‚„æ²’æœ‰ä¸‹éè¨‚å–®çš„ç”¨æˆ¶\"\n",
    "]\n",
    "\n",
    "print(\"ğŸš€ é–‹å§‹æµæ°´ç·šæ¼”ç¤º\\n\")\n",
    "\n",
    "for i, query in enumerate(demo_queries, 1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"æ¼”ç¤º {i}/{len(demo_queries)}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # è™•ç†æŸ¥è©¢\n",
    "    results = pipeline.process_query(query, execute_sql=True)\n",
    "    \n",
    "    # é¡¯ç¤ºçµæœ\n",
    "    print(pipeline.explain_results(results))\n",
    "    \n",
    "    # æš«åœä¸€ä¸‹è®“è¼¸å‡ºæ›´æ¸…æ™°\n",
    "    import time\n",
    "    time.sleep(1)\n",
    "\n",
    "# é¡¯ç¤ºæ•´é«”çµ±è¨ˆ\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"æµæ°´ç·šçµ±è¨ˆ\")\n",
    "print(\"=\"*60)\n",
    "stats = pipeline.get_pipeline_stats()\n",
    "for key, value in stats.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. è©•ä¼°èˆ‡å„ªåŒ–\n",
    "\n",
    "### è©•ä¼°æŒ‡æ¨™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è©•ä¼°ç³»çµ±\n",
    "class RAGEvaluator:\n",
    "    def __init__(self, pipeline: TextSQLRAGPipeline):\n",
    "        \"\"\"åˆå§‹åŒ–è©•ä¼°å™¨\"\"\"\n",
    "        self.pipeline = pipeline\n",
    "    \n",
    "    def evaluate_retrieval(self, test_cases: List[Dict]) -> Dict:\n",
    "        \"\"\"è©•ä¼°æª¢ç´¢æ€§èƒ½\"\"\"\n",
    "        retrieval_scores = []\n",
    "        precision_scores = []\n",
    "        \n",
    "        for test_case in test_cases:\n",
    "            query = test_case['natural_language']\n",
    "            expected_sql = test_case['sql']\n",
    "            \n",
    "            # æª¢ç´¢ç›¸é—œç¤ºä¾‹\n",
    "            retrieval_results = self.pipeline.retriever.retrieve(query)\n",
    "            \n",
    "            # è¨ˆç®—æª¢ç´¢åˆ†æ•¸\n",
    "            retrieval_scores.append(retrieval_results['retrieval_score'])\n",
    "            \n",
    "            # è¨ˆç®—ç²¾ç¢ºåº¦ï¼ˆæª¢ç´¢çµæœä¸­æ˜¯å¦åŒ…å«æ­£ç¢ºç­”æ¡ˆï¼‰\n",
    "            precision = self._calculate_precision(retrieval_results, expected_sql)\n",
    "            precision_scores.append(precision)\n",
    "        \n",
    "        return {\n",
    "            'avg_retrieval_score': np.mean(retrieval_scores),\n",
    "            'avg_precision': np.mean(precision_scores),\n",
    "            'retrieval_scores': retrieval_scores,\n",
    "            'precision_scores': precision_scores\n",
    "        }\n",
    "    \n",
    "    def evaluate_generation(self, test_cases: List[Dict]) -> Dict:\n",
    "        \"\"\"è©•ä¼°SQLç”Ÿæˆæ€§èƒ½\"\"\"\n",
    "        exact_match_scores = []\n",
    "        syntax_valid_scores = []\n",
    "        semantic_similarity_scores = []\n",
    "        \n",
    "        for test_case in test_cases:\n",
    "            query = test_case['natural_language']\n",
    "            expected_sql = test_case['sql']\n",
    "            \n",
    "            # ç”ŸæˆSQL\n",
    "            results = self.pipeline.process_query(query, execute_sql=False)\n",
    "            generated_sql = results['generation_results']['generated_sql']\n",
    "            \n",
    "            # ç²¾ç¢ºåŒ¹é…\n",
    "            exact_match = self._normalize_sql(generated_sql) == self._normalize_sql(expected_sql)\n",
    "            exact_match_scores.append(exact_match)\n",
    "            \n",
    "            # èªæ³•æœ‰æ•ˆæ€§\n",
    "            syntax_valid = results['generation_results']['is_valid']\n",
    "            syntax_valid_scores.append(syntax_valid)\n",
    "            \n",
    "            # èªç¾©ç›¸ä¼¼åº¦ï¼ˆç°¡åŒ–å¯¦ç¾ï¼‰\n",
    "            semantic_sim = self._calculate_semantic_similarity(generated_sql, expected_sql)\n",
    "            semantic_similarity_scores.append(semantic_sim)\n",
    "        \n",
    "        return {\n",
    "            'exact_match_rate': np.mean(exact_match_scores),\n",
    "            'syntax_valid_rate': np.mean(syntax_valid_scores),\n",
    "            'avg_semantic_similarity': np.mean(semantic_similarity_scores),\n",
    "            'exact_match_scores': exact_match_scores,\n",
    "            'syntax_valid_scores': syntax_valid_scores,\n",
    "            'semantic_similarity_scores': semantic_similarity_scores\n",
    "        }\n",
    "    \n",
    "    def evaluate_end_to_end(self, test_cases: List[Dict]) -> Dict:\n",
    "        \"\"\"ç«¯åˆ°ç«¯è©•ä¼°\"\"\"\n",
    "        execution_success_scores = []\n",
    "        result_accuracy_scores = []\n",
    "        \n",
    "        for test_case in test_cases:\n",
    "            query = test_case['natural_language']\n",
    "            expected_sql = test_case['sql']\n",
    "            \n",
    "            # åŸ·è¡Œå®Œæ•´æµæ°´ç·š\n",
    "            results = self.pipeline.process_query(query, execute_sql=True)\n",
    "            \n",
    "            # åŸ·è¡ŒæˆåŠŸç‡\n",
    "            execution_success = (\n",
    "                results['execution_results'] is not None and \n",
    "                results['execution_results']['success']\n",
    "            )\n",
    "            execution_success_scores.append(execution_success)\n",
    "            \n",
    "            # çµæœå‡†ç¢ºæ€§ï¼ˆé€šéåŸ·è¡ŒæœŸæœ›SQLé€²è¡Œæ¯”è¼ƒï¼‰\n",
    "            if execution_success:\n",
    "                result_accuracy = self._compare_execution_results(\n",
    "                    results['generation_results']['generated_sql'],\n",
    "                    expected_sql\n",
    "                )\n",
    "                result_accuracy_scores.append(result_accuracy)\n",
    "            else:\n",
    "                result_accuracy_scores.append(0.0)\n",
    "        \n",
    "        return {\n",
    "            'execution_success_rate': np.mean(execution_success_scores),\n",
    "            'result_accuracy_rate': np.mean(result_accuracy_scores),\n",
    "            'execution_success_scores': execution_success_scores,\n",
    "            'result_accuracy_scores': result_accuracy_scores\n",
    "        }\n",
    "    \n",
    "    def _calculate_precision(self, retrieval_results: Dict, expected_sql: str) -> float:\n",
    "        \"\"\"è¨ˆç®—æª¢ç´¢ç²¾ç¢ºåº¦\"\"\"\n",
    "        similar_examples = retrieval_results['similar_examples']\n",
    "        if not similar_examples:\n",
    "            return 0.0\n",
    "        \n",
    "        # æª¢æŸ¥æ˜¯å¦æœ‰æª¢ç´¢çµæœèˆ‡æœŸæœ›SQLç›¸ä¼¼\n",
    "        for example in similar_examples:\n",
    "            if self._sql_similarity(example['metadata']['sql'], expected_sql) > 0.8:\n",
    "                return 1.0\n",
    "        \n",
    "        return 0.0\n",
    "    \n",
    "    def _normalize_sql(self, sql: str) -> str:\n",
    "        \"\"\"æ¨™æº–åŒ–SQLå­—ç¬¦ä¸²\"\"\"\n",
    "        # ç§»é™¤å¤šé¤˜ç©ºæ ¼ï¼Œè½‰æ›ç‚ºå°å¯«\n",
    "        return ' '.join(sql.lower().split())\n",
    "    \n",
    "    def _calculate_semantic_similarity(self, sql1: str, sql2: str) -> float:\n",
    "        \"\"\"è¨ˆç®—SQLèªç¾©ç›¸ä¼¼åº¦\"\"\"\n",
    "        # ç°¡åŒ–å¯¦ç¾ï¼šåŸºæ–¼é—œéµè©é‡ç–Š\n",
    "        words1 = set(self._normalize_sql(sql1).split())\n",
    "        words2 = set(self._normalize_sql(sql2).split())\n",
    "        \n",
    "        if not words1 and not words2:\n",
    "            return 1.0\n",
    "        if not words1 or not words2:\n",
    "            return 0.0\n",
    "        \n",
    "        intersection = words1.intersection(words2)\n",
    "        union = words1.union(words2)\n",
    "        \n",
    "        return len(intersection) / len(union)\n",
    "    \n",
    "    def _sql_similarity(self, sql1: str, sql2: str) -> float:\n",
    "        \"\"\"è¨ˆç®—SQLç›¸ä¼¼åº¦\"\"\"\n",
    "        return self._calculate_semantic_similarity(sql1, sql2)\n",
    "    \n",
    "    def _compare_execution_results(self, generated_sql: str, expected_sql: str) -> float:\n",
    "        \"\"\"æ¯”è¼ƒåŸ·è¡Œçµæœ\"\"\"\n",
    "        try:\n",
    "            conn = sqlite3.connect('sample_ecommerce.db')\n",
    "            \n",
    "            # åŸ·è¡Œå…©å€‹SQL\n",
    "            result1 = pd.read_sql_query(generated_sql, conn)\n",
    "            result2 = pd.read_sql_query(expected_sql, conn)\n",
    "            \n",
    "            conn.close()\n",
    "            \n",
    "            # æ¯”è¼ƒçµæœ\n",
    "            if result1.equals(result2):\n",
    "                return 1.0\n",
    "            elif len(result1) == len(result2) and len(result1.columns) == len(result2.columns):\n",
    "                return 0.5  # éƒ¨åˆ†åŒ¹é…\n",
    "            else:\n",
    "                return 0.0\n",
    "                \n",
    "        except Exception:\n",
    "            return 0.0\n",
    "    \n",
    "    def generate_evaluation_report(self, test_cases: List[Dict]) -> str:\n",
    "        \"\"\"ç”Ÿæˆè©•ä¼°å ±å‘Š\"\"\"\n",
    "        retrieval_eval = self.evaluate_retrieval(test_cases)\n",
    "        generation_eval = self.evaluate_generation(test_cases)\n",
    "        e2e_eval = self.evaluate_end_to_end(test_cases)\n",
    "        \n",
    "        report = \"\\n\" + \"=\"*50 + \"\\n\"\n",
    "        report += \"TextSQL RAG Pipeline è©•ä¼°å ±å‘Š\\n\"\n",
    "        report += \"=\"*50 + \"\\n\"\n",
    "        \n",
    "        report += \"\\nğŸ“Š æª¢ç´¢æ€§èƒ½:\\n\"\n",
    "        report += f\"  å¹³å‡æª¢ç´¢åˆ†æ•¸: {retrieval_eval['avg_retrieval_score']:.3f}\\n\"\n",
    "        report += f\"  å¹³å‡ç²¾ç¢ºåº¦: {retrieval_eval['avg_precision']:.3f}\\n\"\n",
    "        \n",
    "        report += \"\\nğŸ”§ ç”Ÿæˆæ€§èƒ½:\\n\"\n",
    "        report += f\"  ç²¾ç¢ºåŒ¹é…ç‡: {generation_eval['exact_match_rate']:.3f}\\n\"\n",
    "        report += f\"  èªæ³•æœ‰æ•ˆç‡: {generation_eval['syntax_valid_rate']:.3f}\\n\"\n",
    "        report += f\"  å¹³å‡èªç¾©ç›¸ä¼¼åº¦: {generation_eval['avg_semantic_similarity']:.3f}\\n\"\n",
    "        \n",
    "        report += \"\\nğŸš€ ç«¯åˆ°ç«¯æ€§èƒ½:\\n\"\n",
    "        report += f\"  åŸ·è¡ŒæˆåŠŸç‡: {e2e_eval['execution_success_rate']:.3f}\\n\"\n",
    "        report += f\"  çµæœå‡†ç¢ºç‡: {e2e_eval['result_accuracy_rate']:.3f}\\n\"\n",
    "        \n",
    "        # ç¸½é«”è©•åˆ†\n",
    "        overall_score = np.mean([\n",
    "            retrieval_eval['avg_retrieval_score'],\n",
    "            generation_eval['syntax_valid_rate'],\n",
    "            e2e_eval['execution_success_rate']\n",
    "        ])\n",
    "        \n",
    "        report += f\"\\nâ­ ç¸½é«”è©•åˆ†: {overall_score:.3f}\\n\"\n",
    "        \n",
    "        return report\n",
    "\n",
    "# å‰µå»ºè©•ä¼°å™¨\n",
    "evaluator = RAGEvaluator(pipeline)\n",
    "\n",
    "# é‹è¡Œè©•ä¼°\n",
    "print(\"ğŸ” é–‹å§‹è©•ä¼°æµæ°´ç·šæ€§èƒ½...\")\n",
    "evaluation_report = evaluator.generate_evaluation_report(training_data)\n",
    "print(evaluation_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. å¯¦éš›æ‡‰ç”¨æ¡ˆä¾‹\n",
    "\n",
    "### äº’å‹•å¼æŸ¥è©¢ç•Œé¢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# äº’å‹•å¼æŸ¥è©¢ç•Œé¢\n",
    "class InteractiveQueryInterface:\n",
    "    def __init__(self, pipeline: TextSQLRAGPipeline):\n",
    "        \"\"\"åˆå§‹åŒ–äº’å‹•å¼ç•Œé¢\"\"\"\n",
    "        self.pipeline = pipeline\n",
    "        self.session_history = []\n",
    "    \n",
    "    def start_session(self):\n",
    "        \"\"\"é–‹å§‹äº’å‹•æœƒè©±\"\"\"\n",
    "        print(\"\\nğŸ¯ æ­¡è¿ä½¿ç”¨ TextSQL RAG æŸ¥è©¢ç³»çµ±ï¼\")\n",
    "        print(\"è¼¸å…¥è‡ªç„¶èªè¨€æŸ¥è©¢ï¼Œç³»çµ±å°‡ç”Ÿæˆå°æ‡‰çš„SQLèªå¥ä¸¦åŸ·è¡Œ\")\n",
    "        print(\"è¼¸å…¥ 'help' æŸ¥çœ‹å¹«åŠ©ï¼Œè¼¸å…¥ 'quit' é€€å‡ºç³»çµ±\\n\")\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                user_input = input(\"ğŸ” è«‹è¼¸å…¥æ‚¨çš„æŸ¥è©¢: \").strip()\n",
    "                \n",
    "                if user_input.lower() == 'quit':\n",
    "                    print(\"ğŸ‘‹ æ„Ÿè¬ä½¿ç”¨ï¼Œå†è¦‹ï¼\")\n",
    "                    break\n",
    "                elif user_input.lower() == 'help':\n",
    "                    self._show_help()\n",
    "                    continue\n",
    "                elif user_input.lower() == 'history':\n",
    "                    self._show_history()\n",
    "                    continue\n",
    "                elif user_input.lower() == 'schema':\n",
    "                    self._show_schema()\n",
    "                    continue\n",
    "                elif not user_input:\n",
    "                    continue\n",
    "                \n",
    "                # è™•ç†æŸ¥è©¢\n",
    "                self._process_interactive_query(user_input)\n",
    "                \n",
    "            except KeyboardInterrupt:\n",
    "                print(\"\\nğŸ‘‹ ç”¨æˆ¶ä¸­æ–·ï¼Œå†è¦‹ï¼\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ ç™¼ç”ŸéŒ¯èª¤: {e}\")\n",
    "    \n",
    "    def _process_interactive_query(self, query: str):\n",
    "        \"\"\"è™•ç†äº’å‹•æŸ¥è©¢\"\"\"\n",
    "        print(f\"\\nâ³ è™•ç†ä¸­...\")\n",
    "        \n",
    "        # åŸ·è¡ŒæŸ¥è©¢\n",
    "        results = self.pipeline.process_query(query, execute_sql=True)\n",
    "        \n",
    "        # è¨˜éŒ„æœƒè©±æ­·å²\n",
    "        self.session_history.append(results)\n",
    "        \n",
    "        # é¡¯ç¤ºçµæœ\n",
    "        self._display_results(results)\n",
    "    \n",
    "    def _display_results(self, results: Dict):\n",
    "        \"\"\"é¡¯ç¤ºæŸ¥è©¢çµæœ\"\"\"\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(f\"ğŸ“ æŸ¥è©¢: {results['natural_language_query']}\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        # é¡¯ç¤ºç”Ÿæˆçš„SQL\n",
    "        generation = results['generation_results']\n",
    "        print(f\"\\nğŸ’» ç”Ÿæˆçš„SQL:\")\n",
    "        print(f\"```sql\\n{generation['generated_sql']}\\n```\")\n",
    "        \n",
    "        print(f\"\\nğŸ”§ ç”Ÿæˆæ–¹æ³•: {generation['method']}\")\n",
    "        print(f\"âœ… SQLæœ‰æ•ˆæ€§: {generation['is_valid']}\")\n",
    "        \n",
    "        # é¡¯ç¤ºåŸ·è¡Œçµæœ\n",
    "        if results['execution_results']:\n",
    "            execution = results['execution_results']\n",
    "            if execution['success']:\n",
    "                print(f\"\\nğŸ¯ åŸ·è¡Œçµæœ: æˆåŠŸè¿”å› {execution['row_count']} è¡Œæ•¸æ“š\")\n",
    "                \n",
    "                if execution['data']:\n",
    "                    # å°‡çµæœæ ¼å¼åŒ–ç‚ºè¡¨æ ¼\n",
    "                    df = pd.DataFrame(execution['data'])\n",
    "                    print(\"\\nğŸ“Š æŸ¥è©¢çµæœ:\")\n",
    "                    print(df.to_string(index=False))\n",
    "                else:\n",
    "                    print(\"\\nğŸ“Š æŸ¥è©¢çµæœ: ç„¡æ•¸æ“šè¿”å›\")\n",
    "            else:\n",
    "                print(f\"\\nâŒ åŸ·è¡Œå¤±æ•—: {execution['error']}\")\n",
    "        \n",
    "        # é¡¯ç¤ºæª¢ç´¢ä¿¡æ¯\n",
    "        retrieval = results['retrieval_results']\n",
    "        if retrieval['similar_examples']:\n",
    "            best_match = retrieval['similar_examples'][0]\n",
    "            print(f\"\\nğŸ” æœ€ä½³åŒ¹é…ç¤ºä¾‹: {best_match['text']}\")\n",
    "            print(f\"ğŸ“ˆ ç›¸ä¼¼åº¦: {1-best_match['distance']:.3f}\")\n",
    "    \n",
    "    def _show_help(self):\n",
    "        \"\"\"é¡¯ç¤ºå¹«åŠ©ä¿¡æ¯\"\"\"\n",
    "        help_text = \"\"\"\n",
    "ğŸ“š ä½¿ç”¨å¹«åŠ©:\n",
    "\n",
    "åŸºæœ¬æŸ¥è©¢ç¤ºä¾‹:\n",
    "  â€¢ \"é¡¯ç¤ºæ‰€æœ‰ç”¨æˆ¶\" - æŸ¥è©¢ç”¨æˆ¶è¡¨\n",
    "  â€¢ \"æ‰¾å‡ºæœ€è²´çš„ç”¢å“\" - æŒ‰åƒ¹æ ¼æ’åº\n",
    "  â€¢ \"çµ±è¨ˆæ¯å€‹é¡åˆ¥çš„ç”¢å“æ•¸é‡\" - èšåˆæŸ¥è©¢\n",
    "  â€¢ \"é¡¯ç¤ºå¼µä¸‰çš„æ‰€æœ‰è¨‚å–®\" - è¯åˆæŸ¥è©¢\n",
    "\n",
    "ç‰¹æ®Šå‘½ä»¤:\n",
    "  â€¢ help - é¡¯ç¤ºæ­¤å¹«åŠ©\n",
    "  â€¢ history - é¡¯ç¤ºæŸ¥è©¢æ­·å²\n",
    "  â€¢ schema - é¡¯ç¤ºæ•¸æ“šåº«çµæ§‹\n",
    "  â€¢ quit - é€€å‡ºç³»çµ±\n",
    "\n",
    "ğŸ’¡ æç¤º: ç›¡é‡ä½¿ç”¨è‡ªç„¶èªè¨€æè¿°æ‚¨çš„æŸ¥è©¢éœ€æ±‚ï¼\n",
    "\"\"\"\n",
    "        print(help_text)\n",
    "    \n",
    "    def _show_history(self):\n",
    "        \"\"\"é¡¯ç¤ºæŸ¥è©¢æ­·å²\"\"\"\n",
    "        if not self.session_history:\n",
    "            print(\"\\nğŸ“ æš«ç„¡æŸ¥è©¢æ­·å²\")\n",
    "            return\n",
    "        \n",
    "        print(f\"\\nğŸ“ æŸ¥è©¢æ­·å² (å…± {len(self.session_history)} æ¢):\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        for i, result in enumerate(self.session_history[-5:], 1):  # åªé¡¯ç¤ºæœ€è¿‘5æ¢\n",
    "            query = result['natural_language_query']\n",
    "            sql = result['generation_results']['generated_sql']\n",
    "            success = result['pipeline_success']\n",
    "            status = \"âœ…\" if success else \"âŒ\"\n",
    "            \n",
    "            print(f\"{i}. {status} {query}\")\n",
    "            print(f\"   SQL: {sql[:50]}{'...' if len(sql) > 50 else ''}\")\n",
    "            print()\n",
    "    \n",
    "    def _show_schema(self):\n",
    "        \"\"\"é¡¯ç¤ºæ•¸æ“šåº«çµæ§‹\"\"\"\n",
    "        print(\"\\nğŸ—ƒï¸ æ•¸æ“šåº«çµæ§‹:\")\n",
    "        print(self.pipeline.schema.get_schema_info())\n",
    "\n",
    "# å‰µå»ºäº’å‹•ç•Œé¢\n",
    "interface = InteractiveQueryInterface(pipeline)\n",
    "\n",
    "# æ¼”ç¤ºä¸€äº›è‡ªå‹•æŸ¥è©¢ï¼ˆè€Œä¸æ˜¯çœŸæ­£çš„äº’å‹•æ¨¡å¼ï¼‰\n",
    "print(\"\\nğŸ¯ TextSQL RAG ç³»çµ±æ¼”ç¤º\")\n",
    "print(\"ä»¥ä¸‹æ˜¯ä¸€äº›è‡ªå‹•åŸ·è¡Œçš„æŸ¥è©¢ç¤ºä¾‹:\\n\")\n",
    "\n",
    "demo_queries = [\n",
    "    \"æŸ¥çœ‹æ‰€æœ‰ç”¨æˆ¶çš„ä¿¡æ¯\",\n",
    "    \"æ‰¾å‡ºåƒ¹æ ¼æœ€é«˜çš„ä¸‰å€‹ç”¢å“\",\n",
    "    \"çµ±è¨ˆæ¯å€‹ç”¨æˆ¶çš„è¨‚å–®ç¸½æ•¸\"\n",
    "]\n",
    "\n",
    "for query in demo_queries:\n",
    "    print(f\"ğŸ” æŸ¥è©¢: {query}\")\n",
    "    interface._process_interactive_query(query)\n",
    "    print(\"\\n\" + \"-\"*60 + \"\\n\")\n",
    "\n",
    "print(\"\\nâœ¨ æ¼”ç¤ºå®Œæˆï¼è¦é–‹å§‹çœŸæ­£çš„äº’å‹•æ¨¡å¼ï¼Œè«‹å–æ¶ˆè¨»é‡‹ä¸‹é¢çš„ä»£ç¢¼ï¼š\")\n",
    "print(\"# interface.start_session()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç¸½çµèˆ‡å„ªåŒ–å»ºè­°\n",
    "\n",
    "### å­¸ç¿’ç¸½çµ\n",
    "\n",
    "é€šéæœ¬ç­†è¨˜æœ¬ï¼Œæˆ‘å€‘å®Œæ•´å¯¦ç¾äº†ä¸€å€‹ TextSQL RAG æµæ°´ç·šï¼ŒåŒ…æ‹¬ï¼š\n",
    "\n",
    "1. **æ•¸æ“šé è™•ç†**: å‰µå»ºç¤ºä¾‹æ•¸æ“šåº«å’Œè¨“ç·´æ•¸æ“š\n",
    "2. **å‘é‡åŒ–**: ä½¿ç”¨å¥å­åµŒå…¥æ¨¡å‹å°‡æ–‡æœ¬è½‰æ›ç‚ºå‘é‡\n",
    "3. **æª¢ç´¢ç³»çµ±**: åŸºæ–¼èªç¾©ç›¸ä¼¼åº¦æª¢ç´¢ç›¸é—œç¤ºä¾‹\n",
    "4. **SQLç”Ÿæˆ**: çµåˆæª¢ç´¢çµæœå’Œæ¨¡æ¿ç”ŸæˆSQL\n",
    "5. **å®Œæ•´æµæ°´ç·š**: æ•´åˆæ‰€æœ‰çµ„ä»¶\n",
    "6. **è©•ä¼°ç³»çµ±**: å¤šç¶­åº¦è©•ä¼°æ€§èƒ½\n",
    "7. **å¯¦éš›æ‡‰ç”¨**: äº’å‹•å¼æŸ¥è©¢ç•Œé¢\n",
    "\n",
    "### å„ªåŒ–å»ºè­°\n",
    "\n",
    "1. **æ¨¡å‹å„ªåŒ–**:\n",
    "   - ä½¿ç”¨æ›´å¼·å¤§çš„åµŒå…¥æ¨¡å‹ï¼ˆå¦‚ OpenAI embeddingsï¼‰\n",
    "   - å¯¦ç¾å¾®èª¿æ©Ÿåˆ¶ä»¥é©æ‡‰ç‰¹å®šé ˜åŸŸ\n",
    "\n",
    "2. **æª¢ç´¢å„ªåŒ–**:\n",
    "   - å¯¦ç¾æ··åˆæª¢ç´¢ï¼ˆèªç¾©+é—œéµè©ï¼‰\n",
    "   - æ·»åŠ é‡æ’åºæ©Ÿåˆ¶\n",
    "\n",
    "3. **ç”Ÿæˆå„ªåŒ–**:\n",
    "   - é›†æˆå¤§èªè¨€æ¨¡å‹ï¼ˆå¦‚ GPT-4ï¼‰\n",
    "   - å¯¦ç¾æ›´è¤‡é›œçš„SQLæ¨¡æ¿\n",
    "\n",
    "4. **ç³»çµ±å„ªåŒ–**:\n",
    "   - æ·»åŠ ç·©å­˜æ©Ÿåˆ¶\n",
    "   - å¯¦ç¾åˆ†ä½ˆå¼éƒ¨ç½²\n",
    "   - å¢å¼·éŒ¯èª¤è™•ç†\n",
    "\n",
    "### å¾ŒçºŒå­¸ç¿’æ–¹å‘\n",
    "\n",
    "1. æ·±å…¥å­¸ç¿’ Transformer æ¶æ§‹\n",
    "2. æ¢ç´¢æ›´é«˜ç´šçš„ RAG æŠ€è¡“\n",
    "3. å­¸ç¿’æ•¸æ“šåº«å„ªåŒ–æŠ€è¡“\n",
    "4. ç ”ç©¶å¤šæ¨¡æ…‹ RAG ç³»çµ±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¿å­˜æµæ°´ç·šç‹€æ…‹å’Œçµæœ\n",
    "import pickle\n",
    "\n",
    "def save_pipeline_state():\n",
    "    \"\"\"ä¿å­˜æµæ°´ç·šç‹€æ…‹\"\"\"\n",
    "    state = {\n",
    "        'query_history': pipeline.query_history,\n",
    "        'pipeline_stats': pipeline.get_pipeline_stats(),\n",
    "        'training_data': training_data,\n",
    "        'schema_info': schema.get_schema_info()\n",
    "    }\n",
    "    \n",
    "    with open('pipeline_state.pkl', 'wb') as f:\n",
    "        pickle.dump(state, f)\n",
    "    \n",
    "    # ä¹Ÿä¿å­˜ç‚ºJSONæ ¼å¼ä¾¿æ–¼æŸ¥çœ‹\n",
    "    json_state = {\n",
    "        'pipeline_stats': pipeline.get_pipeline_stats(),\n",
    "        'training_data': training_data,\n",
    "        'schema_info': schema.get_schema_info()\n",
    "    }\n",
    "    \n",
    "    with open('pipeline_summary.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(json_state, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(\"âœ… æµæ°´ç·šç‹€æ…‹å·²ä¿å­˜\")\n",
    "\n",
    "# ä¿å­˜ç‹€æ…‹\n",
    "save_pipeline_state()\n",
    "\n",
    "print(\"\\nğŸ‰ TextSQL RAG Pipeline å­¸ç¿’ç­†è¨˜å®Œæˆï¼\")\n",
    "print(\"\\nğŸ“ å­¸ç¿’æˆæœ:\")\n",
    "print(f\"  â€¢ è™•ç†äº† {len(pipeline.query_history)} å€‹æŸ¥è©¢\")\n",
    "print(f\"  â€¢ æˆåŠŸç‡: {pipeline.get_pipeline_stats().get('success_rate', 0):.1%}\")\n",
    "print(f\"  â€¢ å¹³å‡æª¢ç´¢åˆ†æ•¸: {pipeline.get_pipeline_stats().get('avg_retrieval_score', 0):.3f}\")\n",
    "print(\"\\nğŸš€ æ‚¨ç¾åœ¨å¯ä»¥:\")\n",
    "print(\"  1. åœ¨ Kaggle ç’°å¢ƒä¸­é‹è¡Œæ­¤ç­†è¨˜æœ¬\")\n",
    "print(\"  2. ä¿®æ”¹æ•¸æ“šåº«çµæ§‹å’Œè¨“ç·´æ•¸æ“š\")\n",
    "print(\"  3. å„ªåŒ–æª¢ç´¢å’Œç”Ÿæˆç®—æ³•\")\n",
    "print(\"  4. é›†æˆæ›´å¼·å¤§çš„èªè¨€æ¨¡å‹\")\n",
    "print(\"\\nğŸ’¡ å»ºè­°ä¸‹ä¸€æ­¥: å˜—è©¦åœ¨çœŸå¯¦æ•¸æ“šé›†ä¸Šæ¸¬è©¦æ­¤ç³»çµ±ï¼\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}