{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TextSQL RAG Pipeline å­¸ç¿’ç­†è¨˜\n",
    "\n",
    "æœ¬ç­†è¨˜æœ¬å°‡è©³ç´°ä»‹ç´¹å¦‚ä½•æ§‹å»ºä¸€å€‹å®Œæ•´çš„ textSQL RAG (Retrieval-Augmented Generation) æµæ°´ç·šã€‚\n",
    "\n",
    "## ç›®éŒ„\n",
    "1. [RAG æ¦‚å¿µä»‹ç´¹](#1-rag-æ¦‚å¿µä»‹ç´¹)\n",
    "2. [TextSQL åŸºç¤](#2-textsql-åŸºç¤)\n",
    "3. [ç’°å¢ƒè¨­ç½®](#3-ç’°å¢ƒè¨­ç½®)\n",
    "4. [æ•¸æ“šé è™•ç†](#4-æ•¸æ“šé è™•ç†)\n",
    "5. [å‘é‡åŒ–èˆ‡ç´¢å¼•](#5-å‘é‡åŒ–èˆ‡ç´¢å¼•)\n",
    "6. [æª¢ç´¢ç³»çµ±](#6-æª¢ç´¢ç³»çµ±)\n",
    "7. [SQL ç”Ÿæˆ](#7-sql-ç”Ÿæˆ)\n",
    "8. [å®Œæ•´æµæ°´ç·š](#8-å®Œæ•´æµæ°´ç·š)\n",
    "9. [è©•ä¼°èˆ‡å„ªåŒ–](#9-è©•ä¼°èˆ‡å„ªåŒ–)\n",
    "10. [å¯¦éš›æ‡‰ç”¨æ¡ˆä¾‹](#10-å¯¦éš›æ‡‰ç”¨æ¡ˆä¾‹)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. RAG æ¦‚å¿µä»‹ç´¹\n",
    "\n",
    "### ä»€éº¼æ˜¯ RAGï¼Ÿ\n",
    "RAGï¼ˆRetrieval-Augmented Generationï¼‰æ˜¯ä¸€ç¨®çµåˆäº†æª¢ç´¢å’Œç”Ÿæˆçš„æ¶æ§‹ï¼š\n",
    "1. **æª¢ç´¢éšæ®µ**ï¼šå¾çŸ¥è­˜åº«ä¸­æ‰¾åˆ°ç›¸é—œä¿¡æ¯\n",
    "2. **ç”Ÿæˆéšæ®µ**ï¼šåŸºæ–¼æª¢ç´¢åˆ°çš„ä¿¡æ¯ç”Ÿæˆå›ç­”\n",
    "\n",
    "### TextSQL RAG çš„ç‰¹é»\n",
    "- å°ˆæ³¨æ–¼è‡ªç„¶èªè¨€åˆ° SQL æŸ¥è©¢çš„è½‰æ›\n",
    "- çµåˆæ•¸æ“šåº«æ¨¡å¼ä¿¡æ¯\n",
    "- æ”¯æŒè¤‡é›œçš„æ•¸æ“šåº«æŸ¥è©¢ç”Ÿæˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŸºæœ¬æ¦‚å¿µç¤ºä¾‹\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# RAG æµç¨‹ç¤ºæ„\n",
    "class SimpleRAGConcept:\n",
    "    def __init__(self):\n",
    "        self.knowledge_base = []\n",
    "        self.query_history = []\n",
    "    \n",
    "    def retrieve(self, query: str) -> List[str]:\n",
    "        \"\"\"æª¢ç´¢ç›¸é—œä¿¡æ¯\"\"\"\n",
    "        # é€™è£¡æ˜¯ç°¡åŒ–çš„æª¢ç´¢é‚è¼¯\n",
    "        relevant_docs = [doc for doc in self.knowledge_base \n",
    "                        if any(word.lower() in doc.lower() for word in query.split())]\n",
    "        return relevant_docs[:3]  # è¿”å›å‰3å€‹ç›¸é—œæ–‡æª”\n",
    "    \n",
    "    def generate(self, query: str, context: List[str]) -> str:\n",
    "        \"\"\"åŸºæ–¼ä¸Šä¸‹æ–‡ç”Ÿæˆå›ç­”\"\"\"\n",
    "        # é€™è£¡æ˜¯ç°¡åŒ–çš„ç”Ÿæˆé‚è¼¯\n",
    "        return f\"åŸºæ–¼ä¸Šä¸‹æ–‡ {context} å°æŸ¥è©¢ '{query}' çš„å›ç­”\"\n",
    "    \n",
    "    def rag_pipeline(self, query: str) -> str:\n",
    "        \"\"\"å®Œæ•´çš„RAGæµæ°´ç·š\"\"\"\n",
    "        # 1. æª¢ç´¢\n",
    "        relevant_context = self.retrieve(query)\n",
    "        # 2. ç”Ÿæˆ\n",
    "        response = self.generate(query, relevant_context)\n",
    "        # 3. è¨˜éŒ„æŸ¥è©¢æ­·å²\n",
    "        self.query_history.append({'query': query, 'response': response})\n",
    "        return response\n",
    "\n",
    "# ç¤ºä¾‹ä½¿ç”¨\n",
    "rag_demo = SimpleRAGConcept()\n",
    "rag_demo.knowledge_base = [\n",
    "    \"ç”¨æˆ¶è¡¨åŒ…å«ç”¨æˆ¶IDã€å§“åã€éƒµç®±ç­‰å­—æ®µ\",\n",
    "    \"è¨‚å–®è¡¨è¨˜éŒ„äº†æ‰€æœ‰çš„è³¼è²·ä¿¡æ¯\",\n",
    "    \"ç”¢å“è¡¨å­˜å„²ç”¢å“çš„è©³ç´°ä¿¡æ¯\"\n",
    "]\n",
    "\n",
    "result = rag_demo.rag_pipeline(\"å¦‚ä½•æŸ¥è©¢ç”¨æˆ¶çš„è¨‚å–®ä¿¡æ¯ï¼Ÿ\")\n",
    "print(\"RAG ç¤ºä¾‹çµæœ:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. TextSQL åŸºç¤\n",
    "\n",
    "### æ ¸å¿ƒæ¦‚å¿µ\n",
    "TextSQL æ˜¯å°‡è‡ªç„¶èªè¨€æŸ¥è©¢è½‰æ›ç‚º SQL èªå¥çš„éç¨‹ã€‚é—œéµçµ„ä»¶åŒ…æ‹¬ï¼š\n",
    "- **Schema Understanding**: ç†è§£æ•¸æ“šåº«çµæ§‹\n",
    "- **Intent Recognition**: è­˜åˆ¥ç”¨æˆ¶æ„åœ–\n",
    "- **SQL Generation**: ç”Ÿæˆå°æ‡‰çš„SQLèªå¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TextSQL åŸºç¤çµ„ä»¶\n",
    "class DatabaseSchema:\n",
    "    \"\"\"æ•¸æ“šåº«æ¨¡å¼é¡\"\"\"\n",
    "    def __init__(self):\n",
    "        self.tables = {}\n",
    "        self.relationships = []\n",
    "    \n",
    "    def add_table(self, table_name: str, columns: Dict[str, str]):\n",
    "        \"\"\"æ·»åŠ è¡¨çµæ§‹\"\"\"\n",
    "        self.tables[table_name] = columns\n",
    "    \n",
    "    def add_relationship(self, table1: str, column1: str, table2: str, column2: str):\n",
    "        \"\"\"æ·»åŠ è¡¨é—œä¿‚\"\"\"\n",
    "        self.relationships.append({\n",
    "            'from_table': table1,\n",
    "            'from_column': column1,\n",
    "            'to_table': table2,\n",
    "            'to_column': column2\n",
    "        })\n",
    "    \n",
    "    def get_schema_info(self) -> str:\n",
    "        \"\"\"ç²å–æ¨¡å¼ä¿¡æ¯\"\"\"\n",
    "        schema_info = \"æ•¸æ“šåº«æ¨¡å¼ä¿¡æ¯:\\n\"\n",
    "        for table, columns in self.tables.items():\n",
    "            schema_info += f\"è¡¨ {table}: {columns}\\n\"\n",
    "        return schema_info\n",
    "\n",
    "# å‰µå»ºç¤ºä¾‹æ•¸æ“šåº«æ¨¡å¼\n",
    "schema = DatabaseSchema()\n",
    "schema.add_table('users', {\n",
    "    'user_id': 'INT PRIMARY KEY',\n",
    "    'name': 'VARCHAR(100)',\n",
    "    'email': 'VARCHAR(100)',\n",
    "    'created_at': 'DATETIME'\n",
    "})\n",
    "\n",
    "schema.add_table('orders', {\n",
    "    'order_id': 'INT PRIMARY KEY',\n",
    "    'user_id': 'INT',\n",
    "    'product_id': 'INT',\n",
    "    'quantity': 'INT',\n",
    "    'order_date': 'DATETIME'\n",
    "})\n",
    "\n",
    "schema.add_table('products', {\n",
    "    'product_id': 'INT PRIMARY KEY',\n",
    "    'name': 'VARCHAR(100)',\n",
    "    'price': 'DECIMAL(10,2)',\n",
    "    'category': 'VARCHAR(50)'\n",
    "})\n",
    "\n",
    "schema.add_relationship('orders', 'user_id', 'users', 'user_id')\n",
    "schema.add_relationship('orders', 'product_id', 'products', 'product_id')\n",
    "\n",
    "print(schema.get_schema_info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ç’°å¢ƒè¨­ç½®\n",
    "\n",
    "### å®‰è£å¿…è¦çš„åº«"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åœ¨ Kaggle ç’°å¢ƒä¸­å®‰è£å¿…è¦çš„åŒ…\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "def install_package(package):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "# æ ¸å¿ƒåŒ…åˆ—è¡¨\n",
    "packages = [\n",
    "    'transformers',\n",
    "    'torch',\n",
    "    'sentence-transformers',\n",
    "    'faiss-cpu',\n",
    "    'chromadb',\n",
    "    'langchain',\n",
    "    'openai',\n",
    "    'sqlparse',\n",
    "    'sqlite3'\n",
    "]\n",
    "\n",
    "print(\"æ­£åœ¨å®‰è£å¿…è¦çš„åŒ…...\")\n",
    "for package in packages:\n",
    "    try:\n",
    "        install_package(package)\n",
    "        print(f\"âœ“ {package} å®‰è£æˆåŠŸ\")\n",
    "    except Exception as e:\n",
    "        print(f\"âœ— {package} å®‰è£å¤±æ•—: {e}\")\n",
    "\n",
    "print(\"\\nç’°å¢ƒè¨­ç½®å®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å°å…¥å¿…è¦çš„åº«\n",
    "import os\n",
    "import json\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# å‘é‡åŒ–å’Œæª¢ç´¢\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "\n",
    "# è‡ªç„¶èªè¨€è™•ç†\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "# SQL è§£æ\n",
    "import sqlparse\n",
    "\n",
    "print(\"æ‰€æœ‰å¿…è¦çš„åº«å·²æˆåŠŸå°å…¥ï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. æ•¸æ“šé è™•ç†\n",
    "\n",
    "### å‰µå»ºç¤ºä¾‹æ•¸æ“šåº«å’Œæ•¸æ“š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å‰µå»ºç¤ºä¾‹ SQLite æ•¸æ“šåº«\n",
    "def create_sample_database():\n",
    "    \"\"\"å‰µå»ºç¤ºä¾‹æ•¸æ“šåº«\"\"\"\n",
    "    conn = sqlite3.connect('sample_ecommerce.db')\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # å‰µå»ºç”¨æˆ¶è¡¨\n",
    "    cursor.execute('''\n",
    "    CREATE TABLE IF NOT EXISTS users (\n",
    "        user_id INTEGER PRIMARY KEY,\n",
    "        name TEXT NOT NULL,\n",
    "        email TEXT UNIQUE NOT NULL,\n",
    "        created_at DATETIME DEFAULT CURRENT_TIMESTAMP\n",
    "    )\n",
    "    ''')\n",
    "    \n",
    "    # å‰µå»ºç”¢å“è¡¨\n",
    "    cursor.execute('''\n",
    "    CREATE TABLE IF NOT EXISTS products (\n",
    "        product_id INTEGER PRIMARY KEY,\n",
    "        name TEXT NOT NULL,\n",
    "        price DECIMAL(10,2) NOT NULL,\n",
    "        category TEXT NOT NULL,\n",
    "        description TEXT\n",
    "    )\n",
    "    ''')\n",
    "    \n",
    "    # å‰µå»ºè¨‚å–®è¡¨\n",
    "    cursor.execute('''\n",
    "    CREATE TABLE IF NOT EXISTS orders (\n",
    "        order_id INTEGER PRIMARY KEY,\n",
    "        user_id INTEGER,\n",
    "        product_id INTEGER,\n",
    "        quantity INTEGER NOT NULL,\n",
    "        order_date DATETIME DEFAULT CURRENT_TIMESTAMP,\n",
    "        FOREIGN KEY (user_id) REFERENCES users (user_id),\n",
    "        FOREIGN KEY (product_id) REFERENCES products (product_id)\n",
    "    )\n",
    "    ''')\n",
    "    \n",
    "    # æ’å…¥ç¤ºä¾‹æ•¸æ“š\n",
    "    users_data = [\n",
    "        (1, 'å¼µä¸‰', 'zhang@example.com'),\n",
    "        (2, 'æå››', 'li@example.com'),\n",
    "        (3, 'ç‹äº”', 'wang@example.com')\n",
    "    ]\n",
    "    \n",
    "    products_data = [\n",
    "        (1, 'iPhone 15', 999.99, 'é›»å­ç”¢å“', 'æœ€æ–°æ¬¾æ™ºèƒ½æ‰‹æ©Ÿ'),\n",
    "        (2, 'MacBook Pro', 1299.99, 'é›»å­ç”¢å“', 'å°ˆæ¥­ç­†è¨˜æœ¬é›»è…¦'),\n",
    "        (3, 'å’–å•¡æ©Ÿ', 199.99, 'å®¶é›»', 'å…¨è‡ªå‹•å’–å•¡æ©Ÿ'),\n",
    "        (4, 'æ›¸ç±ï¼šPythonç·¨ç¨‹', 29.99, 'åœ–æ›¸', 'Pythonç·¨ç¨‹å…¥é–€æ•™ç¨‹')\n",
    "    ]\n",
    "    \n",
    "    orders_data = [\n",
    "        (1, 1, 1, 1),\n",
    "        (2, 1, 3, 1),\n",
    "        (3, 2, 2, 1),\n",
    "        (4, 3, 4, 2)\n",
    "    ]\n",
    "    \n",
    "    cursor.executemany('INSERT OR REPLACE INTO users (user_id, name, email) VALUES (?, ?, ?)', users_data)\n",
    "    cursor.executemany('INSERT OR REPLACE INTO products (product_id, name, price, category, description) VALUES (?, ?, ?, ?, ?)', products_data)\n",
    "    cursor.executemany('INSERT OR REPLACE INTO orders (order_id, user_id, product_id, quantity) VALUES (?, ?, ?, ?)', orders_data)\n",
    "    \n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    print(\"ç¤ºä¾‹æ•¸æ“šåº«å‰µå»ºå®Œæˆï¼\")\n",
    "\n",
    "# å‰µå»ºæ•¸æ“šåº«\n",
    "create_sample_database()\n",
    "\n",
    "# é©—è­‰æ•¸æ“š\n",
    "def verify_database():\n",
    "    conn = sqlite3.connect('sample_ecommerce.db')\n",
    "    \n",
    "    print(\"ç”¨æˆ¶è¡¨:\")\n",
    "    users_df = pd.read_sql_query('SELECT * FROM users', conn)\n",
    "    print(users_df)\n",
    "    \n",
    "    print(\"\\nç”¢å“è¡¨:\")\n",
    "    products_df = pd.read_sql_query('SELECT * FROM products', conn)\n",
    "    print(products_df)\n",
    "    \n",
    "    print(\"\\nè¨‚å–®è¡¨:\")\n",
    "    orders_df = pd.read_sql_query('SELECT * FROM orders', conn)\n",
    "    print(orders_df)\n",
    "    \n",
    "    conn.close()\n",
    "\n",
    "verify_database()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æº–å‚™è¨“ç·´æ•¸æ“šé›†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å‰µå»ºè‡ªç„¶èªè¨€åˆ°SQLçš„è¨“ç·´æ•¸æ“šé›†\n",
    "training_data = [\n",
    "    {\n",
    "        \"natural_language\": \"é¡¯ç¤ºæ‰€æœ‰ç”¨æˆ¶çš„ä¿¡æ¯\",\n",
    "        \"sql\": \"SELECT * FROM users;\",\n",
    "        \"explanation\": \"æŸ¥è©¢ç”¨æˆ¶è¡¨ä¸­çš„æ‰€æœ‰è¨˜éŒ„\"\n",
    "    },\n",
    "    {\n",
    "        \"natural_language\": \"æ‰¾å‡ºåƒ¹æ ¼è¶…é500å…ƒçš„ç”¢å“\",\n",
    "        \"sql\": \"SELECT * FROM products WHERE price > 500;\",\n",
    "        \"explanation\": \"ä½¿ç”¨WHEREå­å¥éæ¿¾åƒ¹æ ¼å¤§æ–¼500çš„ç”¢å“\"\n",
    "    },\n",
    "    {\n",
    "        \"natural_language\": \"çµ±è¨ˆæ¯å€‹ç”¨æˆ¶çš„è¨‚å–®æ•¸é‡\",\n",
    "        \"sql\": \"SELECT u.name, COUNT(o.order_id) as order_count FROM users u LEFT JOIN orders o ON u.user_id = o.user_id GROUP BY u.user_id, u.name;\",\n",
    "        \"explanation\": \"ä½¿ç”¨JOINå’ŒGROUP BYçµ±è¨ˆæ¯å€‹ç”¨æˆ¶çš„è¨‚å–®æ•¸é‡\"\n",
    "    },\n",
    "    {\n",
    "        \"natural_language\": \"æŸ¥æ‰¾æœ€è²´çš„ç”¢å“\",\n",
    "        \"sql\": \"SELECT * FROM products ORDER BY price DESC LIMIT 1;\",\n",
    "        \"explanation\": \"ä½¿ç”¨ORDER BYå’ŒLIMITæ‰¾åˆ°åƒ¹æ ¼æœ€é«˜çš„ç”¢å“\"\n",
    "    },\n",
    "    {\n",
    "        \"natural_language\": \"é¡¯ç¤ºç”¨æˆ¶å¼µä¸‰çš„æ‰€æœ‰è¨‚å–®\",\n",
    "        \"sql\": \"SELECT o.*, p.name as product_name FROM orders o JOIN users u ON o.user_id = u.user_id JOIN products p ON o.product_id = p.product_id WHERE u.name = 'å¼µä¸‰';\",\n",
    "        \"explanation\": \"ä½¿ç”¨å¤šè¡¨JOINæŸ¥è©¢ç‰¹å®šç”¨æˆ¶çš„è¨‚å–®ä¿¡æ¯\"\n",
    "    },\n",
    "    {\n",
    "        \"natural_language\": \"è¨ˆç®—æ¯å€‹é¡åˆ¥çš„ç”¢å“å¹³å‡åƒ¹æ ¼\",\n",
    "        \"sql\": \"SELECT category, AVG(price) as avg_price FROM products GROUP BY category;\",\n",
    "        \"explanation\": \"ä½¿ç”¨GROUP BYå’ŒAVGå‡½æ•¸è¨ˆç®—å„é¡åˆ¥çš„å¹³å‡åƒ¹æ ¼\"\n",
    "    },\n",
    "    {\n",
    "        \"natural_language\": \"æ‰¾å‡ºæ²’æœ‰ä¸‹éè¨‚å–®çš„ç”¨æˆ¶\",\n",
    "        \"sql\": \"SELECT u.* FROM users u LEFT JOIN orders o ON u.user_id = o.user_id WHERE o.user_id IS NULL;\",\n",
    "        \"explanation\": \"ä½¿ç”¨LEFT JOINå’ŒIS NULLæ‰¾å‡ºæ²’æœ‰è¨‚å–®çš„ç”¨æˆ¶\"\n",
    "    },\n",
    "    {\n",
    "        \"natural_language\": \"é¡¯ç¤ºéŠ·é‡æœ€é«˜çš„ç”¢å“\",\n",
    "        \"sql\": \"SELECT p.name, SUM(o.quantity) as total_sold FROM products p JOIN orders o ON p.product_id = o.product_id GROUP BY p.product_id, p.name ORDER BY total_sold DESC LIMIT 1;\",\n",
    "        \"explanation\": \"çµ±è¨ˆç”¢å“éŠ·é‡ä¸¦æ‰¾å‡ºéŠ·é‡æœ€é«˜çš„ç”¢å“\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# ä¿å­˜è¨“ç·´æ•¸æ“š\n",
    "with open('training_data.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(training_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"è¨“ç·´æ•¸æ“šé›†åŒ…å« {len(training_data)} å€‹æ¨£æœ¬\")\n",
    "print(\"\\nç¤ºä¾‹æ•¸æ“š:\")\n",
    "for i, item in enumerate(training_data[:3]):\n",
    "    print(f\"\\næ¨£æœ¬ {i+1}:\")\n",
    "    print(f\"è‡ªç„¶èªè¨€: {item['natural_language']}\")\n",
    "    print(f\"SQL: {item['sql']}\")\n",
    "    print(f\"èªªæ˜: {item['explanation']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. å‘é‡åŒ–èˆ‡ç´¢å¼•\n",
    "\n",
    "### ä½¿ç”¨å¥å­åµŒå…¥æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆå§‹åŒ–å¥å­åµŒå…¥æ¨¡å‹\n",
    "class TextEmbedder:\n",
    "    def __init__(self, model_name='all-MiniLM-L6-v2'):\n",
    "        \"\"\"åˆå§‹åŒ–æ–‡æœ¬åµŒå…¥å™¨\"\"\"\n",
    "        try:\n",
    "            self.model = SentenceTransformer(model_name)\n",
    "            print(f\"æˆåŠŸåŠ è¼‰æ¨¡å‹: {model_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"æ¨¡å‹åŠ è¼‰å¤±æ•—: {e}, ä½¿ç”¨å‚™ç”¨æ–¹æ¡ˆ\")\n",
    "            # å‚™ç”¨æ–¹æ¡ˆï¼šä½¿ç”¨ç°¡å–®çš„è©å‘é‡\n",
    "            self.model = None\n",
    "    \n",
    "    def encode(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"å°‡æ–‡æœ¬ç·¨ç¢¼ç‚ºå‘é‡\"\"\"\n",
    "        if self.model:\n",
    "            return self.model.encode(texts)\n",
    "        else:\n",
    "            # ç°¡å–®çš„å‚™ç”¨ç·¨ç¢¼æ–¹æ¡ˆ\n",
    "            return self._simple_encode(texts)\n",
    "    \n",
    "    def _simple_encode(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"ç°¡å–®çš„æ–‡æœ¬ç·¨ç¢¼å‚™ç”¨æ–¹æ¡ˆ\"\"\"\n",
    "        # é€™æ˜¯ä¸€å€‹ç°¡åŒ–çš„å¯¦ç¾ï¼Œå¯¦éš›æ‡‰ç”¨ä¸­æ‡‰ä½¿ç”¨æ›´è¤‡é›œçš„æ–¹æ³•\n",
    "        vocab = {}\n",
    "        for text in texts:\n",
    "            for word in text.lower().split():\n",
    "                if word not in vocab:\n",
    "                    vocab[word] = len(vocab)\n",
    "        \n",
    "        vectors = []\n",
    "        for text in texts:\n",
    "            vector = np.zeros(len(vocab))\n",
    "            for word in text.lower().split():\n",
    "                if word in vocab:\n",
    "                    vector[vocab[word]] = 1\n",
    "            vectors.append(vector)\n",
    "        \n",
    "        return np.array(vectors)\n",
    "\n",
    "# åˆå§‹åŒ–åµŒå…¥å™¨\n",
    "embedder = TextEmbedder()\n",
    "\n",
    "# æ¸¬è©¦åµŒå…¥åŠŸèƒ½\n",
    "test_texts = [\n",
    "    \"æŸ¥è©¢æ‰€æœ‰ç”¨æˆ¶\",\n",
    "    \"é¡¯ç¤ºç”¢å“ä¿¡æ¯\",\n",
    "    \"çµ±è¨ˆè¨‚å–®æ•¸é‡\"\n",
    "]\n",
    "\n",
    "embeddings = embedder.encode(test_texts)\n",
    "print(f\"æ–‡æœ¬åµŒå…¥ç¶­åº¦: {embeddings.shape}\")\n",
    "print(f\"åµŒå…¥ç¤ºä¾‹: {embeddings[0][:5]}...\")  # é¡¯ç¤ºå‰5å€‹ç¶­åº¦"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æ§‹å»ºå‘é‡ç´¢å¼•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ§‹å»ºFAISSå‘é‡ç´¢å¼•\n",
    "class VectorIndex:\n",
    "    def __init__(self, dimension: int):\n",
    "        \"\"\"åˆå§‹åŒ–å‘é‡ç´¢å¼•\"\"\"\n",
    "        self.dimension = dimension\n",
    "        self.index = faiss.IndexFlatL2(dimension)  # L2è·é›¢ç´¢å¼•\n",
    "        self.texts = []  # å­˜å„²åŸå§‹æ–‡æœ¬\n",
    "        self.metadata = []  # å­˜å„²å…ƒæ•¸æ“š\n",
    "    \n",
    "    def add_vectors(self, vectors: np.ndarray, texts: List[str], metadata: List[Dict]):\n",
    "        \"\"\"æ·»åŠ å‘é‡åˆ°ç´¢å¼•\"\"\"\n",
    "        self.index.add(vectors.astype('float32'))\n",
    "        self.texts.extend(texts)\n",
    "        self.metadata.extend(metadata)\n",
    "        print(f\"å·²æ·»åŠ  {len(vectors)} å€‹å‘é‡åˆ°ç´¢å¼•\")\n",
    "    \n",
    "    def search(self, query_vector: np.ndarray, k: int = 5) -> List[Dict]:\n",
    "        \"\"\"æœç´¢æœ€ç›¸ä¼¼çš„å‘é‡\"\"\"\n",
    "        query_vector = query_vector.reshape(1, -1).astype('float32')\n",
    "        distances, indices = self.index.search(query_vector, k)\n",
    "        \n",
    "        results = []\n",
    "        for i, (distance, idx) in enumerate(zip(distances[0], indices[0])):\n",
    "            if idx < len(self.texts):  # ç¢ºä¿ç´¢å¼•æœ‰æ•ˆ\n",
    "                results.append({\n",
    "                    'text': self.texts[idx],\n",
    "                    'metadata': self.metadata[idx],\n",
    "                    'distance': float(distance),\n",
    "                    'rank': i + 1\n",
    "                })\n",
    "        return results\n",
    "    \n",
    "    def get_stats(self) -> Dict:\n",
    "        \"\"\"ç²å–ç´¢å¼•çµ±è¨ˆä¿¡æ¯\"\"\"\n",
    "        return {\n",
    "            'total_vectors': self.index.ntotal,\n",
    "            'dimension': self.dimension,\n",
    "            'index_type': 'FlatL2'\n",
    "        }\n",
    "\n",
    "# ç‚ºè¨“ç·´æ•¸æ“šå‰µå»ºå‘é‡ç´¢å¼•\n",
    "natural_language_queries = [item['natural_language'] for item in training_data]\n",
    "query_embeddings = embedder.encode(natural_language_queries)\n",
    "\n",
    "# åˆå§‹åŒ–å‘é‡ç´¢å¼•\n",
    "vector_index = VectorIndex(query_embeddings.shape[1])\n",
    "\n",
    "# æ·»åŠ å‘é‡åˆ°ç´¢å¼•\n",
    "vector_index.add_vectors(\n",
    "    query_embeddings,\n",
    "    natural_language_queries,\n",
    "    training_data\n",
    ")\n",
    "\n",
    "print(\"å‘é‡ç´¢å¼•çµ±è¨ˆ:\")\n",
    "print(vector_index.get_stats())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. æª¢ç´¢ç³»çµ±\n",
    "\n",
    "### å¯¦ç¾èªç¾©æª¢ç´¢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯¦ç¾æª¢ç´¢ç³»çµ±\n",
    "class SemanticRetriever:\n",
    "    def __init__(self, embedder: TextEmbedder, vector_index: VectorIndex, schema: DatabaseSchema):\n",
    "        \"\"\"åˆå§‹åŒ–èªç¾©æª¢ç´¢å™¨\"\"\"\n",
    "        self.embedder = embedder\n",
    "        self.vector_index = vector_index\n",
    "        self.schema = schema\n",
    "    \n",
    "    def retrieve(self, query: str, k: int = 3) -> List[Dict]:\n",
    "        \"\"\"æª¢ç´¢ç›¸é—œçš„SQLç¤ºä¾‹\"\"\"\n",
    "        # 1. å°‡æŸ¥è©¢ç·¨ç¢¼ç‚ºå‘é‡\n",
    "        query_vector = self.embedder.encode([query])[0]\n",
    "        \n",
    "        # 2. åœ¨å‘é‡ç´¢å¼•ä¸­æœç´¢\n",
    "        similar_examples = self.vector_index.search(query_vector, k)\n",
    "        \n",
    "        # 3. æ·»åŠ æ•¸æ“šåº«æ¨¡å¼ä¿¡æ¯\n",
    "        schema_info = self.schema.get_schema_info()\n",
    "        \n",
    "        # 4. æ§‹å»ºæª¢ç´¢çµæœ\n",
    "        retrieval_results = {\n",
    "            'query': query,\n",
    "            'schema_info': schema_info,\n",
    "            'similar_examples': similar_examples,\n",
    "            'retrieval_score': self._calculate_retrieval_score(similar_examples)\n",
    "        }\n",
    "        \n",
    "        return retrieval_results\n",
    "    \n",
    "    def _calculate_retrieval_score(self, examples: List[Dict]) -> float:\n",
    "        \"\"\"è¨ˆç®—æª¢ç´¢è³ªé‡åˆ†æ•¸\"\"\"\n",
    "        if not examples:\n",
    "            return 0.0\n",
    "        \n",
    "        # åŸºæ–¼è·é›¢è¨ˆç®—åˆ†æ•¸ï¼ˆè·é›¢è¶Šå°ï¼Œåˆ†æ•¸è¶Šé«˜ï¼‰\n",
    "        distances = [ex['distance'] for ex in examples]\n",
    "        avg_distance = sum(distances) / len(distances)\n",
    "        score = max(0, 1 - avg_distance / 10)  # ç°¡å–®çš„è©•åˆ†å…¬å¼\n",
    "        return score\n",
    "    \n",
    "    def explain_retrieval(self, results: Dict) -> str:\n",
    "        \"\"\"è§£é‡‹æª¢ç´¢éç¨‹\"\"\"\n",
    "        explanation = f\"\\næª¢ç´¢çµæœè§£é‡‹ï¼š\\n\"\n",
    "        explanation += f\"æŸ¥è©¢: {results['query']}\\n\"\n",
    "        explanation += f\"æª¢ç´¢åˆ†æ•¸: {results['retrieval_score']:.3f}\\n\"\n",
    "        explanation += f\"æ‰¾åˆ° {len(results['similar_examples'])} å€‹ç›¸ä¼¼ç¤ºä¾‹:\\n\"\n",
    "        \n",
    "        for i, example in enumerate(results['similar_examples']):\n",
    "            explanation += f\"\\n{i+1}. ç›¸ä¼¼åº¦: {1-example['distance']:.3f}\\n\"\n",
    "            explanation += f\"   æŸ¥è©¢: {example['text']}\\n\"\n",
    "            explanation += f\"   SQL: {example['metadata']['sql']}\\n\"\n",
    "        \n",
    "        return explanation\n",
    "\n",
    "# åˆå§‹åŒ–æª¢ç´¢å™¨\n",
    "retriever = SemanticRetriever(embedder, vector_index, schema)\n",
    "\n",
    "# æ¸¬è©¦æª¢ç´¢åŠŸèƒ½\n",
    "test_query = \"æˆ‘è¦æŸ¥çœ‹æ‰€æœ‰ç”¢å“çš„è©³ç´°ä¿¡æ¯\"\n",
    "retrieval_results = retriever.retrieve(test_query)\n",
    "\n",
    "print(\"=== æª¢ç´¢æ¸¬è©¦ ===\")\n",
    "print(retriever.explain_retrieval(retrieval_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. SQL ç”Ÿæˆ\n",
    "\n",
    "### åŸºæ–¼æª¢ç´¢çµæœç”ŸæˆSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQLç”Ÿæˆå™¨\n",
    "class SQLGenerator:\n",
    "    def __init__(self, schema: DatabaseSchema):\n",
    "        \"\"\"åˆå§‹åŒ–SQLç”Ÿæˆå™¨\"\"\"\n",
    "        self.schema = schema\n",
    "        self.templates = self._load_sql_templates()\n",
    "    \n",
    "    def _load_sql_templates(self) -> Dict[str, str]:\n",
    "        \"\"\"åŠ è¼‰SQLæ¨¡æ¿\"\"\"\n",
    "        return {\n",
    "            'select_all': \"SELECT * FROM {table};\",\n",
    "            'select_where': \"SELECT * FROM {table} WHERE {condition};\",\n",
    "            'count': \"SELECT COUNT(*) FROM {table};\",\n",
    "            'join': \"SELECT {columns} FROM {table1} t1 JOIN {table2} t2 ON {join_condition};\",\n",
    "            'group_by': \"SELECT {columns}, {aggregate} FROM {table} GROUP BY {group_columns};\",\n",
    "            'order_by': \"SELECT * FROM {table} ORDER BY {column} {direction};\"\n",
    "        }\n",
    "    \n",
    "    def generate_sql(self, query: str, retrieval_results: Dict) -> Dict:\n",
    "        \"\"\"åŸºæ–¼æª¢ç´¢çµæœç”ŸæˆSQL\"\"\"\n",
    "        # 1. åˆ†ææŸ¥è©¢æ„åœ–\n",
    "        intent = self._analyze_intent(query)\n",
    "        \n",
    "        # 2. å¾æª¢ç´¢çµæœä¸­æå–æœ€ä½³åŒ¹é…\n",
    "        best_match = retrieval_results['similar_examples'][0] if retrieval_results['similar_examples'] else None\n",
    "        \n",
    "        # 3. ç”ŸæˆSQL\n",
    "        if best_match and best_match['distance'] < 0.5:  # é«˜ç›¸ä¼¼åº¦\n",
    "            # ä½¿ç”¨æª¢ç´¢åˆ°çš„SQLä½œç‚ºåŸºç¤\n",
    "            generated_sql = self._adapt_sql(best_match['metadata']['sql'], query)\n",
    "            method = 'retrieval_based'\n",
    "        else:\n",
    "            # ä½¿ç”¨æ¨¡æ¿ç”Ÿæˆ\n",
    "            generated_sql = self._template_based_generation(query, intent)\n",
    "            method = 'template_based'\n",
    "        \n",
    "        # 4. é©—è­‰SQL\n",
    "        is_valid, validation_error = self._validate_sql(generated_sql)\n",
    "        \n",
    "        return {\n",
    "            'query': query,\n",
    "            'generated_sql': generated_sql,\n",
    "            'method': method,\n",
    "            'intent': intent,\n",
    "            'is_valid': is_valid,\n",
    "            'validation_error': validation_error,\n",
    "            'best_match': best_match\n",
    "        }\n",
    "    \n",
    "    def _analyze_intent(self, query: str) -> Dict:\n",
    "        \"\"\"åˆ†ææŸ¥è©¢æ„åœ–\"\"\"\n",
    "        query_lower = query.lower()\n",
    "        \n",
    "        intent = {\n",
    "            'action': 'select',  # é»˜èªç‚ºæŸ¥è©¢\n",
    "            'tables': [],\n",
    "            'conditions': [],\n",
    "            'aggregation': None,\n",
    "            'sorting': None\n",
    "        }\n",
    "        \n",
    "        # è­˜åˆ¥è¡¨å\n",
    "        for table in self.schema.tables.keys():\n",
    "            if table in query_lower or self._table_synonyms(table, query_lower):\n",
    "                intent['tables'].append(table)\n",
    "        \n",
    "        # è­˜åˆ¥èšåˆæ“ä½œ\n",
    "        if any(word in query_lower for word in ['çµ±è¨ˆ', 'è¨ˆç®—', 'ç¸½å’Œ', 'å¹³å‡', 'æœ€å¤§', 'æœ€å°']):\n",
    "            intent['aggregation'] = 'count'  # ç°¡åŒ–è™•ç†\n",
    "        \n",
    "        # è­˜åˆ¥æ’åº\n",
    "        if any(word in query_lower for word in ['æœ€', 'æ’åº', 'æœ€é«˜', 'æœ€ä½']):\n",
    "            intent['sorting'] = 'desc' if 'æœ€é«˜' in query_lower else 'asc'\n",
    "        \n",
    "        return intent\n",
    "    \n",
    "    def _table_synonyms(self, table: str, query: str) -> bool:\n",
    "        \"\"\"æª¢æŸ¥è¡¨ååŒç¾©è©\"\"\"\n",
    "        synonyms = {\n",
    "            'users': ['ç”¨æˆ¶', 'ä½¿ç”¨è€…', 'æœƒå“¡'],\n",
    "            'products': ['ç”¢å“', 'å•†å“', 'ç‰©å“'],\n",
    "            'orders': ['è¨‚å–®', 'è¨‚è³¼', 'è³¼è²·']\n",
    "        }\n",
    "        \n",
    "        if table in synonyms:\n",
    "            return any(syn in query for syn in synonyms[table])\n",
    "        return False\n",
    "    \n",
    "    def _adapt_sql(self, base_sql: str, query: str) -> str:\n",
    "        \"\"\"èª¿æ•´åŸºç¤SQLä»¥åŒ¹é…æ–°æŸ¥è©¢\"\"\"\n",
    "        # é€™è£¡å¯ä»¥å¯¦ç¾æ›´è¤‡é›œçš„SQLèª¿æ•´é‚è¼¯\n",
    "        # ç›®å‰åªæ˜¯ç°¡å–®è¿”å›åŸºç¤SQL\n",
    "        return base_sql\n",
    "    \n",
    "    def _template_based_generation(self, query: str, intent: Dict) -> str:\n",
    "        \"\"\"åŸºæ–¼æ¨¡æ¿ç”ŸæˆSQL\"\"\"\n",
    "        if not intent['tables']:\n",
    "            return \"SELECT 'No table identified' as error;\"\n",
    "        \n",
    "        table = intent['tables'][0]  # ä½¿ç”¨ç¬¬ä¸€å€‹è­˜åˆ¥çš„è¡¨\n",
    "        \n",
    "        if intent['aggregation']:\n",
    "            return f\"SELECT COUNT(*) FROM {table};\"\n",
    "        elif intent['sorting']:\n",
    "            # å‡è¨­æŒ‰ç¬¬ä¸€å€‹æ•¸å€¼åˆ—æ’åº\n",
    "            numeric_columns = self._get_numeric_columns(table)\n",
    "            if numeric_columns:\n",
    "                column = numeric_columns[0]\n",
    "                direction = intent['sorting'].upper()\n",
    "                return f\"SELECT * FROM {table} ORDER BY {column} {direction};\"\n",
    "        \n",
    "        return f\"SELECT * FROM {table};\"\n",
    "    \n",
    "    def _get_numeric_columns(self, table: str) -> List[str]:\n",
    "        \"\"\"ç²å–è¡¨ä¸­çš„æ•¸å€¼åˆ—\"\"\"\n",
    "        if table not in self.schema.tables:\n",
    "            return []\n",
    "        \n",
    "        numeric_types = ['INT', 'DECIMAL', 'FLOAT', 'DOUBLE']\n",
    "        numeric_columns = []\n",
    "        \n",
    "        for column, column_type in self.schema.tables[table].items():\n",
    "            if any(num_type in column_type.upper() for num_type in numeric_types):\n",
    "                numeric_columns.append(column)\n",
    "        \n",
    "        return numeric_columns\n",
    "    \n",
    "    def _validate_sql(self, sql: str) -> Tuple[bool, str]:\n",
    "        \"\"\"é©—è­‰SQLèªæ³•\"\"\"\n",
    "        try:\n",
    "            parsed = sqlparse.parse(sql)[0]\n",
    "            if parsed.tokens:\n",
    "                return True, \"\"\n",
    "            else:\n",
    "                return False, \"Empty SQL statement\"\n",
    "        except Exception as e:\n",
    "            return False, str(e)\n",
    "\n",
    "# åˆå§‹åŒ–SQLç”Ÿæˆå™¨\n",
    "sql_generator = SQLGenerator(schema)\n",
    "\n",
    "# æ¸¬è©¦SQLç”Ÿæˆ\n",
    "test_queries = [\n",
    "    \"é¡¯ç¤ºæ‰€æœ‰ç”¢å“ä¿¡æ¯\",\n",
    "    \"æ‰¾å‡ºæœ€è²´çš„ç”¢å“\",\n",
    "    \"çµ±è¨ˆç”¨æˆ¶æ•¸é‡\"\n",
    "]\n",
    "\n",
    "print(\"=== SQLç”Ÿæˆæ¸¬è©¦ ===\")\n",
    "for query in test_queries:\n",
    "    retrieval_results = retriever.retrieve(query)\n",
    "    generation_results = sql_generator.generate_sql(query, retrieval_results)\n",
    "    \n",
    "    print(f\"\\næŸ¥è©¢: {query}\")\n",
    "    print(f\"ç”Ÿæˆçš„SQL: {generation_results['generated_sql']}\")\n",
    "    print(f\"ç”Ÿæˆæ–¹æ³•: {generation_results['method']}\")\n",
    "    print(f\"SQLæœ‰æ•ˆæ€§: {generation_results['is_valid']}\")\n",
    "    if generation_results['validation_error']:\n",
    "        print(f\"é©—è­‰éŒ¯èª¤: {generation_results['validation_error']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. å®Œæ•´æµæ°´ç·š\n",
    "\n",
    "### æ•´åˆæ‰€æœ‰çµ„ä»¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®Œæ•´çš„TextSQL RAGæµæ°´ç·š\n",
    "class TextSQLRAGPipeline:\n",
    "    def __init__(self, \n",
    "                 embedder: TextEmbedder,\n",
    "                 vector_index: VectorIndex,\n",
    "                 schema: DatabaseSchema,\n",
    "                 retriever: SemanticRetriever,\n",
    "                 sql_generator: SQLGenerator):\n",
    "        \"\"\"åˆå§‹åŒ–å®Œæ•´çš„RAGæµæ°´ç·š\"\"\"\n",
    "        self.embedder = embedder\n",
    "        self.vector_index = vector_index\n",
    "        self.schema = schema\n",
    "        self.retriever = retriever\n",
    "        self.sql_generator = sql_generator\n",
    "        self.query_history = []\n",
    "    \n",
    "    def process_query(self, natural_language_query: str, execute_sql: bool = False) -> Dict:\n",
    "        \"\"\"è™•ç†è‡ªç„¶èªè¨€æŸ¥è©¢çš„å®Œæ•´æµç¨‹\"\"\"\n",
    "        print(f\"\\nè™•ç†æŸ¥è©¢: {natural_language_query}\")\n",
    "        \n",
    "        # ç¬¬1æ­¥ï¼šæª¢ç´¢ç›¸é—œç¤ºä¾‹\n",
    "        print(\"ç¬¬1æ­¥ï¼šæª¢ç´¢ç›¸é—œç¤ºä¾‹...\")\n",
    "        retrieval_results = self.retriever.retrieve(natural_language_query)\n",
    "        \n",
    "        # ç¬¬2æ­¥ï¼šç”ŸæˆSQL\n",
    "        print(\"ç¬¬2æ­¥ï¼šç”ŸæˆSQL...\")\n",
    "        generation_results = self.sql_generator.generate_sql(natural_language_query, retrieval_results)\n",
    "        \n",
    "        # ç¬¬3æ­¥ï¼šåŸ·è¡ŒSQLï¼ˆå¯é¸ï¼‰\n",
    "        execution_results = None\n",
    "        if execute_sql and generation_results['is_valid']:\n",
    "            print(\"ç¬¬3æ­¥ï¼šåŸ·è¡ŒSQL...\")\n",
    "            execution_results = self._execute_sql(generation_results['generated_sql'])\n",
    "        \n",
    "        # ç¬¬4æ­¥ï¼šæ§‹å»ºå®Œæ•´çµæœ\n",
    "        complete_results = {\n",
    "            'natural_language_query': natural_language_query,\n",
    "            'retrieval_results': retrieval_results,\n",
    "            'generation_results': generation_results,\n",
    "            'execution_results': execution_results,\n",
    "            'pipeline_success': generation_results['is_valid'],\n",
    "            'timestamp': pd.Timestamp.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        # ç¬¬5æ­¥ï¼šè¨˜éŒ„æŸ¥è©¢æ­·å²\n",
    "        self.query_history.append(complete_results)\n",
    "        \n",
    "        return complete_results\n",
    "    \n",
    "    def _execute_sql(self, sql: str) -> Dict:\n",
    "        \"\"\"åŸ·è¡ŒSQLæŸ¥è©¢\"\"\"\n",
    "        try:\n",
    "            conn = sqlite3.connect('sample_ecommerce.db')\n",
    "            \n",
    "            # åŸ·è¡ŒæŸ¥è©¢\n",
    "            result_df = pd.read_sql_query(sql, conn)\n",
    "            \n",
    "            conn.close()\n",
    "            \n",
    "            return {\n",
    "                'success': True,\n",
    "                'data': result_df.to_dict('records'),\n",
    "                'row_count': len(result_df),\n",
    "                'columns': list(result_df.columns),\n",
    "                'error': None\n",
    "            }\n",
    "        \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'success': False,\n",
    "                'data': None,\n",
    "                'row_count': 0,\n",
    "                'columns': [],\n",
    "                'error': str(e)\n",
    "            }\n",
    "    \n",
    "    def explain_results(self, results: Dict) -> str:\n",
    "        \"\"\"è§£é‡‹è™•ç†çµæœ\"\"\"\n",
    "        explanation = f\"\\n{'='*50}\\n\"\n",
    "        explanation += f\"æŸ¥è©¢: {results['natural_language_query']}\\n\"\n",
    "        explanation += f\"{'='*50}\\n\"\n",
    "        \n",
    "        # æª¢ç´¢éšæ®µ\n",
    "        explanation += \"\\nğŸ” æª¢ç´¢éšæ®µ:\\n\"\n",
    "        retrieval = results['retrieval_results']\n",
    "        explanation += f\"æ‰¾åˆ° {len(retrieval['similar_examples'])} å€‹ç›¸ä¼¼ç¤ºä¾‹\\n\"\n",
    "        explanation += f\"æª¢ç´¢åˆ†æ•¸: {retrieval['retrieval_score']:.3f}\\n\"\n",
    "        \n",
    "        if retrieval['similar_examples']:\n",
    "            best_match = retrieval['similar_examples'][0]\n",
    "            explanation += f\"æœ€ä½³åŒ¹é…: {best_match['text']} (ç›¸ä¼¼åº¦: {1-best_match['distance']:.3f})\\n\"\n",
    "        \n",
    "        # ç”Ÿæˆéšæ®µ\n",
    "        explanation += \"\\nâš™ï¸ ç”Ÿæˆéšæ®µ:\\n\"\n",
    "        generation = results['generation_results']\n",
    "        explanation += f\"ç”Ÿæˆæ–¹æ³•: {generation['method']}\\n\"\n",
    "        explanation += f\"ç”Ÿæˆçš„SQL: {generation['generated_sql']}\\n\"\n",
    "        explanation += f\"SQLæœ‰æ•ˆæ€§: {generation['is_valid']}\\n\"\n",
    "        \n",
    "        # åŸ·è¡Œéšæ®µ\n",
    "        if results['execution_results']:\n",
    "            explanation += \"\\nğŸš€ åŸ·è¡Œéšæ®µ:\\n\"\n",
    "            execution = results['execution_results']\n",
    "            if execution['success']:\n",
    "                explanation += f\"åŸ·è¡ŒæˆåŠŸï¼Œè¿”å› {execution['row_count']} è¡Œæ•¸æ“š\\n\"\n",
    "                if execution['data']:\n",
    "                    explanation += \"å‰å¹¾è¡Œæ•¸æ“š:\\n\"\n",
    "                    for i, row in enumerate(execution['data'][:3]):\n",
    "                        explanation += f\"  {i+1}: {row}\\n\"\n",
    "            else:\n",
    "                explanation += f\"åŸ·è¡Œå¤±æ•—: {execution['error']}\\n\"\n",
    "        \n",
    "        explanation += f\"\\nâœ… æµæ°´ç·šæˆåŠŸ: {results['pipeline_success']}\\n\"\n",
    "        \n",
    "        return explanation\n",
    "    \n",
    "    def get_pipeline_stats(self) -> Dict:\n",
    "        \"\"\"ç²å–æµæ°´ç·šçµ±è¨ˆä¿¡æ¯\"\"\"\n",
    "        if not self.query_history:\n",
    "            return {'total_queries': 0}\n",
    "        \n",
    "        successful_queries = sum(1 for q in self.query_history if q['pipeline_success'])\n",
    "        \n",
    "        return {\n",
    "            'total_queries': len(self.query_history),\n",
    "            'successful_queries': successful_queries,\n",
    "            'success_rate': successful_queries / len(self.query_history),\n",
    "            'avg_retrieval_score': np.mean([q['retrieval_results']['retrieval_score'] \n",
    "                                          for q in self.query_history])\n",
    "        }\n",
    "\n",
    "# åˆå§‹åŒ–å®Œæ•´æµæ°´ç·š\n",
    "pipeline = TextSQLRAGPipeline(\n",
    "    embedder=embedder,\n",
    "    vector_index=vector_index,\n",
    "    schema=schema,\n",
    "    retriever=retriever,\n",
    "    sql_generator=sql_generator\n",
    ")\n",
    "\n",
    "print(\"âœ… TextSQL RAG æµæ°´ç·šåˆå§‹åŒ–å®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æµæ°´ç·šæ¼”ç¤º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æµæ°´ç·šæ¼”ç¤º\n",
    "demo_queries = [\n",
    "    \"é¡¯ç¤ºæ‰€æœ‰ç”¨æˆ¶çš„åŸºæœ¬ä¿¡æ¯\",\n",
    "    \"æŸ¥æ‰¾åƒ¹æ ¼æœ€é«˜çš„ç”¢å“\",\n",
    "    \"çµ±è¨ˆæ¯å€‹é¡åˆ¥æœ‰å¤šå°‘ç”¢å“\",\n",
    "    \"é¡¯ç¤ºå¼µä¸‰è³¼è²·çš„æ‰€æœ‰å•†å“\",\n",
    "    \"æ‰¾å‡ºé‚„æ²’æœ‰ä¸‹éè¨‚å–®çš„ç”¨æˆ¶\"\n",
    "]\n",
    "\n",
    "print(\"ğŸš€ é–‹å§‹æµæ°´ç·šæ¼”ç¤º\\n\")\n",
    "\n",
    "for i, query in enumerate(demo_queries, 1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"æ¼”ç¤º {i}/{len(demo_queries)}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # è™•ç†æŸ¥è©¢\n",
    "    results = pipeline.process_query(query, execute_sql=True)\n",
    "    \n",
    "    # é¡¯ç¤ºçµæœ\n",
    "    print(pipeline.explain_results(results))\n",
    "    \n",
    "    # æš«åœä¸€ä¸‹è®“è¼¸å‡ºæ›´æ¸…æ™°\n",
    "    import time\n",
    "    time.sleep(1)\n",
    "\n",
    "# é¡¯ç¤ºæ•´é«”çµ±è¨ˆ\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"æµæ°´ç·šçµ±è¨ˆ\")\n",
    "print(\"=\"*60)\n",
    "stats = pipeline.get_pipeline_stats()\n",
    "for key, value in stats.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. è©•ä¼°èˆ‡å„ªåŒ–\n",
    "\n",
    "### è©•ä¼°æŒ‡æ¨™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è©•ä¼°ç³»çµ±\n",
    "class RAGEvaluator:\n",
    "    def __init__(self, pipeline: TextSQLRAGPipeline):\n",
    "        \"\"\"åˆå§‹åŒ–è©•ä¼°å™¨\"\"\"\n",
    "        self.pipeline = pipeline\n",
    "    \n",
    "    def evaluate_retrieval(self, test_cases: List[Dict]) -> Dict:\n",
    "        \"\"\"è©•ä¼°æª¢ç´¢æ€§èƒ½\"\"\"\n",
    "        retrieval_scores = []\n",
    "        precision_scores = []\n",
    "        \n",
    "        for test_case in test_cases:\n",
    "            query = test_case['natural_language']\n",
    "            expected_sql = test_case['sql']\n",
    "            \n",
    "            # æª¢ç´¢ç›¸é—œç¤ºä¾‹\n",
    "            retrieval_results = self.pipeline.retriever.retrieve(query)\n",
    "            \n",
    "            # è¨ˆç®—æª¢ç´¢åˆ†æ•¸\n",
    "            retrieval_scores.append(retrieval_results['retrieval_score'])\n",
    "            \n",
    "            # è¨ˆç®—ç²¾ç¢ºåº¦ï¼ˆæª¢ç´¢çµæœä¸­æ˜¯å¦åŒ…å«æ­£ç¢ºç­”æ¡ˆï¼‰\n",
    "            precision = self._calculate_precision(retrieval_results, expected_sql)\n",
    "            precision_scores.append(precision)\n",
    "        \n",
    "        return {\n",
    "            'avg_retrieval_score': np.mean(retrieval_scores),\n",
    "            'avg_precision': np.mean(precision_scores),\n",
    "            'retrieval_scores': retrieval_scores,\n",
    "            'precision_scores': precision_scores\n",
    "        }\n",
    "    \n",
    "    def evaluate_generation(self, test_cases: List[Dict]) -> Dict:\n",
    "        \"\"\"è©•ä¼°SQLç”Ÿæˆæ€§èƒ½\"\"\"\n",
    "        exact_match_scores = []\n",
    "        syntax_valid_scores = []\n",
    "        semantic_similarity_scores = []\n",
    "        \n",
    "        for test_case in test_cases:\n",
    "            query = test_case['natural_language']\n",
    "            expected_sql = test_case['sql']\n",
    "            \n",
    "            # ç”ŸæˆSQL\n",
    "            results = self.pipeline.process_query(query, execute_sql=False)\n",
    "            generated_sql = results['generation_results']['generated_sql']\n",
    "            \n",
    "            # ç²¾ç¢ºåŒ¹é…\n",
    "            exact_match = self._normalize_sql(generated_sql) == self._normalize_sql(expected_sql)\n",
    "            exact_match_scores.append(exact_match)\n",
    "            \n",
    "            # èªæ³•æœ‰æ•ˆæ€§\n",
    "            syntax_valid = results['generation_results']['is_valid']\n",
    "            syntax_valid_scores.append(syntax_valid)\n",
    "            \n",
    "            # èªç¾©ç›¸ä¼¼åº¦ï¼ˆç°¡åŒ–å¯¦ç¾ï¼‰\n",
    "            semantic_sim = self._calculate_semantic_similarity(generated_sql, expected_sql)\n",
    "            semantic_similarity_scores.append(semantic_sim)\n",
    "        \n",
    "        return {\n",
    "            'exact_match_rate': np.mean(exact_match_scores),\n",
    "            'syntax_valid_rate': np.mean(syntax_valid_scores),\n",
    "            'avg_semantic_similarity': np.mean(semantic_similarity_scores),\n",
    "            'exact_match_scores': exact_match_scores,\n",
    "            'syntax_valid_scores': syntax_valid_scores,\n",
    "            'semantic_similarity_scores': semantic_similarity_scores\n",
    "        }\n",
    "    \n",
    "    def evaluate_end_to_end(self, test_cases: List[Dict]) -> Dict:\n",
    "        \"\"\"ç«¯åˆ°ç«¯è©•ä¼°\"\"\"\n",
    "        execution_success_scores = []\n",
    "        result_accuracy_scores = []\n",
    "        \n",
    "        for test_case in test_cases:\n",
    "            query = test_case['natural_language']\n",
    "            expected_sql = test_case['sql']\n",
    "            \n",
    "            # åŸ·è¡Œå®Œæ•´æµæ°´ç·š\n",
    "            results = self.pipeline.process_query(query, execute_sql=True)\n",
    "            \n",
    "            # åŸ·è¡ŒæˆåŠŸç‡\n",
    "            execution_success = (\n",
    "                results['execution_results'] is not None and \n",
    "                results['execution_results']['success']\n",
    "            )\n",
    "            execution_success_scores.append(execution_success)\n",
    "            \n",
    "            # çµæœå‡†ç¢ºæ€§ï¼ˆé€šéåŸ·è¡ŒæœŸæœ›SQLé€²è¡Œæ¯”è¼ƒï¼‰\n",
    "            if execution_success:\n",
    "                result_accuracy = self._compare_execution_results(\n",
    "                    results['generation_results']['generated_sql'],\n",
    "                    expected_sql\n",
    "                )\n",
    "                result_accuracy_scores.append(result_accuracy)\n",
    "            else:\n",
    "                result_accuracy_scores.append(0.0)\n",
    "        \n",
    "        return {\n",
    "            'execution_success_rate': np.mean(execution_success_scores),\n",
    "            'result_accuracy_rate': np.mean(result_accuracy_scores),\n",
    "            'execution_success_scores': execution_success_scores,\n",
    "            'result_accuracy_scores': result_accuracy_scores\n",
    "        }\n",
    "    \n",
    "    def _calculate_precision(self, retrieval_results: Dict, expected_sql: str) -> float:\n",
    "        \"\"\"è¨ˆç®—æª¢ç´¢ç²¾ç¢ºåº¦\"\"\"\n",
    "        similar_examples = retrieval_results['similar_examples']\n",
    "        if not similar_examples:\n",
    "            return 0.0\n",
    "        \n",
    "        # æª¢æŸ¥æ˜¯å¦æœ‰æª¢ç´¢çµæœèˆ‡æœŸæœ›SQLç›¸ä¼¼\n",
    "        for example in similar_examples:\n",
    "            if self._sql_similarity(example['metadata']['sql'], expected_sql) > 0.8:\n",
    "                return 1.0\n",
    "        \n",
    "        return 0.0\n",
    "    \n",
    "    def _normalize_sql(self, sql: str) -> str:\n",
    "        \"\"\"æ¨™æº–åŒ–SQLå­—ç¬¦ä¸²\"\"\"\n",
    "        # ç§»é™¤å¤šé¤˜ç©ºæ ¼ï¼Œè½‰æ›ç‚ºå°å¯«\n",
    "        return ' '.join(sql.lower().split())\n",
    "    \n",
    "    def _calculate_semantic_similarity(self, sql1: str, sql2: str) -> float:\n",
    "        \"\"\"è¨ˆç®—SQLèªç¾©ç›¸ä¼¼åº¦\"\"\"\n",
    "        # ç°¡åŒ–å¯¦ç¾ï¼šåŸºæ–¼é—œéµè©é‡ç–Š\n",
    "        words1 = set(self._normalize_sql(sql1).split())\n",
    "        words2 = set(self._normalize_sql(sql2).split())\n",
    "        \n",
    "        if not words1 and not words2:\n",
    "            return 1.0\n",
    "        if not words1 or not words2:\n",
    "            return 0.0\n",
    "        \n",
    "        intersection = words1.intersection(words2)\n",
    "        union = words1.union(words2)\n",
    "        \n",
    "        return len(intersection) / len(union)\n",
    "    \n",
    "    def _sql_similarity(self, sql1: str, sql2: str) -> float:\n",
    "        \"\"\"è¨ˆç®—SQLç›¸ä¼¼åº¦\"\"\"\n",
    "        return self._calculate_semantic_similarity(sql1, sql2)\n",
    "    \n",
    "    def _compare_execution_results(self, generated_sql: str, expected_sql: str) -> float:\n",
    "        \"\"\"æ¯”è¼ƒåŸ·è¡Œçµæœ\"\"\"\n",
    "        try:\n",
    "            conn = sqlite3.connect('sample_ecommerce.db')\n",
    "            \n",
    "            # åŸ·è¡Œå…©å€‹SQL\n",
    "            result1 = pd.read_sql_query(generated_sql, conn)\n",
    "            result2 = pd.read_sql_query(expected_sql, conn)\n",
    "            \n",
    "            conn.close()\n",
    "            \n",
    "            # æ¯”è¼ƒçµæœ\n",
    "            if result1.equals(result2):\n",
    "                return 1.0\n",
    "            elif len(result1) == len(result2) and len(result1.columns) == len(result2.columns):\n",
    "                return 0.5  # éƒ¨åˆ†åŒ¹é…\n",
    "            else:\n",
    "                return 0.0\n",
    "                \n",
    "        except Exception:\n",
    "            return 0.0\n",
    "    \n",
    "    def generate_evaluation_report(self, test_cases: List[Dict]) -> str:\n",
    "        \"\"\"ç”Ÿæˆè©•ä¼°å ±å‘Š\"\"\"\n",
    "        retrieval_eval = self.evaluate_retrieval(test_cases)\n",
    "        generation_eval = self.evaluate_generation(test_cases)\n",
    "        e2e_eval = self.evaluate_end_to_end(test_cases)\n",
    "        \n",
    "        report = \"\\n\" + \"=\"*50 + \"\\n\"\n",
    "        report += \"TextSQL RAG Pipeline è©•ä¼°å ±å‘Š\\n\"\n",
    "        report += \"=\"*50 + \"\\n\"\n",
    "        \n",
    "        report += \"\\nğŸ“Š æª¢ç´¢æ€§èƒ½:\\n\"\n",
    "        report += f\"  å¹³å‡æª¢ç´¢åˆ†æ•¸: {retrieval_eval['avg_retrieval_score']:.3f}\\n\"\n",
    "        report += f\"  å¹³å‡ç²¾ç¢ºåº¦: {retrieval_eval['avg_precision']:.3f}\\n\"\n",
    "        \n",
    "        report += \"\\nğŸ”§ ç”Ÿæˆæ€§èƒ½:\\n\"\n",
    "        report += f\"  ç²¾ç¢ºåŒ¹é…ç‡: {generation_eval['exact_match_rate']:.3f}\\n\"\n",
    "        report += f\"  èªæ³•æœ‰æ•ˆç‡: {generation_eval['syntax_valid_rate']:.3f}\\n\"\n",
    "        report += f\"  å¹³å‡èªç¾©ç›¸ä¼¼åº¦: {generation_eval['avg_semantic_similarity']:.3f}\\n\"\n",
    "        \n",
    "        report += \"\\nğŸš€ ç«¯åˆ°ç«¯æ€§èƒ½:\\n\"\n",
    "        report += f\"  åŸ·è¡ŒæˆåŠŸç‡: {e2e_eval['execution_success_rate']:.3f}\\n\"\n",
    "        report += f\"  çµæœå‡†ç¢ºç‡: {e2e_eval['result_accuracy_rate']:.3f}\\n\"\n",
    "        \n",
    "        # ç¸½é«”è©•åˆ†\n",
    "        overall_score = np.mean([\n",
    "            retrieval_eval['avg_retrieval_score'],\n",
    "            generation_eval['syntax_valid_rate'],\n",
    "            e2e_eval['execution_success_rate']\n",
    "        ])\n",
    "        \n",
    "        report += f\"\\nâ­ ç¸½é«”è©•åˆ†: {overall_score:.3f}\\n\"\n",
    "        \n",
    "        return report\n",
    "\n",
    "# å‰µå»ºè©•ä¼°å™¨\n",
    "evaluator = RAGEvaluator(pipeline)\n",
    "\n",
    "# é‹è¡Œè©•ä¼°\n",
    "print(\"ğŸ” é–‹å§‹è©•ä¼°æµæ°´ç·šæ€§èƒ½...\")\n",
    "evaluation_report = evaluator.generate_evaluation_report(training_data)\n",
    "print(evaluation_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. å¯¦éš›æ‡‰ç”¨æ¡ˆä¾‹\n",
    "\n",
    "### äº’å‹•å¼æŸ¥è©¢ç•Œé¢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# äº’å‹•å¼æŸ¥è©¢ç•Œé¢\n",
    "class InteractiveQueryInterface:\n",
    "    def __init__(self, pipeline: TextSQLRAGPipeline):\n",
    "        \"\"\"åˆå§‹åŒ–äº’å‹•å¼ç•Œé¢\"\"\"\n",
    "        self.pipeline = pipeline\n",
    "        self.session_history = []\n",
    "    \n",
    "    def start_session(self):\n",
    "        \"\"\"é–‹å§‹äº’å‹•æœƒè©±\"\"\"\n",
    "        print(\"\\nğŸ¯ æ­¡è¿ä½¿ç”¨ TextSQL RAG æŸ¥è©¢ç³»çµ±ï¼\")\n",
    "        print(\"è¼¸å…¥è‡ªç„¶èªè¨€æŸ¥è©¢ï¼Œç³»çµ±å°‡ç”Ÿæˆå°æ‡‰çš„SQLèªå¥ä¸¦åŸ·è¡Œ\")\n",
    "        print(\"è¼¸å…¥ 'help' æŸ¥çœ‹å¹«åŠ©ï¼Œè¼¸å…¥ 'quit' é€€å‡ºç³»çµ±\\n\")\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                user_input = input(\"ğŸ” è«‹è¼¸å…¥æ‚¨çš„æŸ¥è©¢: \").strip()\n",
    "                \n",
    "                if user_input.lower() == 'quit':\n",
    "                    print(\"ğŸ‘‹ æ„Ÿè¬ä½¿ç”¨ï¼Œå†è¦‹ï¼\")\n",
    "                    break\n",
    "                elif user_input.lower() == 'help':\n",
    "                    self._show_help()\n",
    "                    continue\n",
    "                elif user_input.lower() == 'history':\n",
    "                    self._show_history()\n",
    "                    continue\n",
    "                elif user_input.lower() == 'schema':\n",
    "                    self._show_schema()\n",
    "                    continue\n",
    "                elif not user_input:\n",
    "                    continue\n",
    "                \n",
    "                # è™•ç†æŸ¥è©¢\n",
    "                self._process_interactive_query(user_input)\n",
    "                \n",
    "            except KeyboardInterrupt:\n",
    "                print(\"\\nğŸ‘‹ ç”¨æˆ¶ä¸­æ–·ï¼Œå†è¦‹ï¼\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ ç™¼ç”ŸéŒ¯èª¤: {e}\")\n",
    "    \n",
    "    def _process_interactive_query(self, query: str):\n",
    "        \"\"\"è™•ç†äº’å‹•æŸ¥è©¢\"\"\"\n",
    "        print(f\"\\nâ³ è™•ç†ä¸­...\")\n",
    "        \n",
    "        # åŸ·è¡ŒæŸ¥è©¢\n",
    "        results = self.pipeline.process_query(query, execute_sql=True)\n",
    "        \n",
    "        # è¨˜éŒ„æœƒè©±æ­·å²\n",
    "        self.session_history.append(results)\n",
    "        \n",
    "        # é¡¯ç¤ºçµæœ\n",
    "        self._display_results(results)\n",
    "    \n",
    "    def _display_results(self, results: Dict):\n",
    "        \"\"\"é¡¯ç¤ºæŸ¥è©¢çµæœ\"\"\"\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(f\"ğŸ“ æŸ¥è©¢: {results['natural_language_query']}\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        # é¡¯ç¤ºç”Ÿæˆçš„SQL\n",
    "        generation = results['generation_results']\n",
    "        print(f\"\\nğŸ’» ç”Ÿæˆçš„SQL:\")\n",
    "        print(f\"```sql\\n{generation['generated_sql']}\\n```\")\n",
    "        \n",
    "        print(f\"\\nğŸ”§ ç”Ÿæˆæ–¹æ³•: {generation['method']}\")\n",
    "        print(f\"âœ… SQLæœ‰æ•ˆæ€§: {generation['is_valid']}\")\n",
    "        \n",
    "        # é¡¯ç¤ºåŸ·è¡Œçµæœ\n",
    "        if results['execution_results']:\n",
    "            execution = results['execution_results']\n",
    "            if execution['success']:\n",
    "                print(f\"\\nğŸ¯ åŸ·è¡Œçµæœ: æˆåŠŸè¿”å› {execution['row_count']} è¡Œæ•¸æ“š\")\n",
    "                \n",
    "                if execution['data']:\n",
    "                    # å°‡çµæœæ ¼å¼åŒ–ç‚ºè¡¨æ ¼\n",
    "                    df = pd.DataFrame(execution['data'])\n",
    "                    print(\"\\nğŸ“Š æŸ¥è©¢çµæœ:\")\n",
    "                    print(df.to_string(index=False))\n",
    "                else:\n",
    "                    print(\"\\nğŸ“Š æŸ¥è©¢çµæœ: ç„¡æ•¸æ“šè¿”å›\")\n",
    "            else:\n",
    "                print(f\"\\nâŒ åŸ·è¡Œå¤±æ•—: {execution['error']}\")\n",
    "        \n",
    "        # é¡¯ç¤ºæª¢ç´¢ä¿¡æ¯\n",
    "        retrieval = results['retrieval_results']\n",
    "        if retrieval['similar_examples']:\n",
    "            best_match = retrieval['similar_examples'][0]\n",
    "            print(f\"\\nğŸ” æœ€ä½³åŒ¹é…ç¤ºä¾‹: {best_match['text']}\")\n",
    "            print(f\"ğŸ“ˆ ç›¸ä¼¼åº¦: {1-best_match['distance']:.3f}\")\n",
    "    \n",
    "    def _show_help(self):\n",
    "        \"\"\"é¡¯ç¤ºå¹«åŠ©ä¿¡æ¯\"\"\"\n",
    "        help_text = \"\"\"\n",
    "ğŸ“š ä½¿ç”¨å¹«åŠ©:\n",
    "\n",
    "åŸºæœ¬æŸ¥è©¢ç¤ºä¾‹:\n",
    "  â€¢ \"é¡¯ç¤ºæ‰€æœ‰ç”¨æˆ¶\" - æŸ¥è©¢ç”¨æˆ¶è¡¨\n",
    "  â€¢ \"æ‰¾å‡ºæœ€è²´çš„ç”¢å“\" - æŒ‰åƒ¹æ ¼æ’åº\n",
    "  â€¢ \"çµ±è¨ˆæ¯å€‹é¡åˆ¥çš„ç”¢å“æ•¸é‡\" - èšåˆæŸ¥è©¢\n",
    "  â€¢ \"é¡¯ç¤ºå¼µä¸‰çš„æ‰€æœ‰è¨‚å–®\" - è¯åˆæŸ¥è©¢\n",
    "\n",
    "ç‰¹æ®Šå‘½ä»¤:\n",
    "  â€¢ help - é¡¯ç¤ºæ­¤å¹«åŠ©\n",
    "  â€¢ history - é¡¯ç¤ºæŸ¥è©¢æ­·å²\n",
    "  â€¢ schema - é¡¯ç¤ºæ•¸æ“šåº«çµæ§‹\n",
    "  â€¢ quit - é€€å‡ºç³»çµ±\n",
    "\n",
    "ğŸ’¡ æç¤º: ç›¡é‡ä½¿ç”¨è‡ªç„¶èªè¨€æè¿°æ‚¨çš„æŸ¥è©¢éœ€æ±‚ï¼\n",
    "\"\"\"\n",
    "        print(help_text)\n",
    "    \n",
    "    def _show_history(self):\n",
    "        \"\"\"é¡¯ç¤ºæŸ¥è©¢æ­·å²\"\"\"\n",
    "        if not self.session_history:\n",
    "            print(\"\\nğŸ“ æš«ç„¡æŸ¥è©¢æ­·å²\")\n",
    "            return\n",
    "        \n",
    "        print(f\"\\nğŸ“ æŸ¥è©¢æ­·å² (å…± {len(self.session_history)} æ¢):\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        for i, result in enumerate(self.session_history[-5:], 1):  # åªé¡¯ç¤ºæœ€è¿‘5æ¢\n",
    "            query = result['natural_language_query']\n",
    "            sql = result['generation_results']['generated_sql']\n",
    "            success = result['pipeline_success']\n",
    "            status = \"âœ…\" if success else \"âŒ\"\n",
    "            \n",
    "            print(f\"{i}. {status} {query}\")\n",
    "            print(f\"   SQL: {sql[:50]}{'...' if len(sql) > 50 else ''}\")\n",
    "            print()\n",
    "    \n",
    "    def _show_schema(self):\n",
    "        \"\"\"é¡¯ç¤ºæ•¸æ“šåº«çµæ§‹\"\"\"\n",
    "        print(\"\\nğŸ—ƒï¸ æ•¸æ“šåº«çµæ§‹:\")\n",
    "        print(self.pipeline.schema.get_schema_info())\n",
    "\n",
    "# å‰µå»ºäº’å‹•ç•Œé¢\n",
    "interface = InteractiveQueryInterface(pipeline)\n",
    "\n",
    "# æ¼”ç¤ºä¸€äº›è‡ªå‹•æŸ¥è©¢ï¼ˆè€Œä¸æ˜¯çœŸæ­£çš„äº’å‹•æ¨¡å¼ï¼‰\n",
    "print(\"\\nğŸ¯ TextSQL RAG ç³»çµ±æ¼”ç¤º\")\n",
    "print(\"ä»¥ä¸‹æ˜¯ä¸€äº›è‡ªå‹•åŸ·è¡Œçš„æŸ¥è©¢ç¤ºä¾‹:\\n\")\n",
    "\n",
    "demo_queries = [\n",
    "    \"æŸ¥çœ‹æ‰€æœ‰ç”¨æˆ¶çš„ä¿¡æ¯\",\n",
    "    \"æ‰¾å‡ºåƒ¹æ ¼æœ€é«˜çš„ä¸‰å€‹ç”¢å“\",\n",
    "    \"çµ±è¨ˆæ¯å€‹ç”¨æˆ¶çš„è¨‚å–®ç¸½æ•¸\"\n",
    "]\n",
    "\n",
    "for query in demo_queries:\n",
    "    print(f\"ğŸ” æŸ¥è©¢: {query}\")\n",
    "    interface._process_interactive_query(query)\n",
    "    print(\"\\n\" + \"-\"*60 + \"\\n\")\n",
    "\n",
    "print(\"\\nâœ¨ æ¼”ç¤ºå®Œæˆï¼è¦é–‹å§‹çœŸæ­£çš„äº’å‹•æ¨¡å¼ï¼Œè«‹å–æ¶ˆè¨»é‡‹ä¸‹é¢çš„ä»£ç¢¼ï¼š\")\n",
    "print(\"# interface.start_session()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç¸½çµèˆ‡å„ªåŒ–å»ºè­°\n",
    "\n",
    "### å­¸ç¿’ç¸½çµ\n",
    "\n",
    "é€šéæœ¬ç­†è¨˜æœ¬ï¼Œæˆ‘å€‘å®Œæ•´å¯¦ç¾äº†ä¸€å€‹ TextSQL RAG æµæ°´ç·šï¼ŒåŒ…æ‹¬ï¼š\n",
    "\n",
    "1. **æ•¸æ“šé è™•ç†**: å‰µå»ºç¤ºä¾‹æ•¸æ“šåº«å’Œè¨“ç·´æ•¸æ“š\n",
    "2. **å‘é‡åŒ–**: ä½¿ç”¨å¥å­åµŒå…¥æ¨¡å‹å°‡æ–‡æœ¬è½‰æ›ç‚ºå‘é‡\n",
    "3. **æª¢ç´¢ç³»çµ±**: åŸºæ–¼èªç¾©ç›¸ä¼¼åº¦æª¢ç´¢ç›¸é—œç¤ºä¾‹\n",
    "4. **SQLç”Ÿæˆ**: çµåˆæª¢ç´¢çµæœå’Œæ¨¡æ¿ç”ŸæˆSQL\n",
    "5. **å®Œæ•´æµæ°´ç·š**: æ•´åˆæ‰€æœ‰çµ„ä»¶\n",
    "6. **è©•ä¼°ç³»çµ±**: å¤šç¶­åº¦è©•ä¼°æ€§èƒ½\n",
    "7. **å¯¦éš›æ‡‰ç”¨**: äº’å‹•å¼æŸ¥è©¢ç•Œé¢\n",
    "\n",
    "### å„ªåŒ–å»ºè­°\n",
    "\n",
    "1. **æ¨¡å‹å„ªåŒ–**:\n",
    "   - ä½¿ç”¨æ›´å¼·å¤§çš„åµŒå…¥æ¨¡å‹ï¼ˆå¦‚ OpenAI embeddingsï¼‰\n",
    "   - å¯¦ç¾å¾®èª¿æ©Ÿåˆ¶ä»¥é©æ‡‰ç‰¹å®šé ˜åŸŸ\n",
    "\n",
    "2. **æª¢ç´¢å„ªåŒ–**:\n",
    "   - å¯¦ç¾æ··åˆæª¢ç´¢ï¼ˆèªç¾©+é—œéµè©ï¼‰\n",
    "   - æ·»åŠ é‡æ’åºæ©Ÿåˆ¶\n",
    "\n",
    "3. **ç”Ÿæˆå„ªåŒ–**:\n",
    "   - é›†æˆå¤§èªè¨€æ¨¡å‹ï¼ˆå¦‚ GPT-4ï¼‰\n",
    "   - å¯¦ç¾æ›´è¤‡é›œçš„SQLæ¨¡æ¿\n",
    "\n",
    "4. **ç³»çµ±å„ªåŒ–**:\n",
    "   - æ·»åŠ ç·©å­˜æ©Ÿåˆ¶\n",
    "   - å¯¦ç¾åˆ†ä½ˆå¼éƒ¨ç½²\n",
    "   - å¢å¼·éŒ¯èª¤è™•ç†\n",
    "\n",
    "### å¾ŒçºŒå­¸ç¿’æ–¹å‘\n",
    "\n",
    "1. æ·±å…¥å­¸ç¿’ Transformer æ¶æ§‹\n",
    "2. æ¢ç´¢æ›´é«˜ç´šçš„ RAG æŠ€è¡“\n",
    "3. å­¸ç¿’æ•¸æ“šåº«å„ªåŒ–æŠ€è¡“\n",
    "4. ç ”ç©¶å¤šæ¨¡æ…‹ RAG ç³»çµ±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¿å­˜æµæ°´ç·šç‹€æ…‹å’Œçµæœ\n",
    "import pickle\n",
    "\n",
    "def save_pipeline_state():\n",
    "    \"\"\"ä¿å­˜æµæ°´ç·šç‹€æ…‹\"\"\"\n",
    "    state = {\n",
    "        'query_history': pipeline.query_history,\n",
    "        'pipeline_stats': pipeline.get_pipeline_stats(),\n",
    "        'training_data': training_data,\n",
    "        'schema_info': schema.get_schema_info()\n",
    "    }\n",
    "    \n",
    "    with open('pipeline_state.pkl', 'wb') as f:\n",
    "        pickle.dump(state, f)\n",
    "    \n",
    "    # ä¹Ÿä¿å­˜ç‚ºJSONæ ¼å¼ä¾¿æ–¼æŸ¥çœ‹\n",
    "    json_state = {\n",
    "        'pipeline_stats': pipeline.get_pipeline_stats(),\n",
    "        'training_data': training_data,\n",
    "        'schema_info': schema.get_schema_info()\n",
    "    }\n",
    "    \n",
    "    with open('pipeline_summary.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(json_state, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(\"âœ… æµæ°´ç·šç‹€æ…‹å·²ä¿å­˜\")\n",
    "\n",
    "# ä¿å­˜ç‹€æ…‹\n",
    "save_pipeline_state()\n",
    "\n",
    "print(\"\\nğŸ‰ TextSQL RAG Pipeline å­¸ç¿’ç­†è¨˜å®Œæˆï¼\")\n",
    "print(\"\\nğŸ“ å­¸ç¿’æˆæœ:\")\n",
    "print(f\"  â€¢ è™•ç†äº† {len(pipeline.query_history)} å€‹æŸ¥è©¢\")\n",
    "print(f\"  â€¢ æˆåŠŸç‡: {pipeline.get_pipeline_stats().get('success_rate', 0):.1%}\")\n",
    "print(f\"  â€¢ å¹³å‡æª¢ç´¢åˆ†æ•¸: {pipeline.get_pipeline_stats().get('avg_retrieval_score', 0):.3f}\")\n",
    "print(\"\\nğŸš€ æ‚¨ç¾åœ¨å¯ä»¥:\")\n",
    "print(\"  1. åœ¨ Kaggle ç’°å¢ƒä¸­é‹è¡Œæ­¤ç­†è¨˜æœ¬\")\n",
    "print(\"  2. ä¿®æ”¹æ•¸æ“šåº«çµæ§‹å’Œè¨“ç·´æ•¸æ“š\")\n",
    "print(\"  3. å„ªåŒ–æª¢ç´¢å’Œç”Ÿæˆç®—æ³•\")\n",
    "print(\"  4. é›†æˆæ›´å¼·å¤§çš„èªè¨€æ¨¡å‹\")\n",
    "print(\"\\nğŸ’¡ å»ºè­°ä¸‹ä¸€æ­¥: å˜—è©¦åœ¨çœŸå¯¦æ•¸æ“šé›†ä¸Šæ¸¬è©¦æ­¤ç³»çµ±ï¼\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
