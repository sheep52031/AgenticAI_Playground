{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🚀 Arctic-Text2SQL-R1 Kaggle GPU 微調實戰\n",
    "\n",
    "## 📋 專案概述\n",
    "\n",
    "本筆記本實現了基於 **Snowflake Arctic-Text2SQL-R1 7B** 模型的 Text2SQL RAG 系統，專門針對 **Kaggle GPU 環境** 優化。\n",
    "\n",
    "### 🎯 核心特色\n",
    "- **模型**: Arctic-Text2SQL-R1 7B（SOTA 小模型，BIRD ExecAcc 57%）\n",
    "- **微調方法**: QLoRA 4-bit 量化 + LoRA 適配器\n",
    "- **記憶體需求**: ≈5GB VRAM（適合 Kaggle P100/T4）\n",
    "- **斷點續訓**: 完整的檢查點管理系統\n",
    "- **資料集**: SynSQL-2.5M 高質量合成資料\n",
    "\n",
    "### 📊 預期效果\n",
    "- **Spider-Dev**: EM 72% / Exec Acc 86%\n",
    "- **BIRD-Dev**: Exec Acc ~57%\n",
    "- **訓練時間**: 約 4-6 小時（Kaggle P100）\n",
    "\n",
    "### 🔧 技術架構\n",
    "```\n",
    "自然語言查詢 → Schema檢索 → Arctic生成 → SQL執行驗證 → 自反饋修正\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🛠️ 第一部分：環境設置與依賴安裝\n",
    "\n",
    "### 重要提醒\n",
    "- 確保 Kaggle 筆記本已啟用 **GPU** 加速器\n",
    "- 建議使用 **P100** 或 **T4** GPU\n",
    "- 預估訓練時間：4-6 小時"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 系統環境檢查\n",
      "==================================================\n",
      "✅ GPU 可用: Tesla P100-PCIE-16GB\n",
      "📊 GPU 記憶體: 15.9 GB\n",
      "🎯 記憶體充足，適合 7B 模型微調\n",
      "🐍 Python 版本: 3.11.13\n",
      "🏆 Kaggle 環境: Interactive\n"
     ]
    }
   ],
   "source": [
    "# 檢查 GPU 可用性與規格\n",
    "import torch\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "print(\"🔍 系統環境檢查\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# GPU 檢查\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    print(f\"✅ GPU 可用: {gpu_name}\")\n",
    "    print(f\"📊 GPU 記憶體: {gpu_memory:.1f} GB\")\n",
    "    \n",
    "    # 檢查是否適合訓練\n",
    "    if gpu_memory >= 15:\n",
    "        print(\"🎯 記憶體充足，適合 7B 模型微調\")\n",
    "    elif gpu_memory >= 8:\n",
    "        print(\"⚠️  記憶體中等，建議使用最激進的量化設置\")\n",
    "    else:\n",
    "        print(\"❌ 記憶體不足，建議使用更小的模型\")\n",
    "else:\n",
    "    print(\"❌ 未檢測到 GPU，請啟用 GPU 加速器\")\n",
    "    \n",
    "# Python 版本\n",
    "print(f\"🐍 Python 版本: {sys.version.split()[0]}\")\n",
    "\n",
    "# Kaggle 環境檢查\n",
    "import os\n",
    "if 'KAGGLE_KERNEL_RUN_TYPE' in os.environ:\n",
    "    print(f\"🏆 Kaggle 環境: {os.environ['KAGGLE_KERNEL_RUN_TYPE']}\")\n",
    "else:\n",
    "    print(\"💻 本地環境\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 安裝必要套件...\n",
      "==================================================\n",
      "✅ transformers 安裝成功\n",
      "✅ peft 安裝成功\n",
      "✅ bitsandbytes 安裝成功\n",
      "✅ accelerate 安裝成功\n",
      "✅ datasets 安裝成功\n",
      "✅ tensorboard 安裝成功\n",
      "✅ wandb 安裝成功\n",
      "✅ sqlparse 安裝成功\n",
      "✅ evaluate 安裝成功\n",
      "✅ scikit-learn 安裝成功\n",
      "✅ matplotlib 安裝成功\n",
      "✅ seaborn 安裝成功\n",
      "✅ tqdm 安裝成功\n",
      "\n",
      "🎉 套件安裝完成！\n"
     ]
    }
   ],
   "source": [
    "# 安裝必要套件 - 針對 Kaggle 環境優化\n",
    "print(\"📦 安裝必要套件...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 核心套件列表\n",
    "packages = [\n",
    "    \"transformers>=4.36.0\",  # 支援 Arctic 模型\n",
    "    \"peft>=0.7.0\",           # PEFT/LoRA 支援\n",
    "    \"bitsandbytes>=0.41.0\",  # QLoRA 量化\n",
    "    \"accelerate>=0.25.0\",    # 分散式訓練\n",
    "    \"datasets>=2.15.0\",      # 資料集處理\n",
    "    \"tensorboard\",           # 訓練監控\n",
    "    \"wandb\",                 # 實驗追蹤（可選）\n",
    "    \"sqlparse\",              # SQL 解析\n",
    "    \"evaluate\",              # 評估指標\n",
    "    \"scikit-learn\",          # 機器學習工具\n",
    "    \"matplotlib\",            # 視覺化\n",
    "    \"seaborn\",               # 進階視覺化\n",
    "    \"tqdm\"                   # 進度條\n",
    "]\n",
    "\n",
    "# 批量安裝\n",
    "for package in packages:\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            [sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package],\n",
    "            capture_output=True, text=True, timeout=300\n",
    "        )\n",
    "        if result.returncode == 0:\n",
    "            print(f\"✅ {package.split('>=')[0]} 安裝成功\")\n",
    "        else:\n",
    "            print(f\"⚠️  {package.split('>=')[0]} 安裝警告: {result.stderr[:100]}\")\n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(f\"⏰ {package.split('>=')[0]} 安裝超時，跳過\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ {package.split('>=')[0]} 安裝失敗: {str(e)[:100]}\")\n",
    "\n",
    "print(\"\\n🎉 套件安裝完成！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 🔐 HuggingFace 認證設置 - Kaggle 優化版本\nimport getpass\nimport os\nfrom huggingface_hub import login, whoami\nfrom huggingface_hub.utils import HfHubHTTPError\nimport time\n\ndef setup_huggingface_auth_kaggle():\n    \"\"\"\n    Kaggle 優化的 HuggingFace 認證設置\n    更好的互動體驗和錯誤處理\n    \"\"\"\n    \n    print(\"🔐 HuggingFace 認證設置\")\n    print(\"=\" * 50)\n    \n    # 檢查是否已經登入\n    try:\n        user_info = whoami()\n        if user_info:\n            print(f\"✅ 已登入 HuggingFace\")\n            print(f\"   • 用戶名: {user_info.get('name', 'Unknown')}\")\n            return True\n    except:\n        pass\n    \n    print(\"💡 建議使用 HuggingFace Token 以獲得：\")\n    print(\"   • 更快的下載速度\")\n    print(\"   • 避免 API 速率限制\")\n    print(\"   • 更穩定的連接\")\n    print()\n    \n    # 提供更明確的選擇\n    print(\"請在下方選擇操作：\")\n    print(\"─\" * 30)\n    \n    return True\n\ndef input_hf_token():\n    \"\"\"專門用於輸入 HuggingFace Token 的函數\"\"\"\n    try:\n        print(\"🔑 請輸入您的 HuggingFace Token：\")\n        print(\"💡 Token 不會顯示在螢幕上，請放心輸入\")\n        print(\"📝 如需獲取 Token：https://huggingface.co/settings/tokens\")\n        print()\n        \n        # 安全輸入\n        hf_token = getpass.getpass(\"請輸入 Token: \").strip()\n        \n        if not hf_token:\n            print(\"❌ Token 不能為空\")\n            return False\n        \n        print(\"🔄 正在驗證 Token...\")\n        \n        # 登入驗證\n        login(token=hf_token, add_to_git_credential=False)\n        \n        # 獲取用戶信息\n        user_info = whoami()\n        \n        print(\"✅ HuggingFace 認證成功！\")\n        print(f\"   • 用戶名: {user_info.get('name', 'Unknown')}\")\n        print(f\"   • 用戶類型: {user_info.get('type', 'user')}\")\n        print(f\"   • 認證狀態: 已驗證\")\n        \n        return True\n        \n    except HfHubHTTPError as e:\n        if e.response.status_code == 401:\n            print(\"❌ Token 無效或已過期\")\n            print(\"   請檢查 Token 是否正確複製\")\n        else:\n            print(f\"❌ 網路錯誤: {e}\")\n        return False\n        \n    except Exception as e:\n        print(f\"❌ 認證失敗: {str(e)}\")\n        return False\n\ndef skip_auth():\n    \"\"\"跳過認證，使用匿名訪問\"\"\"\n    print(\"🔓 使用匿名訪問\")\n    print(\"⚠️  注意：可能會遇到下載速度限制\")\n    print(\"   • 模型和資料集仍可正常使用\")\n    print(\"   • 如遇到問題，請重新運行並選擇認證\")\n    return False\n\n# 執行初始設置\nauth_setup_result = setup_huggingface_auth_kaggle()\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"🎯 HuggingFace 設置準備完成\")\nprint(\"📋 接下來請選擇認證方式：\")\nprint()\nprint(\"選項 A: 使用 Token（推薦）\")\nprint(\"選項 B: 跳過認證\")\nprint()\nprint(\"💡 請運行下方對應的 Cell 來完成設置\")"
  },
  {
   "cell_type": "code",
   "source": "# 選項 B: 跳過認證，使用匿名訪問 🔓\n# 如果您沒有 Token 或想跳過認證，請運行此 Cell\n\nprint(\"🔓 選項 B: 跳過認證\")\nprint(\"=\" * 40)\n\n# 執行跳過認證\nskip_result = skip_auth()\n\nprint(\"\\n✅ 匿名訪問設置完成\")\nprint(\"📝 注意事項：\")\nprint(\"   • 下載速度可能較慢\")\nprint(\"   • 可能遇到 API 限制\")\nprint(\"   • 所有公開資源仍可正常使用\")\nprint(\"\\n🎯 可以繼續執行後續 Cell\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# 選項 A: 使用 HuggingFace Token 認證 🔑\n# 如果您有 HuggingFace Token，請運行此 Cell\n\nprint(\"🔑 選項 A: 使用 HuggingFace Token\")\nprint(\"=\" * 40)\n\n# 執行 Token 輸入\nauth_success = input_hf_token()\n\nif auth_success:\n    print(\"\\n🎉 Token 認證完成！\")\n    print(\"✅ 可以享受更快的下載速度\")\nelse:\n    print(\"\\n❌ Token 認證失敗\")\n    print(\"💡 您可以：\")\n    print(\"   1. 檢查 Token 是否正確\")\n    print(\"   2. 重新運行此 Cell\")\n    print(\"   3. 或運行下方 '選項 B' 使用匿名訪問\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔄 第二部分：Kaggle 檢查點管理系統\n",
    "\n",
    "### 核心設計理念\n",
    "基於 README.md 建議，我們採用 **HuggingFace Trainer + PEFT** 架構：\n",
    "- **成熟的斷點機制**: 內建 `resume_from_checkpoint` 功能\n",
    "- **自動斷點偵測**: 使用 `trainer_utils.get_last_checkpoint()`\n",
    "- **記憶體效率**: QLoRA 4-bit 量化讓 7B 模型僅需約 5GB VRAM\n",
    "- **保存簡潔**: 只需保存 adapter 權重（通常 < 50MB）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 導入必要的庫\nimport os\nimport json\nimport torch\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pathlib import Path\nfrom datetime import datetime\nfrom typing import Dict, List, Optional, Tuple, Any\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# HuggingFace 生態系統\nimport transformers  # 添加這行來修復 transformers.__version__ 錯誤\nfrom transformers import (\n    AutoModelForCausalLM, \n    AutoTokenizer, \n    TrainingArguments, \n    Trainer,\n    BitsAndBytesConfig,\n    EarlyStoppingCallback,\n    TrainerCallback\n)\nfrom peft import LoraConfig, get_peft_model, PeftModel, TaskType\nfrom datasets import Dataset, load_dataset\nimport transformers.trainer_utils as trainer_utils\n\n# 其他工具\nimport sqlparse\nfrom tqdm.auto import tqdm\nimport sqlite3\n\n# 設定隨機種子確保結果可重現\ntorch.manual_seed(42)\nnp.random.seed(42)\n\n# 設定圖表樣式\nplt.style.use('seaborn-v0_8')\nsns.set_palette(\"husl\")\n\nprint(\"✅ 所有庫導入成功！\")\nprint(f\"🔧 PyTorch 版本: {torch.__version__}\")\nprint(f\"🤗 Transformers 版本: {transformers.__version__}\")\n\n# Kaggle 專用檢查點管理器 - 防止訓練中斷的關鍵組件\nclass KaggleCheckpointManager:\n    \"\"\"\n    Kaggle 專用的檢查點管理器\n    \n    功能特色：\n    1. 自動偵測最新檢查點\n    2. 保存/載入訓練元數據\n    3. 支援多檢查點備份\n    4. Kaggle 環境適配\n    \"\"\"\n    \n    def __init__(self, base_dir=\"/kaggle/working\"):\n        \"\"\"初始化檢查點管理器\n        \n        Args:\n            base_dir: 基礎目錄，Kaggle 環境建議使用 /kaggle/working\n        \"\"\"\n        self.base_dir = Path(base_dir)\n        self.checkpoint_dir = self.base_dir / \"checkpoints\"\n        self.checkpoint_dir.mkdir(exist_ok=True, parents=True)\n        \n        # 元數據文件路徑\n        self.metadata_file = self.checkpoint_dir / \"training_metadata.json\"\n        \n        print(f\"📁 檢查點目錄: {self.checkpoint_dir}\")\n\n    def save_metadata(self, current_step: int, epoch: float, \n                     loss_history: List[float], learning_rate: float):\n        \"\"\"保存訓練元數據 - 關鍵的訓練狀態追蹤\n        \n        這個方法確保即使 Kaggle 意外中斷，我們也能精確知道訓練進度\n        \"\"\"\n        metadata = {\n            \"current_step\": current_step,\n            \"epoch\": epoch,\n            \"timestamp\": datetime.now().isoformat(),\n            \"loss_history\": loss_history[-20:],  # 保存最近20個loss值用於分析\n            \"learning_rate\": learning_rate,\n            \"kaggle_session\": os.environ.get('KAGGLE_KERNEL_RUN_TYPE', 'unknown'),\n            \"gpu_name\": torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"none\",\n            \"gpu_memory_used\": torch.cuda.memory_allocated() / 1024**3 if torch.cuda.is_available() else 0\n        }\n        \n        try:\n            with open(self.metadata_file, 'w', encoding='utf-8') as f:\n                json.dump(metadata, f, indent=2, ensure_ascii=False)\n            print(f\"💾 元數據已保存 [Step: {current_step}, Loss: {loss_history[-1]:.4f}]\")\n        except Exception as e:\n            print(f\"⚠️  元數據保存失敗: {e}\")\n    \n    def load_metadata(self) -> Optional[Dict]:\n        \"\"\"載入訓練元數據 - 用於續訓時的狀態恢復\"\"\"\n        if self.metadata_file.exists():\n            try:\n                with open(self.metadata_file, 'r', encoding='utf-8') as f:\n                    metadata = json.load(f)\n                \n                print(f\"📊 載入元數據成功:\")\n                print(f\"   • 步驟: {metadata['current_step']}\")\n                print(f\"   • 輪數: {metadata['epoch']:.2f}\")\n                print(f\"   • 最新Loss: {metadata['loss_history'][-1]:.4f}\")\n                print(f\"   • 學習率: {metadata['learning_rate']:.2e}\")\n                print(f\"   • 時間: {metadata['timestamp']}\")\n                \n                return metadata\n            except Exception as e:\n                print(f\"❌ 元數據載入失敗: {e}\")\n                return None\n        \n        print(\"📝 未找到元數據文件，將創建新的訓練記錄\")\n        return None\n    \n    def get_latest_checkpoint(self) -> Optional[str]:\n        \"\"\"獲取最新檢查點路徑 - 自動續訓的核心功能\"\"\"\n        try:\n            checkpoint_path = trainer_utils.get_last_checkpoint(str(self.checkpoint_dir))\n            \n            if checkpoint_path:\n                # 驗證檢查點完整性  \n                checkpoint_files = list(Path(checkpoint_path).glob(\"*\"))\n                essential_files = [\"adapter_config.json\", \"adapter_model.safetensors\"]\n                \n                missing_files = []\n                for file in essential_files:\n                    if not (Path(checkpoint_path) / file).exists():\n                        missing_files.append(file)\n                \n                if missing_files:\n                    print(f\"⚠️  檢查點不完整，缺少文件: {missing_files}\")\n                    return None\n                \n                print(f\"🔄 找到完整檢查點: {checkpoint_path}\")\n                print(f\"📂 包含文件: {len(checkpoint_files)} 個\")\n                return checkpoint_path\n            else:\n                print(\"🆕 未找到檢查點，將開始新的訓練\")\n                return None\n                \n        except Exception as e:\n            print(f\"❌ 檢查點檢測失敗: {e}\")\n            return None\n\n# 初始化檢查點管理器\ncheckpoint_manager = KaggleCheckpointManager()\n\nprint(\"\\n🎯 檢查點管理器初始化完成！\")\nprint(\"   • 支援自動續訓\")\nprint(\"   • 完整元數據追蹤\")\nprint(\"   • Kaggle 環境優化\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 第三部分：Arctic-Text2SQL-R1 模型配置\n",
    "\n",
    "### 模型選擇理由\n",
    "根據 README.md 分析，**Arctic-Text2SQL-R1 7B** 是最佳選擇：\n",
    "- 🏆 **SOTA 表現**: BIRD ExecAcc 57%，同級模型最佳\n",
    "- 💰 **資源友好**: 4-bit QLoRA 僅需 ≈5GB VRAM\n",
    "- 🚀 **專業訓練**: Execution-reward RL 訓練，專注正確率\n",
    "- 📜 **開源授權**: Apache-2.0，商用友好"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arctic-Text2SQL-R1 模型配置類\n",
    "class ArcticModelConfig:\n",
    "    \"\"\"\n",
    "    Arctic-Text2SQL-R1 7B 模型配置管理\n",
    "    \n",
    "    基於 README.md 建議，實現 QLoRA 4-bit 量化配置\n",
    "    確保在 Kaggle P100/T4 環境下穩定運行\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # 模型基本信息\n",
    "        self.model_name = \"Snowflake/snowflake-arctic-instruct\"  # Arctic 基座模型\n",
    "        self.model_type = \"arctic-text2sql\"\n",
    "        self.max_length = 2048  # 適合 Text2SQL 任務的序列長度\n",
    "        \n",
    "        # QLoRA 4-bit 量化配置 - 關鍵的記憶體優化\n",
    "        self.quantization_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,                    # 啟用 4-bit 量化\n",
    "            bnb_4bit_quant_type=\"nf4\",           # 使用 NF4 量化類型（推薦）\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16, # 計算精度（Arctic 推薦 bfloat16）\n",
    "            bnb_4bit_use_double_quant=True,       # 雙重量化進一步節省記憶體\n",
    "            bnb_4bit_quant_storage=\"uint8\"       # 存儲格式\n",
    "        )\n",
    "        \n",
    "        # LoRA 配置 - 基於 Arctic 架構特點調整\n",
    "        self.lora_config = LoraConfig(\n",
    "            # 目標模組 - Arctic 使用 MHA 結構\n",
    "            target_modules=[\n",
    "                \"q_proj\",    # Query 投影\n",
    "                \"k_proj\",    # Key 投影  \n",
    "                \"v_proj\",    # Value 投影\n",
    "                \"o_proj\",    # Output 投影\n",
    "                \"gate_proj\", # Arctic MoE gate\n",
    "                \"up_proj\",   # MLP up 投影\n",
    "                \"down_proj\"  # MLP down 投影\n",
    "            ],\n",
    "            r=32,                    # LoRA rank（平衡效果與效率）\n",
    "            lora_alpha=64,          # LoRA scaling（通常是 r 的 2 倍）\n",
    "            lora_dropout=0.05,      # LoRA dropout（防止過擬合）\n",
    "            bias=\"none\",            # 不訓練 bias\n",
    "            task_type=TaskType.CAUSAL_LM,  # 因果語言模型\n",
    "            use_rslora=True,        # 使用 RSLoRA（穩定性改進）\n",
    "            use_dora=False          # 暫不使用 DoRA（節省計算）\n",
    "        )\n",
    "        \n",
    "        print(\"🎯 Arctic 模型配置初始化完成：\")\n",
    "        print(f\"   • 基座模型: {self.model_name}\")\n",
    "        print(f\"   • 量化: 4-bit NF4 + 雙重量化\")\n",
    "        print(f\"   • LoRA: r={self.lora_config.r}, α={self.lora_config.lora_alpha}\")\n",
    "        print(f\"   • 目標模組: {len(self.lora_config.target_modules)} 個\")\n",
    "    \n",
    "    def estimate_memory_usage(self) -> Dict[str, float]:\n",
    "        \"\"\"估算記憶體使用量 - 幫助 Kaggle 用戶規劃資源\"\"\"\n",
    "        # 基於經驗值估算（7B 模型）\n",
    "        base_model_4bit = 4.2  # 4-bit 量化後的基座模型\n",
    "        lora_adapters = 0.3    # LoRA 適配器權重\n",
    "        optimizer_states = 0.6  # AdamW optimizer 狀態\n",
    "        gradient_cache = 0.4   # 梯度緩存\n",
    "        activation_cache = 0.8  # 啟動值緩存\n",
    "        misc_overhead = 0.7    # 其他開銷\n",
    "        \n",
    "        total_estimated = (\n",
    "            base_model_4bit + lora_adapters + \n",
    "            optimizer_states + gradient_cache + \n",
    "            activation_cache + misc_overhead\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"base_model_4bit\": base_model_4bit,\n",
    "            \"lora_adapters\": lora_adapters,\n",
    "            \"optimizer_states\": optimizer_states,\n",
    "            \"gradient_cache\": gradient_cache,\n",
    "            \"activation_cache\": activation_cache,\n",
    "            \"misc_overhead\": misc_overhead,\n",
    "            \"total_estimated\": total_estimated\n",
    "        }\n",
    "    \n",
    "    def print_memory_breakdown(self):\n",
    "        \"\"\"打印詳細的記憶體使用分析\"\"\"\n",
    "        memory_usage = self.estimate_memory_usage()\n",
    "        \n",
    "        print(\"\\n💾 預估記憶體使用量分析（GB）：\")\n",
    "        print(\"=\" * 40)\n",
    "        for component, usage in memory_usage.items():\n",
    "            if component != \"total_estimated\":\n",
    "                percentage = (usage / memory_usage[\"total_estimated\"]) * 100\n",
    "                print(f\"  {component:.<20} {usage:>6.1f} GB ({percentage:4.1f}%)\")\n",
    "        \n",
    "        print(\"-\" * 40)\n",
    "        print(f\"  {'總計':.<20} {memory_usage['total_estimated']:>6.1f} GB (100.0%)\")\n",
    "        \n",
    "        # GPU 兼容性檢查\n",
    "        if torch.cuda.is_available():\n",
    "            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "            print(f\"\\n🖥️  當前 GPU 記憶體: {gpu_memory:.1f} GB\")\n",
    "            \n",
    "            if memory_usage[\"total_estimated\"] <= gpu_memory * 0.9:  # 留 10% 緩衝\n",
    "                print(\"✅ 記憶體充足，可以開始訓練\")\n",
    "            elif memory_usage[\"total_estimated\"] <= gpu_memory:\n",
    "                print(\"⚠️  記憶體緊張，建議監控使用量\")\n",
    "            else:\n",
    "                print(\"❌ 記憶體不足，考慮：\")\n",
    "                print(\"   • 減少 batch size\")\n",
    "                print(\"   • 減少 max_length\")\n",
    "                print(\"   • 使用更小的 LoRA rank\")\n",
    "\n",
    "# 初始化模型配置\n",
    "model_config = ArcticModelConfig()\n",
    "model_config.print_memory_breakdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arctic 模型載入與初始化\n",
    "class ArcticModelLoader:\n",
    "    \"\"\"Arctic-Text2SQL 模型載入器 - 支援斷點續訓\"\"\"\n",
    "    \n",
    "    def __init__(self, config: ArcticModelConfig, checkpoint_manager: KaggleCheckpointManager):\n",
    "        self.config = config\n",
    "        self.checkpoint_manager = checkpoint_manager\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        \n",
    "        print(f\"🚀 Arctic 模型載入器初始化\")\n",
    "    \n",
    "    def load_tokenizer(self):\n",
    "        \"\"\"載入分詞器 - Arctic 專用配置\"\"\"\n",
    "        print(\"\\n📝 載入 Arctic 分詞器...\")\n",
    "        \n",
    "        try:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "                self.config.model_name,\n",
    "                trust_remote_code=True,          # Arctic 需要自定義代碼\n",
    "                use_fast=True,                   # 使用快速分詞器\n",
    "                padding_side=\"left\"              # Text2SQL 任務建議左填充\n",
    "            )\n",
    "            \n",
    "            # 設置特殊 token（如果未設置）\n",
    "            if self.tokenizer.pad_token is None:\n",
    "                self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "                print(\"   • 設置 pad_token = eos_token\")\n",
    "                \n",
    "            if self.tokenizer.chat_template is None:\n",
    "                # 為 Text2SQL 任務設置自定義模板\n",
    "                self.tokenizer.chat_template = (\n",
    "                    \"{% for message in messages %}\"\n",
    "                    \"{% if message['role'] == 'user' %}\"\n",
    "                    \"### 指令:\\\\n根據資料庫結構生成SQL查詢\\\\n\\\\n### 輸入:\\\\n{{ message['content'] }}\\\\n\\\\n### 回應:\\\\n\"\n",
    "                    \"{% elif message['role'] == 'assistant' %}\"\n",
    "                    \"{{ message['content'] }}{% if not loop.last %}\\\\n\\\\n{% endif %}\"\n",
    "                    \"{% endif %}\"\n",
    "                    \"{% endfor %}\"\n",
    "                )\n",
    "                print(\"   • 設置 Text2SQL 聊天模板\")\n",
    "            \n",
    "            print(f\"✅ 分詞器載入成功\")\n",
    "            print(f\"   • 詞彙表大小: {len(self.tokenizer):,}\")\n",
    "            print(f\"   • 特殊 token: pad={self.tokenizer.pad_token}, eos={self.tokenizer.eos_token}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ 分詞器載入失敗: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def load_base_model(self):\n",
    "        \"\"\"載入基座模型 - 支援 4-bit 量化\"\"\"\n",
    "        print(\"\\n🤖 載入 Arctic 基座模型（4-bit 量化）...\")\n",
    "        \n",
    "        try:\n",
    "            # 清理 GPU 記憶體\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "                initial_memory = torch.cuda.memory_allocated() / 1024**3\n",
    "                print(f\"   • 初始 GPU 記憶體: {initial_memory:.2f} GB\")\n",
    "            \n",
    "            # 載入量化模型\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                self.config.model_name,\n",
    "                quantization_config=self.config.quantization_config,\n",
    "                device_map=\"auto\",               # 自動設備映射\n",
    "                trust_remote_code=True,          # Arctic 需要自定義代碼\n",
    "                torch_dtype=torch.bfloat16,      # Arctic 推薦精度\n",
    "                attn_implementation=\"flash_attention_2\",  # 使用 Flash Attention（如果可用）\n",
    "                low_cpu_mem_usage=True,          # 降低 CPU 記憶體使用\n",
    "                cache_dir=\"/kaggle/working/model_cache\"  # Kaggle 緩存目錄\n",
    "            )\n",
    "            \n",
    "            # 檢查記憶體使用\n",
    "            if torch.cuda.is_available():\n",
    "                after_memory = torch.cuda.memory_allocated() / 1024**3\n",
    "                model_memory = after_memory - initial_memory\n",
    "                print(f\"   • 模型記憶體使用: {model_memory:.2f} GB\")\n",
    "                print(f\"   • 總計 GPU 記憶體: {after_memory:.2f} GB\")\n",
    "            \n",
    "            # 啟用梯度檢查點（節省記憶體）\n",
    "            self.model.gradient_checkpointing_enable()\n",
    "            print(\"   • 啟用梯度檢查點\")\n",
    "            \n",
    "            print(f\"✅ 基座模型載入成功\")\n",
    "            print(f\"   • 參數量: {sum(p.numel() for p in self.model.parameters()):,}\")\n",
    "            print(f\"   • 可訓練參數: {sum(p.numel() for p in self.model.parameters() if p.requires_grad):,}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ 基座模型載入失敗: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def apply_lora(self, resume_from_checkpoint: Optional[str] = None):\n",
    "        \"\"\"應用 LoRA 適配器 - 支援續訓\"\"\"\n",
    "        if resume_from_checkpoint:\n",
    "            print(f\"\\n🔄 從檢查點載入 PEFT 模型: {resume_from_checkpoint}\")\n",
    "            try:\n",
    "                self.model = PeftModel.from_pretrained(\n",
    "                    self.model,\n",
    "                    resume_from_checkpoint,\n",
    "                    is_trainable=True  # 重要：確保可以繼續訓練\n",
    "                )\n",
    "                print(\"✅ PEFT 模型續訓載入成功\")\n",
    "            except Exception as e:\n",
    "                print(f\"❌ PEFT 模型載入失敗: {e}\")\n",
    "                print(\"   將創建新的 LoRA 適配器\")\n",
    "                self.model = get_peft_model(self.model, self.config.lora_config)\n",
    "        else:\n",
    "            print(\"\\n🆕 創建新的 LoRA 適配器...\")\n",
    "            self.model = get_peft_model(self.model, self.config.lora_config)\n",
    "            print(\"✅ LoRA 適配器創建成功\")\n",
    "        \n",
    "        # 打印可訓練參數統計\n",
    "        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
    "        total_params = sum(p.numel() for p in self.model.parameters())\n",
    "        \n",
    "        print(f\"\\n📊 LoRA 參數統計:\")\n",
    "        print(f\"   • 可訓練參數: {trainable_params:,} ({100*trainable_params/total_params:.2f}%)\")\n",
    "        print(f\"   • 總參數: {total_params:,}\")\n",
    "        print(f\"   • 記憶體效率: {100*(1-trainable_params/total_params):.1f}% 節省\")\n",
    "    \n",
    "    def initialize_model(self, resume_from_checkpoint: Optional[str] = None):\n",
    "        \"\"\"完整模型初始化流程\"\"\"\n",
    "        print(\"🎯 開始 Arctic-Text2SQL 模型初始化流程\\n\")\n",
    "        \n",
    "        # 步驟 1: 載入分詞器\n",
    "        self.load_tokenizer()\n",
    "        \n",
    "        # 步驟 2: 載入基座模型\n",
    "        self.load_base_model()\n",
    "        \n",
    "        # 步驟 3: 應用 LoRA\n",
    "        self.apply_lora(resume_from_checkpoint)\n",
    "        \n",
    "        print(\"\\n🎉 Arctic 模型初始化完成！準備開始訓練\")\n",
    "        \n",
    "        return self.model, self.tokenizer\n",
    "\n",
    "# 檢查是否有可用的檢查點\n",
    "checkpoint_path = checkpoint_manager.get_latest_checkpoint()\n",
    "metadata = checkpoint_manager.load_metadata()\n",
    "\n",
    "# 初始化模型載入器\n",
    "model_loader = ArcticModelLoader(model_config, checkpoint_manager)\n",
    "\n",
    "print(\"🔧 模型載入器準備就緒\")\n",
    "if checkpoint_path:\n",
    "    print(f\"   將從檢查點續訓: {checkpoint_path}\")\n",
    "else:\n",
    "    print(\"   將開始新的訓練\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📊 第四部分：SynSQL-2.5M 資料處理\n",
    "\n",
    "### 資料集選擇理由\n",
    "根據 README.md 建議，我們使用 **SynSQL-2.5M** 資料集：\n",
    "- 🎯 **高品質**: 250萬個高質量合成樣本\n",
    "- 🌐 **廣覆蓋**: 覆蓋1.6萬個資料庫結構\n",
    "- 🔄 **可擴展**: 支持自由取樣，適合不同訓練規模\n",
    "- 📈 **SOTA基礎**: 現代Text2SQL模型的標準訓練資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SynSQL-2.5M 資料處理器\n",
    "class SynSQLDataProcessor:\n",
    "    \"\"\"處理 SynSQL-2.5M 資料集的專用類別\n",
    "    \n",
    "    功能特色：\n",
    "    1. 智能取樣 - 根據 Kaggle 資源限制調整資料量\n",
    "    2. 格式標準化 - 統一 Text2SQL 格式\n",
    "    3. 質量過濾 - 移除低品質樣本\n",
    "    4. Kaggle 最佳化 - 考慮訓練時間限制\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, tokenizer: AutoTokenizer, max_samples: int = 50000):\n",
    "        \"\"\"初始化資料處理器\n",
    "        \n",
    "        Args:\n",
    "            tokenizer: Arctic 分詞器\n",
    "            max_samples: 最大樣本數（Kaggle 建議 50K 以內）\n",
    "        \"\"\"\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_samples = max_samples\n",
    "        self.max_length = 2048  # Arctic 建議長度\n",
    "        \n",
    "        # Text2SQL 提示模板\n",
    "        self.prompt_template = (\n",
    "            \"### 指令\\n\"\n",
    "            \"根據給定的資料庫結構和自然語言問題，生成對應的 SQL 查詢。\\n\\n\"\n",
    "            \"### 資料庫結構\\n\"\n",
    "            \"{schema}\\n\\n\"\n",
    "            \"### 問題\\n\"\n",
    "            \"{question}\\n\\n\"\n",
    "            \"### SQL 查詢\\n\"\n",
    "            \"{sql}\"\n",
    "        )\n",
    "        \n",
    "        print(f\"📊 SynSQL 資料處理器初始化\")\n",
    "        print(f\"   • 最大樣本數: {max_samples:,}\")\n",
    "        print(f\"   • 序列長度: {self.max_length}\")\n",
    "    \n",
    "    def load_synsql_dataset(self, subset_size: Optional[int] = None) -> Dataset:\n",
    "        \"\"\"載入 SynSQL-2.5M 資料集\n",
    "        \n",
    "        Args:\n",
    "            subset_size: 子集大小，None 表示使用 max_samples\n",
    "        \"\"\"\n",
    "        print(\"\\n🔄 載入 SynSQL-2.5M 資料集...\")\n",
    "        \n",
    "        try:\n",
    "            # 實際大小\n",
    "            actual_size = subset_size or self.max_samples\n",
    "            \n",
    "            # 載入資料集（取樣以節省時間）\n",
    "            dataset = load_dataset(\n",
    "                \"seeklhy/SynSQL-2.5M\",\n",
    "                split=f\"train[:{actual_size}]\",  # 只取前 N 個樣本\n",
    "                cache_dir=\"/kaggle/working/dataset_cache\"  # Kaggle 緩存目錄\n",
    "            )\n",
    "            \n",
    "            print(f\"✅ 資料集載入成功\")\n",
    "            print(f\"   • 樣本數: {len(dataset):,}\")\n",
    "            print(f\"   • 資料欄位: {list(dataset.features.keys())}\")\n",
    "            \n",
    "            # 顯示資料集統計\n",
    "            self._print_dataset_stats(dataset)\n",
    "            \n",
    "            return dataset\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ 資料集載入失敗: {e}\")\n",
    "            print(\"🔄 使用備用資料集創建方法...\")\n",
    "            return self._create_fallback_dataset()\n",
    "    \n",
    "    def _create_fallback_dataset(self) -> Dataset:\n",
    "        \"\"\"創建備用資料集（如果 SynSQL 載入失敗）\"\"\"\n",
    "        print(\"📝 創建備用 Text2SQL 資料集...\")\n",
    "        \n",
    "        # 備用樣本數據\n",
    "        fallback_data = [\n",
    "            {\n",
    "                \"schema\": \"CREATE TABLE users (id INT, name VARCHAR(50), email VARCHAR(100));\",\n",
    "                \"question\": \"查找所有用戶的姓名和郵箱\",\n",
    "                \"sql\": \"SELECT name, email FROM users;\"\n",
    "            },\n",
    "            {\n",
    "                \"schema\": \"CREATE TABLE products (id INT, name VARCHAR(100), price DECIMAL(10,2));\",\n",
    "                \"question\": \"找出價格超過100的產品\",\n",
    "                \"sql\": \"SELECT * FROM products WHERE price > 100;\"\n",
    "            },\n",
    "            {\n",
    "                \"schema\": \"CREATE TABLE orders (id INT, user_id INT, total DECIMAL(10,2));\",\n",
    "                \"question\": \"計算訂單總金額\",\n",
    "                \"sql\": \"SELECT SUM(total) FROM orders;\"\n",
    "            }\n",
    "        ] * (self.max_samples // 3)  # 重複以達到所需樣本數\n",
    "        \n",
    "        return Dataset.from_list(fallback_data[:self.max_samples])\n",
    "    \n",
    "    def _print_dataset_stats(self, dataset: Dataset):\n",
    "        \"\"\"打印資料集統計信息\"\"\"\n",
    "        print(f\"\\n📈 資料集統計分析:\")\n",
    "        \n",
    "        # 檢查必要欄位\n",
    "        required_fields = ['schema', 'question', 'sql']\n",
    "        available_fields = list(dataset.features.keys())\n",
    "        \n",
    "        print(f\"   • 必要欄位檢查:\")\n",
    "        for field in required_fields:\n",
    "            if field in available_fields:\n",
    "                print(f\"     ✅ {field}\")\n",
    "            else:\n",
    "                # 尋找相似欄位\n",
    "                similar_fields = [f for f in available_fields if field.lower() in f.lower()]\n",
    "                if similar_fields:\n",
    "                    print(f\"     ⚠️  {field} (找到相似: {similar_fields})\")\n",
    "                else:\n",
    "                    print(f\"     ❌ {field} (缺失)\")\n",
    "        \n",
    "        # 樣本長度分析\n",
    "        if len(dataset) > 0:\n",
    "            sample = dataset[0]\n",
    "            if 'question' in sample:\n",
    "                avg_question_len = np.mean([len(str(item.get('question', ''))) for item in dataset[:1000]])\n",
    "                print(f\"   • 平均問題長度: {avg_question_len:.1f} 字符\")\n",
    "            \n",
    "            if 'sql' in sample:\n",
    "                avg_sql_len = np.mean([len(str(item.get('sql', ''))) for item in dataset[:1000]])\n",
    "                print(f\"   • 平均SQL長度: {avg_sql_len:.1f} 字符\")\n",
    "    \n",
    "    def preprocess_dataset(self, dataset: Dataset, train_split: float = 0.9) -> Tuple[Dataset, Dataset]:\n",
    "        \"\"\"預處理資料集並分割為訓練/驗證集\n",
    "        \n",
    "        Args:\n",
    "            dataset: 原始資料集\n",
    "            train_split: 訓練集比例\n",
    "        \"\"\"\n",
    "        print(f\"\\n🔧 開始資料預處理...\")\n",
    "        \n",
    "        # 1. 資料清理和格式化\n",
    "        print(\"   • 資料清理中...\")\n",
    "        cleaned_dataset = dataset.map(\n",
    "            self._clean_and_format,\n",
    "            remove_columns=dataset.column_names,  # 移除原始欄位\n",
    "            desc=\"清理資料\"\n",
    "        )\n",
    "        \n",
    "        # 2. 過濾無效樣本\n",
    "        print(\"   • 過濾無效樣本...\")\n",
    "        valid_dataset = cleaned_dataset.filter(\n",
    "            lambda x: len(x['formatted_text']) > 10 and len(x['formatted_text']) < self.max_length * 4\n",
    "        )\n",
    "        \n",
    "        print(f\"   • 過濾後樣本數: {len(valid_dataset):,} (原始: {len(dataset):,})\")\n",
    "        \n",
    "        # 3. 分詞化\n",
    "        print(\"   • 分詞化處理...\")\n",
    "        tokenized_dataset = valid_dataset.map(\n",
    "            self._tokenize_function,\n",
    "            batched=True,\n",
    "            batch_size=1000,\n",
    "            desc=\"分詞化\"\n",
    "        )\n",
    "        \n",
    "        # 4. 分割訓練/驗證集\n",
    "        print(f\"   • 分割資料集 (訓練:{train_split:.0%}, 驗證:{1-train_split:.0%})...\")\n",
    "        split_dataset = tokenized_dataset.train_test_split(\n",
    "            test_size=1-train_split,\n",
    "            shuffle=True,\n",
    "            seed=42\n",
    "        )\n",
    "        \n",
    "        train_dataset = split_dataset['train']\n",
    "        eval_dataset = split_dataset['test']\n",
    "        \n",
    "        print(f\"✅ 資料預處理完成\")\n",
    "        print(f\"   • 訓練集: {len(train_dataset):,} 樣本\")\n",
    "        print(f\"   • 驗證集: {len(eval_dataset):,} 樣本\")\n",
    "        \n",
    "        return train_dataset, eval_dataset\n",
    "    \n",
    "    def _clean_and_format(self, example: Dict) -> Dict:\n",
    "        \"\"\"清理並格式化單個樣本\"\"\"\n",
    "        # 提取欄位（處理不同的欄位名稱）\n",
    "        schema = example.get('schema', example.get('db_schema', ''))\n",
    "        question = example.get('question', example.get('nl_question', example.get('query', '')))\n",
    "        sql = example.get('sql', example.get('sql_query', ''))\n",
    "        \n",
    "        # 清理文本\n",
    "        schema = str(schema).strip()\n",
    "        question = str(question).strip()\n",
    "        sql = str(sql).strip()\n",
    "        \n",
    "        # 格式化為統一的提示格式\n",
    "        formatted_text = self.prompt_template.format(\n",
    "            schema=schema,\n",
    "            question=question,\n",
    "            sql=sql\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'formatted_text': formatted_text,\n",
    "            'schema': schema,\n",
    "            'question': question,\n",
    "            'sql': sql\n",
    "        }\n",
    "    \n",
    "    def _tokenize_function(self, examples: Dict) -> Dict:\n",
    "        \"\"\"批量分詞化函數\"\"\"\n",
    "        # 分詞化\n",
    "        tokenized = self.tokenizer(\n",
    "            examples['formatted_text'],\n",
    "            truncation=True,\n",
    "            padding=False,  # 動態填充更有效\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=None  # 返回 Python list\n",
    "        )\n",
    "        \n",
    "        # 對於因果語言模型，labels 就是 input_ids\n",
    "        tokenized['labels'] = tokenized['input_ids'].copy()\n",
    "        \n",
    "        return tokenized\n",
    "    \n",
    "    def create_data_collator(self):\n",
    "        \"\"\"創建資料整理器 - 用於動態填充\"\"\"\n",
    "        from transformers import DataCollatorForLanguageModeling\n",
    "        \n",
    "        return DataCollatorForLanguageModeling(\n",
    "            tokenizer=self.tokenizer,\n",
    "            mlm=False,  # 不使用 MLM（因為是因果語言模型）\n",
    "            pad_to_multiple_of=8  # 填充到8的倍數（提高效率）\n",
    "        )\n",
    "\n",
    "# 資料處理器將在模型載入後初始化\n",
    "print(\"📊 SynSQL 資料處理器類別定義完成\")\n",
    "print(\"   等待模型載入後進行資料處理...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🚀 第五部分：QLoRA 微調訓練實現\n",
    "\n",
    "### 核心訓練架構\n",
    "基於 README.md 最佳實踐，實現完整的 QLoRA 訓練流程：\n",
    "- **HuggingFace Trainer**: 成熟的斷點續訓機制\n",
    "- **自定義回調**: 實時監控訓練狀態和記憶體使用\n",
    "- **動態調整**: 根據 GPU 記憶體自動調整批次大小\n",
    "- **多指標評估**: Loss、學習率、GPU 使用率等全方位監控"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QLoRA 訓練器 - 完整的訓練流程實現\n",
    "class ArcticQLoRATrainer:\n",
    "    \"\"\"\n",
    "    Arctic-Text2SQL QLoRA 微調訓練器\n",
    "    \n",
    "    核心功能：\n",
    "    1. 自動記憶體管理和批次大小調整\n",
    "    2. 完整的斷點續訓支援\n",
    "    3. 實時訓練監控和日誌記錄\n",
    "    4. Kaggle 環境優化配置\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 model,\n",
    "                 tokenizer, \n",
    "                 checkpoint_manager: KaggleCheckpointManager,\n",
    "                 config: ArcticModelConfig):\n",
    "        \"\"\"初始化訓練器\n",
    "        \n",
    "        Args:\n",
    "            model: 已配置 LoRA 的 Arctic 模型\n",
    "            tokenizer: Arctic 分詞器\n",
    "            checkpoint_manager: 檢查點管理器\n",
    "            config: 模型配置\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.checkpoint_manager = checkpoint_manager\n",
    "        self.config = config\n",
    "        \n",
    "        # 訓練狀態追蹤\n",
    "        self.training_history = {\n",
    "            'train_loss': [],\n",
    "            'eval_loss': [],\n",
    "            'learning_rate': [],\n",
    "            'gpu_memory': [],\n",
    "            'timestamps': []\n",
    "        }\n",
    "        \n",
    "        print(\"🎯 Arctic QLoRA 訓練器初始化完成\")\n",
    "    \n",
    "    def create_training_arguments(self, \n",
    "                                batch_size: int = 1,\n",
    "                                gradient_accumulation_steps: int = 8,\n",
    "                                num_epochs: int = 3,\n",
    "                                learning_rate: float = 2e-5) -> TrainingArguments:\n",
    "        \"\"\"創建訓練參數配置 - 針對 Kaggle 環境優化\"\"\"\n",
    "        \n",
    "        return TrainingArguments(\n",
    "            # 基本設置\n",
    "            output_dir=str(self.checkpoint_manager.checkpoint_dir),\n",
    "            overwrite_output_dir=False,  # 保護現有檢查點\n",
    "            run_name=f\"arctic-text2sql-{datetime.now().strftime('%Y%m%d_%H%M')}\",\n",
    "            \n",
    "            # 訓練配置\n",
    "            num_train_epochs=num_epochs,\n",
    "            per_device_train_batch_size=batch_size,\n",
    "            per_device_eval_batch_size=batch_size,\n",
    "            gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "            \n",
    "            # 優化器設置\n",
    "            learning_rate=learning_rate,\n",
    "            weight_decay=0.01,\n",
    "            adam_beta1=0.9,\n",
    "            adam_beta2=0.999,\n",
    "            adam_epsilon=1e-8,\n",
    "            max_grad_norm=1.0,\n",
    "            \n",
    "            # 學習率調度\n",
    "            lr_scheduler_type=\"cosine\",\n",
    "            warmup_ratio=0.03,  # 3% warmup\n",
    "            \n",
    "            # 檢查點設置（頻繁保存適應 Kaggle）\n",
    "            save_strategy=\"steps\",\n",
    "            save_steps=50,  # 每50步保存一次\n",
    "            save_total_limit=3,  # 保留最近3個檢查點\n",
    "            \n",
    "            # 評估設置\n",
    "            evaluation_strategy=\"steps\",\n",
    "            eval_steps=50,\n",
    "            eval_accumulation_steps=4,  # 減少記憶體使用\n",
    "            \n",
    "            # 早停和模型選擇\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"eval_loss\",\n",
    "            greater_is_better=False,\n",
    "            \n",
    "            # 日誌設置\n",
    "            logging_strategy=\"steps\",\n",
    "            logging_steps=10,\n",
    "            logging_dir=str(self.checkpoint_manager.base_dir / \"logs\"),\n",
    "            report_to=[\"tensorboard\"],\n",
    "            \n",
    "            # 性能優化\n",
    "            fp16=True,  # 混合精度訓練\n",
    "            dataloader_drop_last=True,\n",
    "            remove_unused_columns=False,\n",
    "            group_by_length=True,  # 按長度分組提高效率\n",
    "            \n",
    "            # Kaggle 特定優化\n",
    "            dataloader_num_workers=2,  # 適合 Kaggle CPU 核心數\n",
    "            ignore_data_skip=True,  # 加速續訓\n",
    "            save_safetensors=True,  # 使用更安全的格式\n",
    "            \n",
    "            # 記憶體優化\n",
    "            gradient_checkpointing=True,\n",
    "            optim=\"adamw_torch_fused\",  # 更快的優化器（如果可用）\n",
    "        )\n",
    "    \n",
    "    def create_custom_callbacks(self):\n",
    "        \"\"\"創建自定義回調函數 - 增強監控和自動化功能\"\"\"\n",
    "        \n",
    "        class ArcticTrainingCallback(TrainerCallback):\n",
    "            \"\"\"Arctic 專用訓練回調 - 集成檢查點管理和記憶體監控\"\"\"\n",
    "            \n",
    "            def __init__(self, checkpoint_manager, training_history):\n",
    "                self.checkpoint_manager = checkpoint_manager\n",
    "                self.training_history = training_history\n",
    "                self.best_eval_loss = float('inf')\n",
    "                \n",
    "            def on_log(self, args, state, control, model=None, logs=None, **kwargs):\n",
    "                \"\"\"日誌記錄時的回調 - 記錄詳細訓練狀態\"\"\"\n",
    "                if logs:\n",
    "                    current_time = datetime.now().isoformat()\n",
    "                    \n",
    "                    # 記錄訓練指標\n",
    "                    if \"train_loss\" in logs:\n",
    "                        self.training_history['train_loss'].append(logs[\"train_loss\"])\n",
    "                    if \"eval_loss\" in logs:\n",
    "                        self.training_history['eval_loss'].append(logs[\"eval_loss\"])\n",
    "                    if \"learning_rate\" in logs:\n",
    "                        self.training_history['learning_rate'].append(logs[\"learning_rate\"])\n",
    "                    \n",
    "                    # 記錄 GPU 記憶體使用\n",
    "                    if torch.cuda.is_available():\n",
    "                        gpu_memory = torch.cuda.memory_allocated() / 1024**3\n",
    "                        self.training_history['gpu_memory'].append(gpu_memory)\n",
    "                    \n",
    "                    self.training_history['timestamps'].append(current_time)\n",
    "                    \n",
    "                    # 每50步保存詳細元數據\n",
    "                    if state.global_step % 50 == 0 and len(self.training_history['train_loss']) > 0:\n",
    "                        self.checkpoint_manager.save_metadata(\n",
    "                            current_step=state.global_step,\n",
    "                            epoch=state.epoch,\n",
    "                            loss_history=self.training_history['train_loss'],\n",
    "                            learning_rate=logs.get(\"learning_rate\", 0)\n",
    "                        )\n",
    "            \n",
    "            def on_evaluate(self, args, state, control, model=None, logs=None, **kwargs):\n",
    "                \"\"\"評估完成時的回調 - 檢查是否需要早停\"\"\"\n",
    "                if logs and \"eval_loss\" in logs:\n",
    "                    current_eval_loss = logs[\"eval_loss\"]\n",
    "                    \n",
    "                    # 更新最佳評估結果\n",
    "                    if current_eval_loss < self.best_eval_loss:\n",
    "                        self.best_eval_loss = current_eval_loss\n",
    "                        print(f\"🎉 新的最佳評估結果: {current_eval_loss:.4f}\")\n",
    "                    \n",
    "                    # 記憶體使用報告\n",
    "                    if torch.cuda.is_available():\n",
    "                        memory_used = torch.cuda.memory_allocated() / 1024**3\n",
    "                        memory_total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "                        print(f\"💾 GPU 記憶體: {memory_used:.1f}/{memory_total:.1f} GB ({memory_used/memory_total*100:.1f}%)\")\n",
    "            \n",
    "            def on_save(self, args, state, control, model=None, **kwargs):\n",
    "                \"\"\"檢查點保存時的回調\"\"\"\n",
    "                print(f\"💾 檢查點已保存 - Step: {state.global_step}, Epoch: {state.epoch:.2f}\")\n",
    "        \n",
    "        # 回調函數列表\n",
    "        callbacks = [\n",
    "            ArcticTrainingCallback(self.checkpoint_manager, self.training_history),\n",
    "            EarlyStoppingCallback(\n",
    "                early_stopping_patience=3,  # 3次評估無改善則早停\n",
    "                early_stopping_threshold=0.01  # 改善閾值\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        return callbacks\n",
    "    \n",
    "    def setup_data_collator(self):\n",
    "        \"\"\"設置資料整理器 - 動態填充優化\"\"\"\n",
    "        from transformers import DataCollatorForLanguageModeling\n",
    "        \n",
    "        return DataCollatorForLanguageModeling(\n",
    "            tokenizer=self.tokenizer,\n",
    "            mlm=False,  # 因果語言模型\n",
    "            pad_to_multiple_of=8,  # 對齊到8的倍數提高效率\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "    \n",
    "    def estimate_optimal_batch_size(self) -> Tuple[int, int]:\n",
    "        \"\"\"智能估算最佳批次大小 - 根據 GPU 記憶體動態調整\"\"\"\n",
    "        if not torch.cuda.is_available():\n",
    "            return 1, 8  # CPU 環境的保守設置\n",
    "        \n",
    "        gpu_memory_gb = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "        \n",
    "        # 基於 GPU 記憶體的啟發式規則\n",
    "        if gpu_memory_gb >= 24:  # RTX 4090, A100 等\n",
    "            return 2, 4\n",
    "        elif gpu_memory_gb >= 16:  # P100, T4, RTX 3080 等\n",
    "            return 1, 8\n",
    "        elif gpu_memory_gb >= 12:  # GTX 1080 Ti 等\n",
    "            return 1, 12\n",
    "        else:  # 更小的 GPU\n",
    "            return 1, 16\n",
    "    \n",
    "    def train_model(self, \n",
    "                   train_dataset: Dataset, \n",
    "                   eval_dataset: Dataset,\n",
    "                   resume_from_checkpoint: Optional[str] = None,\n",
    "                   custom_training_args: Optional[Dict] = None) -> Any:\n",
    "        \"\"\"執行完整的 QLoRA 微調訓練\"\"\"\n",
    "        \n",
    "        print(\"🚀 開始 Arctic-Text2SQL QLoRA 微調訓練\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # 1. 智能批次大小設置\n",
    "        optimal_batch_size, optimal_grad_accum = self.estimate_optimal_batch_size()\n",
    "        print(f\"📊 優化配置:\")\n",
    "        print(f\"   • 批次大小: {optimal_batch_size}\")\n",
    "        print(f\"   • 梯度累積步數: {optimal_grad_accum}\")\n",
    "        print(f\"   • 有效批次大小: {optimal_batch_size * optimal_grad_accum}\")\n",
    "        \n",
    "        # 2. 創建訓練參數\n",
    "        training_args_kwargs = {\n",
    "            'batch_size': optimal_batch_size,\n",
    "            'gradient_accumulation_steps': optimal_grad_accum,\n",
    "        }\n",
    "        if custom_training_args:\n",
    "            training_args_kwargs.update(custom_training_args)\n",
    "        \n",
    "        training_args = self.create_training_arguments(**training_args_kwargs)\n",
    "        \n",
    "        # 3. 設置回調和資料整理器\n",
    "        callbacks = self.create_custom_callbacks()\n",
    "        data_collator = self.setup_data_collator()\n",
    "        \n",
    "        # 4. 創建 Trainer\n",
    "        trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=eval_dataset,\n",
    "            tokenizer=self.tokenizer,\n",
    "            data_collator=data_collator,\n",
    "            callbacks=callbacks,\n",
    "        )\n",
    "        \n",
    "        # 5. 開始訓練（支援續訓）\n",
    "        print(f\"\\n🎯 訓練開始\")\n",
    "        if resume_from_checkpoint:\n",
    "            print(f\"   🔄 從檢查點續訓: {resume_from_checkpoint}\")\n",
    "            train_result = trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n",
    "        else:\n",
    "            print(\"   🆕 開始新的訓練\")\n",
    "            train_result = trainer.train()\n",
    "        \n",
    "        # 6. 保存最終模型\n",
    "        final_model_dir = self.checkpoint_manager.base_dir / \"final_model\"\n",
    "        trainer.save_model(str(final_model_dir))\n",
    "        \n",
    "        # 7. 訓練總結\n",
    "        print(f\"\\n🎉 訓練完成！\")\n",
    "        print(f\"   • 最終模型保存至: {final_model_dir}\")\n",
    "        print(f\"   • 訓練步數: {train_result.global_step}\")\n",
    "        print(f\"   • 最終損失: {train_result.training_loss:.4f}\")\n",
    "        \n",
    "        return train_result, trainer\n",
    "    \n",
    "    def plot_training_history(self):\n",
    "        \"\"\"可視化訓練歷史 - 生成詳細的訓練報告\"\"\"\n",
    "        if not self.training_history['train_loss']:\n",
    "            print(\"⚠️  沒有訓練歷史數據可供可視化\")\n",
    "            return\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        fig.suptitle('Arctic-Text2SQL QLoRA 訓練監控', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        # 損失曲線\n",
    "        axes[0, 0].plot(self.training_history['train_loss'], label='訓練損失', color='blue', alpha=0.7)\n",
    "        if self.training_history['eval_loss']:\n",
    "            axes[0, 0].plot(self.training_history['eval_loss'], label='驗證損失', color='red', alpha=0.7)\n",
    "        axes[0, 0].set_title('損失函數變化')\n",
    "        axes[0, 0].set_xlabel('步數')\n",
    "        axes[0, 0].set_ylabel('損失值')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 學習率曲線\n",
    "        if self.training_history['learning_rate']:\n",
    "            axes[0, 1].plot(self.training_history['learning_rate'], color='green', alpha=0.7)\n",
    "            axes[0, 1].set_title('學習率調度')\n",
    "            axes[0, 1].set_xlabel('步數')\n",
    "            axes[0, 1].set_ylabel('學習率')\n",
    "            axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # GPU 記憶體使用\n",
    "        if self.training_history['gpu_memory']:\n",
    "            axes[1, 0].plot(self.training_history['gpu_memory'], color='purple', alpha=0.7)\n",
    "            axes[1, 0].set_title('GPU 記憶體使用')\n",
    "            axes[1, 0].set_xlabel('步數')\n",
    "            axes[1, 0].set_ylabel('記憶體 (GB)')\n",
    "            axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 訓練進度統計\n",
    "        axes[1, 1].axis('off')\n",
    "        stats_text = f\"\"\"\n",
    "訓練統計摘要:\n",
    "        \n",
    "• 總訓練步數: {len(self.training_history['train_loss'])}\n",
    "• 最終訓練損失: {self.training_history['train_loss'][-1]:.4f}\n",
    "• 最低驗證損失: {min(self.training_history['eval_loss']) if self.training_history['eval_loss'] else 'N/A'}\n",
    "• 平均 GPU 記憶體: {np.mean(self.training_history['gpu_memory']):.1f} GB\n",
    "• 訓練持續時間: {len(self.training_history['timestamps'])} 個記錄點\n",
    "        \"\"\"\n",
    "        axes[1, 1].text(0.1, 0.9, stats_text, transform=axes[1, 1].transAxes, \n",
    "                        fontsize=12, verticalalignment='top', fontfamily='monospace')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # 保存圖表\n",
    "        plot_path = self.checkpoint_manager.base_dir / \"training_history.png\"\n",
    "        plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"📊 訓練歷史圖表已保存: {plot_path}\")\n",
    "\n",
    "print(\"🔧 Arctic QLoRA 訓練器類別定義完成\")\n",
    "print(\"   準備開始模型初始化和資料載入...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 第六部分：完整訓練流程執行\n",
    "\n",
    "### 執行步驟\n",
    "1. **模型初始化** - 載入 Arctic 模型並配置 LoRA\n",
    "2. **資料載入** - 處理 SynSQL 資料集並預處理\n",
    "3. **訓練執行** - 開始 QLoRA 微調訓練\n",
    "4. **結果評估** - 分析訓練結果和模型性能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 步驟 1: 初始化 Arctic 模型和分詞器\n",
    "print(\"🚀 第一步：初始化 Arctic-Text2SQL 模型\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "try:\n",
    "    # 初始化模型（支援斷點續訓）\n",
    "    model, tokenizer = model_loader.initialize_model(checkpoint_path)\n",
    "    \n",
    "    print(\"\\n✅ 模型初始化成功！\")\n",
    "    print(f\"   • 模型類型: {type(model).__name__}\")\n",
    "    print(f\"   • 分詞器類型: {type(tokenizer).__name__}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ 模型初始化失敗: {e}\")\n",
    "    print(\"請檢查:\")\n",
    "    print(\"   • GPU 記憶體是否充足\")\n",
    "    print(\"   • 網路連接是否正常\")\n",
    "    print(\"   • Hugging Face 是否可訪問\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 步驟 2: 載入和預處理 SynSQL 資料集\n",
    "print(\"📊 第二步：載入和預處理 SynSQL-2.5M 資料集\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "try:\n",
    "    # 初始化資料處理器\n",
    "    data_processor = SynSQLDataProcessor(\n",
    "        tokenizer=tokenizer,\n",
    "        max_samples=10000  # Kaggle 環境建議樣本數，可根據需要調整\n",
    "    )\n",
    "    \n",
    "    # 載入原始資料集\n",
    "    print(\"\\n🔄 載入原始資料集...\")\n",
    "    raw_dataset = data_processor.load_synsql_dataset()\n",
    "    \n",
    "    # 預處理和分割資料集\n",
    "    print(\"\\n🔧 預處理資料集...\")\n",
    "    train_dataset, eval_dataset = data_processor.preprocess_dataset(\n",
    "        raw_dataset, \n",
    "        train_split=0.9  # 90% 用於訓練，10% 用於驗證\n",
    "    )\n",
    "    \n",
    "    # 顯示樣本示例\n",
    "    print(\"\\n📋 資料集樣本預覽:\")\n",
    "    print(\"訓練集樣本:\")\n",
    "    sample_idx = 0\n",
    "    if len(train_dataset) > 0:\n",
    "        sample = train_dataset[sample_idx]\n",
    "        print(f\"  • Input IDs 長度: {len(sample['input_ids'])}\")\n",
    "        print(f\"  • Labels 長度: {len(sample['labels'])}\")\n",
    "        \n",
    "        # 解碼一個樣本看看格式\n",
    "        decoded_text = tokenizer.decode(sample['input_ids'][:200], skip_special_tokens=True)\n",
    "        print(f\"  • 樣本內容預覽: {decoded_text[:200]}...\")\n",
    "    \n",
    "    print(f\"\\n✅ 資料集準備完成！\")\n",
    "    print(f\"   • 訓練集樣本數: {len(train_dataset):,}\")\n",
    "    print(f\"   • 驗證集樣本數: {len(eval_dataset):,}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ 資料集載入失敗: {e}\")\n",
    "    print(\"   嘗試使用備用資料集...\")\n",
    "    \n",
    "    # 使用備用資料集\n",
    "    data_processor = SynSQLDataProcessor(tokenizer=tokenizer, max_samples=1000)\n",
    "    raw_dataset = data_processor._create_fallback_dataset()\n",
    "    train_dataset, eval_dataset = data_processor.preprocess_dataset(raw_dataset)\n",
    "    \n",
    "    print(f\"✅ 備用資料集載入成功\")\n",
    "    print(f\"   • 訓練集樣本數: {len(train_dataset):,}\")\n",
    "    print(f\"   • 驗證集樣本數: {len(eval_dataset):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 步驟 3: 初始化訓練器並開始 QLoRA 微調\n",
    "print(\"🎯 第三步：開始 QLoRA 微調訓練\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 初始化 QLoRA 訓練器\n",
    "trainer = ArcticQLoRATrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    checkpoint_manager=checkpoint_manager,\n",
    "    config=model_config\n",
    ")\n",
    "\n",
    "# 配置訓練參數（可根據需要調整）\n",
    "custom_training_config = {\n",
    "    'num_epochs': 2,  # Kaggle 時間限制，使用較少輪次\n",
    "    'learning_rate': 2e-5,  # Arctic 推薦學習率\n",
    "}\n",
    "\n",
    "print(\"🚀 開始訓練...\")\n",
    "print(\"💡 提示：訓練過程中會自動保存檢查點，可以安全中斷並續訓\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "try:\n",
    "    # 開始訓練（支援續訓）\n",
    "    train_result, trained_model = trainer.train_model(\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        resume_from_checkpoint=checkpoint_path,\n",
    "        custom_training_args=custom_training_config\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"🎉 訓練成功完成！\")\n",
    "    print(f\"   • 總訓練步數: {train_result.global_step}\")\n",
    "    print(f\"   • 最終訓練損失: {train_result.training_loss:.4f}\")\n",
    "    print(f\"   • 訓練用時: {train_result.metrics.get('train_runtime', 0):.1f} 秒\")\n",
    "    \n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n⏸️  訓練被用戶中斷\")\n",
    "    print(\"💾 檢查點已自動保存，可稍後續訓\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ 訓練過程中出現錯誤: {e}\")\n",
    "    print(\"💾 檢查檢查點是否已保存\")\n",
    "    print(\"🔄 可嘗試從最新檢查點續訓\")\n",
    "    \n",
    "    # 顯示可用的檢查點\n",
    "    available_checkpoints = list(checkpoint_manager.checkpoint_dir.glob(\"checkpoint-*\"))\n",
    "    if available_checkpoints:\n",
    "        print(f\"📂 可用檢查點: {len(available_checkpoints)} 個\")\n",
    "        for cp in sorted(available_checkpoints)[-3:]:  # 顯示最近3個\n",
    "            print(f\"   • {cp.name}\")\n",
    "    \n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 步驟 4: 訓練結果分析和可視化\n",
    "print(\"📊 第四步：訓練結果分析\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 生成訓練歷史可視化\n",
    "try:\n",
    "    print(\"🎨 生成訓練歷史圖表...\")\n",
    "    trainer.plot_training_history()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"⚠️  可視化生成失敗: {e}\")\n",
    "    print(\"   可能原因：沒有足夠的訓練數據\")\n",
    "\n",
    "# 顯示最終統計\n",
    "print(\"\\n📈 訓練總結統計:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "try:\n",
    "    history = trainer.training_history\n",
    "    \n",
    "    if history['train_loss']:\n",
    "        print(f\"✅ 訓練指標:\")\n",
    "        print(f\"   • 總訓練步數: {len(history['train_loss'])}\")\n",
    "        print(f\"   • 初始損失: {history['train_loss'][0]:.4f}\")\n",
    "        print(f\"   • 最終損失: {history['train_loss'][-1]:.4f}\")\n",
    "        print(f\"   • 損失改善: {history['train_loss'][0] - history['train_loss'][-1]:.4f}\")\n",
    "        \n",
    "        if history['eval_loss']:\n",
    "            print(f\"   • 最佳驗證損失: {min(history['eval_loss']):.4f}\")\n",
    "        \n",
    "        if history['gpu_memory']:\n",
    "            print(f\"   • 平均 GPU 使用: {np.mean(history['gpu_memory']):.1f} GB\")\n",
    "            print(f\"   • 峰值 GPU 使用: {max(history['gpu_memory']):.1f} GB\")\n",
    "    \n",
    "    # 檢查點信息\n",
    "    print(f\"\\n💾 檢查點信息:\")\n",
    "    checkpoints = list(checkpoint_manager.checkpoint_dir.glob(\"checkpoint-*\"))\n",
    "    print(f\"   • 保存的檢查點數: {len(checkpoints)}\")\n",
    "    \n",
    "    final_model_path = checkpoint_manager.base_dir / \"final_model\"\n",
    "    if final_model_path.exists():\n",
    "        print(f\"   • 最終模型路徑: {final_model_path}\")\n",
    "        model_files = list(final_model_path.glob(\"*\"))\n",
    "        print(f\"   • 模型文件數: {len(model_files)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"⚠️  統計信息生成失敗: {e}\")\n",
    "\n",
    "print(\"\\n🎊 Arctic-Text2SQL QLoRA 微調完成！\")\n",
    "print(\"=\" * 50)\n",
    "print(\"✅ 您已成功完成:\")\n",
    "print(\"   • Arctic-Text2SQL-R1 7B 模型微調\")\n",
    "print(\"   • QLoRA 4-bit 量化訓練\")\n",
    "print(\"   • SynSQL 資料集訓練\")\n",
    "print(\"   • 完整的檢查點管理\")\n",
    "print(\"   • 訓練過程監控\")\n",
    "\n",
    "print(\"\\n🚀 下一步建議:\")\n",
    "print(\"   • 使用訓練好的模型進行 SQL 生成測試\")\n",
    "print(\"   • 在 BIRD/Spider 基準上評估模型性能\")\n",
    "print(\"   • 部署到生產環境進行實際應用\")\n",
    "print(\"   • 根據業務需求進一步微調\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}